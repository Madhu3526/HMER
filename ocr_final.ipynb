{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5f850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mathwriting_transformer_train_cpu_friendly.py\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Reshape, Dense,\n",
    "                                     BatchNormalization, Dropout, Embedding, MultiHeadAttention,\n",
    "                                     LayerNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import re\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd73718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "DATA_ROOT = \"./mathwriting-2024\"\n",
    "TRAIN_DIR = r\"D:\\OCR_MATH\\mathwriting-2024\\train\"\n",
    "VALID_DIR = r\"D:\\OCR_MATH\\mathwriting-2024\\valid\"\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "\n",
    "# CPU-friendly training defaults (tune when on GPU)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "MAX_OUTPUT_TOKENS = 120\n",
    "EMBED_DIM = 256            # token embedding dim\n",
    "TRANSFORMER_DIM = 256      # model hidden dim (d_model)\n",
    "NUM_HEADS = 4\n",
    "NUM_DEC_LAYERS = 2\n",
    "DFF = 512                  # feed-forward inner dim (reduced for CPU)\n",
    "DROPOUT_RATE = 0.2\n",
    "VOCAB_OOV_TOKEN = \"<UNK>\"\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "TRAIN_SUBSET = 500\n",
    "VALID_SUBSET = 100\n",
    "\n",
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "PAD_TOKEN = \"<PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 500, Valid size: 100\n",
      "Vocab size: 429\n",
      "Tokenizer saved to tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- InkML Processing Functions ---\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx-minx, maxy-miny\n",
    "    width, height = (width if width > 0 else 1), (height if height > 0 else 1)\n",
    "    scale = (IMG_SIZE - 2*PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx)*scale + PADDING, (y - miny)*scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x-STROKE_WIDTH, y-STROKE_WIDTH, x+STROKE_WIDTH, y+STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i-1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # strokes = 1\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# --- Tokenization ---\n",
    "# fixed regex: backslash-cmds, braces, numbers, identifiers, operators, other chars\n",
    "token_pattern = re.compile(r\"(\\\\[A-Za-z]+)|([{}^_])|(\\d+\\.\\d+|\\d+)|([A-Za-z]+)|([+\\-*/=(),\\[\\].:;%])|(\\\\.)|(\\S)\", re.VERBOSE)\n",
    "\n",
    "def tokenize_latex_lite(s):\n",
    "    if not s: return []\n",
    "    return [m.group(0) for m in token_pattern.finditer(s)]\n",
    "\n",
    "def normalize_label_for_tokenizer(label):\n",
    "    if not label or label.strip() == \"\":\n",
    "        return PAD_TOKEN\n",
    "    # add START/END here\n",
    "    tokens = tokenize_latex_lite(label)\n",
    "    return f\"{START_TOKEN} \" + \" \".join(tokens) + f\" {END_TOKEN}\"\n",
    "\n",
    "def build_file_label_list(folder):\n",
    "    files, labels = [], []\n",
    "    for fp in glob(os.path.join(folder, \"*.inkml\")):\n",
    "        try:\n",
    "            tree = ET.parse(fp)\n",
    "            root = tree.getroot()\n",
    "            ann = root.find(XML_NS + \"annotation[@type='normalizedLabel']\")\n",
    "            label = ann.text.strip() if ann is not None and ann.text else \"\"\n",
    "        except Exception:\n",
    "            label = \"\"\n",
    "        files.append(fp)\n",
    "        labels.append(normalize_label_for_tokenizer(label))\n",
    "    return files, labels\n",
    "\n",
    "# --- Load filelists and subset for CPU test ---\n",
    "train_files, train_labels = build_file_label_list(TRAIN_DIR)\n",
    "valid_files, valid_labels = build_file_label_list(VALID_DIR)\n",
    "\n",
    "# Subset to run quick test on CPU\n",
    "train_files, train_labels = train_files[:TRAIN_SUBSET], train_labels[:TRAIN_SUBSET]\n",
    "valid_files, valid_labels = valid_files[:VALID_SUBSET], valid_labels[:VALID_SUBSET]\n",
    "\n",
    "print(f\"Train size: {len(train_files)}, Valid size: {len(valid_files)}\")\n",
    "\n",
    "# Build tokenizer and save it\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token=VOCAB_OOV_TOKEN, split=' ')\n",
    "tokenizer.fit_on_texts([PAD_TOKEN, START_TOKEN, END_TOKEN] + train_labels + valid_labels)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v:k for k,v in word_index.items()}\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer saved to tokenizer.pkl\")\n",
    "\n",
    "# --- Encode helper ---\n",
    "def encode_label_to_sequence(label):\n",
    "    seq = tokenizer.texts_to_sequences([label])[0]\n",
    "    # ensure length MAX_OUTPUT_TOKENS\n",
    "    if len(seq) < MAX_OUTPUT_TOKENS:\n",
    "        seq = seq + [word_index.get(PAD_TOKEN, 0)] * (MAX_OUTPUT_TOKENS - len(seq))\n",
    "    else:\n",
    "        seq = seq[:MAX_OUTPUT_TOKENS]\n",
    "    return np.array(seq, dtype=np.int32)\n",
    "\n",
    "def gen_example(file_path, label):\n",
    "    img = inkml_to_image_array(file_path)\n",
    "    img = np.expand_dims(img, -1).astype(np.float32)\n",
    "    seq = encode_label_to_sequence(label)\n",
    "    return img, seq\n",
    "\n",
    "# --- tf.data helpers ---\n",
    "def tf_load_and_preprocess(path, label):\n",
    "    img, seq = tf.py_function(func=lambda p, l: gen_example(p.numpy().decode('utf-8'), l.numpy().decode('utf-8')),\n",
    "                              inp=[path, label],\n",
    "                              Tout=[tf.float32, tf.int32])\n",
    "    img.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "    seq.set_shape((MAX_OUTPUT_TOKENS,))\n",
    "    return img, seq\n",
    "\n",
    "def make_training_dataset(files, labels, batch=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(files), 1000))\n",
    "    ds = ds.map(tf_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    def prepare(img, seq):\n",
    "        # decoder input = seq[:-1] padded to MAX_OUTPUT_TOKENS\n",
    "        decoder_input = seq[:-1]  # length = MAX_OUTPUT_TOKENS-1\n",
    "        decoder_target = seq[1:]  # length = MAX_OUTPUT_TOKENS-1\n",
    "\n",
    "        # pad both to MAX_OUTPUT_TOKENS\n",
    "        pad_len = MAX_OUTPUT_TOKENS - tf.shape(decoder_input)[0]\n",
    "        decoder_input = tf.concat([decoder_input, tf.fill([pad_len], word_index.get(PAD_TOKEN, 0))], axis=0)\n",
    "        decoder_target = tf.concat([decoder_target, tf.fill([pad_len], word_index.get(PAD_TOKEN, 0))], axis=0)\n",
    "\n",
    "        decoder_target = tf.expand_dims(decoder_target, -1)  # (MAX_OUTPUT_TOKENS, 1)\n",
    "        return (img, decoder_input), decoder_target\n",
    "\n",
    "    ds = ds.map(prepare, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_tf = make_training_dataset(train_files, train_labels, batch=BATCH_SIZE)\n",
    "valid_ds_tf = make_training_dataset(valid_files, valid_labels, batch=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Transformer decoder layer factory ---\n",
    "def transformer_decoder_layer(d_model, num_heads, dff, dropout_rate):\n",
    "    # returns a small functional model implementing one decoder layer\n",
    "    inputs_seq = Input(shape=(None, d_model))    # decoder sequence (batch, t_dec, d_model)\n",
    "    enc_outputs = Input(shape=(None, d_model))   # encoder outputs (batch, t_enc, d_model)\n",
    "\n",
    "    # masked self-attention (we rely on causal masking during training by shaping inputs appropriately if needed)\n",
    "    attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(\n",
    "        inputs_seq, inputs_seq, attention_mask=None)\n",
    "    attn1 = Dropout(dropout_rate)(attn1)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs_seq + attn1)\n",
    "\n",
    "    # cross-attention with encoder\n",
    "    attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(\n",
    "        out1, enc_outputs)\n",
    "    attn2 = Dropout(dropout_rate)(attn2)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "\n",
    "    # feed-forward\n",
    "    ffn = Dense(dff, activation='relu')(out2)\n",
    "    ffn = Dense(d_model)(ffn)\n",
    "    ffn = Dropout(dropout_rate)(ffn)\n",
    "    output = LayerNormalization(epsilon=1e-6)(out2 + ffn)\n",
    "\n",
    "    return Model([inputs_seq, enc_outputs], output, name=\"transformer_decoder_layer\")\n",
    "\n",
    "# --- Build model ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7015cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformer decoder layer factory ---\n",
    "def transformer_decoder_layer(d_model, num_heads, dff, dropout_rate, name=\"transformer_decoder_layer\"):\n",
    "    # returns a small functional model implementing one decoder layer\n",
    "    inputs_seq = Input(shape=(None, d_model))    # decoder sequence (batch, t_dec, d_model)\n",
    "    enc_outputs = Input(shape=(None, d_model))   # encoder outputs (batch, t_enc, d_model)\n",
    "\n",
    "    # masked self-attention (we rely on causal masking during training by shaping inputs appropriately if needed)\n",
    "    attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(\n",
    "        inputs_seq, inputs_seq, attention_mask=None)\n",
    "    attn1 = Dropout(dropout_rate)(attn1)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs_seq + attn1)\n",
    "\n",
    "    # cross-attention with encoder\n",
    "    attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(\n",
    "        out1, enc_outputs)\n",
    "    attn2 = Dropout(dropout_rate)(attn2)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "\n",
    "    # feed-forward\n",
    "    ffn = Dense(dff, activation='relu')(out2)\n",
    "    ffn = Dense(d_model)(ffn)\n",
    "    ffn = Dropout(dropout_rate)(ffn)\n",
    "    output = LayerNormalization(epsilon=1e-6)(out2 + ffn)\n",
    "\n",
    "    return Model([inputs_seq, enc_outputs], output, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca86a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5000, Valid size: 500\n",
      "Vocab size: 1364\n",
      "Tokenizer saved to tokenizer.pkl\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " img_input (InputLayer)      [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 128, 128, 32)         320       ['img_input[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 128, 128, 32)         128       ['conv2d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_27 (MaxPooli  (None, 64, 64, 32)           0         ['batch_normalization_12[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)        (None, 64, 64, 32)           0         ['max_pooling2d_27[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 64, 64, 64)           18496     ['dropout_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 64, 64, 64)           256       ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_28 (MaxPooli  (None, 32, 32, 64)           0         ['batch_normalization_13[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)        (None, 32, 32, 64)           0         ['max_pooling2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 32, 32, 128)          73856     ['dropout_51[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 32, 32, 128)          512       ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_29 (MaxPooli  (None, 16, 16, 128)          0         ['batch_normalization_14[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)     [(None, 120)]                0         []                            \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)         (None, 16, 2048)             0         ['max_pooling2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " token_embedding (Embedding  (None, 120, 256)             349184    ['dec_inputs[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " enc_proj (Dense)            (None, 16, 256)              524544    ['reshape_9[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_decoder_layer_  (None, None, 256)            790784    ['token_embedding[0][0]',     \n",
      " 0 (Functional)                                                      'enc_proj[0][0]']            \n",
      "                                                                                                  \n",
      " transformer_decoder_layer_  (None, None, 256)            790784    ['transformer_decoder_layer_0[\n",
      " 1 (Functional)                                                     0][0]',                       \n",
      "                                                                     'enc_proj[0][0]']            \n",
      "                                                                                                  \n",
      " outputs (Dense)             (None, 120, 1364)            350548    ['transformer_decoder_layer_1[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2899412 (11.06 MB)\n",
      "Trainable params: 2898964 (11.06 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 132s 398ms/step - loss: 0.7735 - accuracy: 0.8890 - val_loss: 0.4345 - val_accuracy: 0.9139\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 126s 403ms/step - loss: 0.4268 - accuracy: 0.9113 - val_loss: 0.3733 - val_accuracy: 0.9213\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 138s 440ms/step - loss: 0.3816 - accuracy: 0.9184 - val_loss: 0.3453 - val_accuracy: 0.9265\n",
      "\n",
      "Model saved \n"
     ]
    }
   ],
   "source": [
    "# mathwriting_transformer_train_cpu_friendly.py\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Reshape, Dense,\n",
    "                                     BatchNormalization, Dropout, Embedding, MultiHeadAttention,\n",
    "                                     LayerNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# --- Config ---\n",
    "DATA_ROOT = \"./mathwriting-2024\"\n",
    "# IMPORTANT: Update these paths to your actual data directories\n",
    "TRAIN_DIR = r\"D:\\OCR_MATH\\mathwriting-2024\\train\"\n",
    "VALID_DIR = r\"D:\\OCR_MATH\\mathwriting-2024\\valid\"\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "\n",
    "# CPU-friendly training defaults (tune when on GPU)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "MAX_OUTPUT_TOKENS = 120\n",
    "EMBED_DIM = 256          # token embedding dim\n",
    "TRANSFORMER_DIM = 256    # model hidden dim (d_model)\n",
    "NUM_HEADS = 4\n",
    "NUM_DEC_LAYERS = 2\n",
    "DFF = 512                # feed-forward inner dim (reduced for CPU)\n",
    "DROPOUT_RATE = 0.2\n",
    "VOCAB_OOV_TOKEN = \"<UNK>\"\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "# Using a smaller subset for quick testing\n",
    "TRAIN_SUBSET = 5000\n",
    "VALID_SUBSET = 500\n",
    "\n",
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "# --- InkML Processing Functions ---\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx-minx, maxy-miny\n",
    "    width, height = (width if width > 0 else 1), (height if height > 0 else 1)\n",
    "    scale = (IMG_SIZE - 2*PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx)*scale + PADDING, (y - miny)*scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x-STROKE_WIDTH, y-STROKE_WIDTH, x+STROKE_WIDTH, y+STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i-1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # strokes = 1\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# --- Tokenization ---\n",
    "# fixed regex: backslash-cmds, braces, numbers, identifiers, operators, other chars\n",
    "token_pattern = re.compile(r\"(\\\\[A-Za-z]+)|([{}^_])|(\\d+\\.\\d+|\\d+)|([A-Za-z]+)|([+\\-*/=(),\\[\\].:;%])|(\\\\.)|(\\S)\", re.VERBOSE)\n",
    "\n",
    "def tokenize_latex_lite(s):\n",
    "    if not s: return []\n",
    "    return [m.group(0) for m in token_pattern.finditer(s)]\n",
    "\n",
    "def normalize_label_for_tokenizer(label):\n",
    "    if not label or label.strip() == \"\":\n",
    "        return PAD_TOKEN\n",
    "    # add START/END here\n",
    "    tokens = tokenize_latex_lite(label)\n",
    "    return f\"{START_TOKEN} \" + \" \".join(tokens) + f\" {END_TOKEN}\"\n",
    "\n",
    "def build_file_label_list(folder):\n",
    "    files, labels = [], []\n",
    "    for fp in glob(os.path.join(folder, \"*.inkml\")):\n",
    "        try:\n",
    "            tree = ET.parse(fp)\n",
    "            root = tree.getroot()\n",
    "            ann = root.find(XML_NS + \"annotation[@type='normalizedLabel']\")\n",
    "            label = ann.text.strip() if ann is not None and ann.text else \"\"\n",
    "        except Exception:\n",
    "            label = \"\"\n",
    "        files.append(fp)\n",
    "        labels.append(normalize_label_for_tokenizer(label))\n",
    "    return files, labels\n",
    "\n",
    "# --- Load filelists and subset for CPU test ---\n",
    "train_files, train_labels = build_file_label_list(TRAIN_DIR)\n",
    "valid_files, valid_labels = build_file_label_list(VALID_DIR)\n",
    "\n",
    "# Subset to run quick test on CPU\n",
    "train_files, train_labels = train_files[:TRAIN_SUBSET], train_labels[:TRAIN_SUBSET]\n",
    "valid_files, valid_labels = valid_files[:VALID_SUBSET], valid_labels[:VALID_SUBSET]\n",
    "\n",
    "print(f\"Train size: {len(train_files)}, Valid size: {len(valid_files)}\")\n",
    "\n",
    "# Build tokenizer and save it\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token=VOCAB_OOV_TOKEN, split=' ')\n",
    "tokenizer.fit_on_texts([PAD_TOKEN, START_TOKEN, END_TOKEN] + train_labels + valid_labels)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v:k for k,v in word_index.items()}\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer saved to tokenizer.pkl\")\n",
    "\n",
    "# --- Encode helper ---\n",
    "def encode_label_to_sequence(label):\n",
    "    seq = tokenizer.texts_to_sequences([label])[0]\n",
    "    # ensure length MAX_OUTPUT_TOKENS\n",
    "    if len(seq) < MAX_OUTPUT_TOKENS:\n",
    "        seq = seq + [word_index.get(PAD_TOKEN, 0)] * (MAX_OUTPUT_TOKENS - len(seq))\n",
    "    else:\n",
    "        seq = seq[:MAX_OUTPUT_TOKENS]\n",
    "    return np.array(seq, dtype=np.int32)\n",
    "\n",
    "def gen_example(file_path, label):\n",
    "    img = inkml_to_image_array(file_path)\n",
    "    img = np.expand_dims(img, -1).astype(np.float32)\n",
    "    seq = encode_label_to_sequence(label)\n",
    "    return img, seq\n",
    "\n",
    "# --- tf.data helpers ---\n",
    "def tf_load_and_preprocess(path, label):\n",
    "    img, seq = tf.py_function(func=lambda p, l: gen_example(p.numpy().decode('utf-8'), l.numpy().decode('utf-8')),\n",
    "                              inp=[path, label],\n",
    "                              Tout=[tf.float32, tf.int32])\n",
    "    img.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "    seq.set_shape((MAX_OUTPUT_TOKENS,))\n",
    "    return img, seq\n",
    "\n",
    "def make_training_dataset(files, labels, batch=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(files), 1000))\n",
    "    ds = ds.map(tf_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    def prepare(img, seq):\n",
    "        # decoder input = seq[:-1]\n",
    "        # decoder target = seq[1:]\n",
    "        decoder_input = seq[:-1]\n",
    "        decoder_target = seq[1:]\n",
    "\n",
    "        # Pad both to MAX_OUTPUT_TOKENS-1 to ensure consistent shape\n",
    "        pad_len_input = (MAX_OUTPUT_TOKENS - 1) - tf.shape(decoder_input)[0]\n",
    "        decoder_input = tf.concat([decoder_input, tf.fill([pad_len_input], word_index.get(PAD_TOKEN, 0))], axis=0)\n",
    "        \n",
    "        # We shift decoder_input for teacher forcing, but the final model input must be MAX_OUTPUT_TOKENS\n",
    "        # Let's adjust the prepare function to be simpler and correct.\n",
    "        # The decoder input should be the full sequence padded, as handled in build_transformer_model\n",
    "        # The target should be the sequence shifted left by one.\n",
    "\n",
    "        # The logic in the original file was a bit complex. Let's simplify and correct.\n",
    "        # The `dec_inputs` will be `seq` (padded).\n",
    "        # The target `y` will be `seq` shifted left by 1 and padded.\n",
    "        \n",
    "        # Correction to the original prepare logic:\n",
    "        # The input to the decoder should be seq[:-1], target should be seq[1:]\n",
    "        # The model inputs are (img, decoder_input), and the output is decoder_target\n",
    "        target = seq[1:]\n",
    "        # The decoder_input for training is the sequence shifted right\n",
    "        decoder_input = seq[:-1]\n",
    "        \n",
    "        # The model expects MAX_OUTPUT_TOKENS, but we have MAX_OUTPUT_TOKENS-1\n",
    "        # The original code's padding logic was complex, a simpler way is to pad them back\n",
    "        pad_token_id = word_index.get(PAD_TOKEN, 0)\n",
    "        decoder_input = tf.pad(decoder_input, [[0, 1]], constant_values=pad_token_id)\n",
    "        target = tf.pad(target, [[0, 1]], constant_values=pad_token_id)\n",
    "\n",
    "        return (img, decoder_input), target\n",
    "\n",
    "    ds = ds.map(prepare, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_tf = make_training_dataset(train_files, train_labels, batch=BATCH_SIZE)\n",
    "valid_ds_tf = make_training_dataset(valid_files, valid_labels, batch=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Transformer decoder layer factory ---\n",
    "# CORRECTED FUNCTION\n",
    "def transformer_decoder_layer(d_model, num_heads, dff, dropout_rate, name=\"transformer_decoder_layer\"): # CHANGED: Added name argument\n",
    "    # returns a small functional model implementing one decoder layer\n",
    "    inputs_seq = Input(shape=(None, d_model))    # decoder sequence (batch, t_dec, d_model)\n",
    "    enc_outputs = Input(shape=(None, d_model))   # encoder outputs (batch, t_enc, d_model)\n",
    "\n",
    "    # masked self-attention\n",
    "    attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(\n",
    "        inputs_seq, inputs_seq, attention_mask=None)\n",
    "    attn1 = Dropout(dropout_rate)(attn1)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs_seq + attn1)\n",
    "\n",
    "    # cross-attention with encoder\n",
    "    attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(\n",
    "        out1, enc_outputs)\n",
    "    attn2 = Dropout(dropout_rate)(attn2)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "\n",
    "    # feed-forward\n",
    "    ffn = Dense(dff, activation='relu')(out2)\n",
    "    ffn = Dense(d_model)(ffn)\n",
    "    ffn = Dropout(dropout_rate)(ffn)\n",
    "    output = LayerNormalization(epsilon=1e-6)(out2 + ffn)\n",
    "\n",
    "    return Model([inputs_seq, enc_outputs], output, name=name) # CHANGED: Use the name variable\n",
    "\n",
    "# --- Build model ---\n",
    "# CORRECTED FUNCTION\n",
    "def build_transformer_model():\n",
    "    img_input = Input(shape=(IMG_SIZE, IMG_SIZE, 1), name=\"img_input\")\n",
    "    x = Conv2D(32, (3,3), padding='same', activation='relu')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    x = Conv2D(64,(3,3),padding='same',activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    x = Conv2D(128,(3,3),padding='same',activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    pool_h = IMG_SIZE//8\n",
    "    pool_w = IMG_SIZE//8\n",
    "    x = Reshape((pool_h, pool_w*128))(x)\n",
    "    enc_outputs = Dense(TRANSFORMER_DIM, activation='tanh', name=\"enc_proj\")(x)\n",
    "\n",
    "    dec_inputs = Input(shape=(MAX_OUTPUT_TOKENS,), name=\"dec_inputs\")\n",
    "    embedding_layer = Embedding(vocab_size, EMBED_DIM, mask_zero=True, name=\"token_embedding\")\n",
    "    dec_emb = embedding_layer(dec_inputs)\n",
    "\n",
    "    if EMBED_DIM != TRANSFORMER_DIM:\n",
    "        dec_emb = Dense(TRANSFORMER_DIM,name=\"dec_emb_proj\")(dec_emb)\n",
    "\n",
    "    xdec = dec_emb\n",
    "    for i in range(NUM_DEC_LAYERS):\n",
    "        # CHANGED: Create a unique name for each layer and pass it\n",
    "        layer_name = f\"transformer_decoder_layer_{i}\"\n",
    "        layer = transformer_decoder_layer(TRANSFORMER_DIM, NUM_HEADS, DFF, DROPOUT_RATE, name=layer_name)\n",
    "        xdec = layer([xdec, enc_outputs])\n",
    "\n",
    "    outputs = Dense(vocab_size, activation='softmax', name=\"outputs\")(xdec)\n",
    "    model = Model([img_input, dec_inputs], outputs)\n",
    "    model.compile(optimizer=Adam(1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "transformer_model = build_transformer_model()\n",
    "transformer_model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# Train\n",
    "# -------------------------------\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "transformer_model.fit(train_ds_tf,\n",
    "                      validation_data=valid_ds_tf,\n",
    "                      epochs=EPOCHS)\n",
    "\n",
    "transformer_model.save(\"math_ocr_transformer_final.h5\")\n",
    "print(\"\\nModel saved \")\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction\n",
    "# -------------------------------\n",
    "def predict_sample(model, tokenizer, inkml_path, max_tokens=MAX_OUTPUT_TOKENS):\n",
    "    img = inkml_to_image_array(inkml_path)\n",
    "    img = np.expand_dims(img,-1).astype(np.float32)\n",
    "    img = np.expand_dims(img,0)\n",
    "    start_idx = tokenizer.word_index.get(START_TOKEN,1)\n",
    "    end_idx = tokenizer.word_index.get(END_TOKEN,2)\n",
    "    pad_idx = tokenizer.word_index.get(PAD_TOKEN, 0)\n",
    "    \n",
    "    # Initialize the decoder sequence with the start token\n",
    "    seq = [start_idx] + [pad_idx] * (max_tokens - 1)\n",
    "    \n",
    "    for pos in range(max_tokens - 1):\n",
    "        dec_input = np.array([seq])\n",
    "        preds = model.predict([img, dec_input], verbose=0)\n",
    "        \n",
    "        # Get the predicted token ID for the current position\n",
    "        next_id = int(np.argmax(preds[0, pos, :]))\n",
    "        \n",
    "        # Update the sequence with the predicted token\n",
    "        seq[pos + 1] = next_id\n",
    "        \n",
    "        # If the end token is predicted, stop generating\n",
    "        if next_id == end_idx:\n",
    "            break\n",
    "            \n",
    "    # Convert token IDs back to words\n",
    "    tokens = [index_word.get(i, \"<UNK>\") for i in seq]\n",
    "    \n",
    "    # Clean up the output sequence\n",
    "    if tokens and tokens[0] == START_TOKEN:\n",
    "        tokens = tokens[1:]\n",
    "    if END_TOKEN in tokens:\n",
    "        tokens = tokens[:tokens.index(END_TOKEN)]\n",
    "        \n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc44d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model and tokenizer...\n",
      "Loading complete.\n",
      "--------------------------------------------------\n",
      "Input File:    0a0c50d6b6100236.inkml\n",
      "Predicted LaTeX: \\int _ { \\partial x ^ { 1 }\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# --- Configuration ---\n",
    "# These must match the settings used during training\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "MAX_OUTPUT_TOKENS = 120\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "# Paths to the saved model and tokenizer\n",
    "MODEL_PATH = \"math_ocr_transformer_final.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer.pkl\"\n",
    "\n",
    "# --- InkML Processing Functions (Copied from training script) ---\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx - minx, maxy - miny\n",
    "    width, height = (width if width > 0 else 1), (height if height > 0 else 1)\n",
    "    scale = (IMG_SIZE - 2 * PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx) * scale + PADDING, (y - miny) * scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x - STROKE_WIDTH, y - STROKE_WIDTH, x + STROKE_WIDTH, y + STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i - 1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # Invert so strokes are 1s and background is 0\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# --- Prediction Function ---\n",
    "def predict_from_inkml(model, tokenizer, inkml_path):\n",
    "    \"\"\"\n",
    "    Loads an InkML file, preprocesses it, and returns the model's prediction.\n",
    "    \"\"\"\n",
    "    # Create the reverse mapping from integer IDs to tokens\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    \n",
    "    # Get special token IDs\n",
    "    start_idx = tokenizer.word_index.get(\"<START>\", 1)\n",
    "    end_idx = tokenizer.word_index.get(\"<END>\", 2)\n",
    "    pad_idx = tokenizer.word_index.get(\"<PAD>\", 0)\n",
    "\n",
    "    # Preprocess the input InkML file\n",
    "    img_array = inkml_to_image_array(inkml_path)\n",
    "    # Add batch and channel dimensions: (1, 128, 128, 1)\n",
    "    img_tensor = np.expand_dims(img_array, axis=0)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=-1)\n",
    "\n",
    "    # Initialize the decoder sequence with the start token\n",
    "    decoder_input_seq = [start_idx] + [pad_idx] * (MAX_OUTPUT_TOKENS - 1)\n",
    "    \n",
    "    # Autoregressive decoding loop\n",
    "    for pos in range(MAX_OUTPUT_TOKENS - 1):\n",
    "        decoder_input_tensor = np.array([decoder_input_seq])\n",
    "        \n",
    "        # Get model predictions\n",
    "        preds = model.predict([img_tensor, decoder_input_tensor], verbose=0)\n",
    "        \n",
    "        # Find the token with the highest probability at the current position\n",
    "        next_id = int(np.argmax(preds[0, pos, :]))\n",
    "        \n",
    "        # Update the sequence with the predicted token\n",
    "        decoder_input_seq[pos + 1] = next_id\n",
    "        \n",
    "        # If the model predicts the end token, stop generating\n",
    "        if next_id == end_idx:\n",
    "            break\n",
    "            \n",
    "    # Convert token IDs back to words\n",
    "    tokens = [index_word.get(i, \"<UNK>\") for i in decoder_input_seq]\n",
    "    \n",
    "    # Clean up the output by removing special tokens\n",
    "    if tokens and tokens[0] == \"<START>\":\n",
    "        tokens = tokens[1:]\n",
    "    if \"<END>\" in tokens:\n",
    "        tokens = tokens[:tokens.index(\"<END>\")]\n",
    "    if \"<PAD>\" in tokens: # Should not happen if <END> is found, but as a safeguard\n",
    "        tokens = tokens[:tokens.index(\"<PAD>\")]\n",
    "        \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT ---\n",
    "    # Change this path to point to an InkML file you want to test\n",
    "    SAMPLE_INKML_PATH = r\"D:\\OCR_MATH\\sample\\train\\0a0c50d6b6100236.inkml\" # <--- CHANGE THIS\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH) or not os.path.exists(TOKENIZER_PATH):\n",
    "        print(\"Error: Model or tokenizer file not found.\")\n",
    "        print(f\"Make sure '{MODEL_PATH}' and '{TOKENIZER_PATH}' are in the same directory as this script.\")\n",
    "    elif not os.path.exists(SAMPLE_INKML_PATH):\n",
    "        print(f\"Error: Sample file not found at '{SAMPLE_INKML_PATH}'\")\n",
    "        print(\"Please update the SAMPLE_INKML_PATH variable to a valid .inkml file.\")\n",
    "    else:\n",
    "        # Load the trained model and tokenizer\n",
    "        print(\"Loading trained model and tokenizer...\")\n",
    "        model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        with open(TOKENIZER_PATH, 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        print(\"Loading complete.\")\n",
    "\n",
    "        # Make a prediction\n",
    "        predicted_latex = predict_from_inkml(model, tokenizer, SAMPLE_INKML_PATH)\n",
    "\n",
    "        # Display the result\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Input File:    {os.path.basename(SAMPLE_INKML_PATH)}\")\n",
    "        print(f\"Predicted LaTeX: {predicted_latex}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model and tokenizer...\n",
      "Loading complete.\n",
      "Generating and displaying input image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Input File:    fcbbec9d3f7956ac.inkml\n",
      "Predicted LaTeX: \\int _ { \\partial x )\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt # CHANGED: Added matplotlib import\n",
    "\n",
    "# --- Configuration ---\n",
    "# These must match the settings used during training\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "MAX_OUTPUT_TOKENS = 120\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "# Paths to the saved model and tokenizer\n",
    "MODEL_PATH = \"math_ocr_transformer_final.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer.pkl\"\n",
    "\n",
    "# --- InkML Processing Functions (Copied from training script) ---\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx - minx, maxy - miny\n",
    "    width, height = (width if width > 0 else 1), (height if height > 0 else 1)\n",
    "    scale = (IMG_SIZE - 2 * PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx) * scale + PADDING, (y - miny) * scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x - STROKE_WIDTH, y - STROKE_WIDTH, x + STROKE_WIDTH, y + STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i - 1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # Invert so strokes are 1s and background is 0\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# --- Prediction Function ---\n",
    "def predict_from_inkml(model, tokenizer, inkml_path):\n",
    "    # (This function remains unchanged)\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    start_idx = tokenizer.word_index.get(\"<START>\", 1)\n",
    "    end_idx = tokenizer.word_index.get(\"<END>\", 2)\n",
    "    pad_idx = tokenizer.word_index.get(\"<PAD>\", 0)\n",
    "    img_array = inkml_to_image_array(inkml_path)\n",
    "    img_tensor = np.expand_dims(img_array, axis=0)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=-1)\n",
    "    decoder_input_seq = [start_idx] + [pad_idx] * (MAX_OUTPUT_TOKENS - 1)\n",
    "    for pos in range(MAX_OUTPUT_TOKENS - 1):\n",
    "        decoder_input_tensor = np.array([decoder_input_seq])\n",
    "        preds = model.predict([img_tensor, decoder_input_tensor], verbose=0)\n",
    "        next_id = int(np.argmax(preds[0, pos, :]))\n",
    "        decoder_input_seq[pos + 1] = next_id\n",
    "        if next_id == end_idx:\n",
    "            break\n",
    "    tokens = [index_word.get(i, \"<UNK>\") for i in decoder_input_seq]\n",
    "    if tokens and tokens[0] == \"<START>\":\n",
    "        tokens = tokens[1:]\n",
    "    if \"<END>\" in tokens:\n",
    "        tokens = tokens[:tokens.index(\"<END>\")]\n",
    "    if \"<PAD>\" in tokens:\n",
    "        tokens = tokens[:tokens.index(\"<PAD>\")]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT ---\n",
    "    # Change this path to point to an InkML file you want to test\n",
    "    SAMPLE_INKML_PATH = r\"D:\\OCR_MATH\\mathwriting-2024\\valid\\fcbbec9d3f7956ac.inkml\" # <--- CHANGE THIS\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH) or not os.path.exists(TOKENIZER_PATH):\n",
    "        print(\"Error: Model or tokenizer file not found.\")\n",
    "    elif not os.path.exists(SAMPLE_INKML_PATH):\n",
    "        print(f\"Error: Sample file not found at '{SAMPLE_INKML_PATH}'\")\n",
    "    else:\n",
    "        # Load the trained model and tokenizer\n",
    "        print(\"Loading trained model and tokenizer...\")\n",
    "        model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        with open(TOKENIZER_PATH, 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        print(\"Loading complete.\")\n",
    "\n",
    "        # --- CHANGED: CODE TO DISPLAY THE IMAGE ---\n",
    "        print(\"Generating and displaying input image...\")\n",
    "        # Convert the InkML to an image array\n",
    "        image_to_display = inkml_to_image_array(SAMPLE_INKML_PATH)\n",
    "        \n",
    "        # Use matplotlib to show the image\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image_to_display, cmap='gray') # Use a grayscale colormap\n",
    "        plt.title(f\"Input: {os.path.basename(SAMPLE_INKML_PATH)}\")\n",
    "        plt.axis('off') # Hide the axes\n",
    "        plt.show()\n",
    "        # --- END OF NEW CODE ---\n",
    "\n",
    "        # Make a prediction\n",
    "        predicted_latex = predict_from_inkml(model, tokenizer, SAMPLE_INKML_PATH)\n",
    "\n",
    "        # Display the result\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Input File:    {os.path.basename(SAMPLE_INKML_PATH)}\")\n",
    "        print(f\"Predicted LaTeX: {predicted_latex}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f632d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5000, Valid size: 500\n",
      "Vocab size: 1364\n",
      "Tokenizer saved to tokenizer.pkl\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " img_input (InputLayer)      [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " random_rotation (RandomRot  (None, 128, 128, 1)          0         ['img_input[0][0]']           \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " random_translation (Random  (None, 128, 128, 1)          0         ['random_rotation[0][0]']     \n",
      " Translation)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 128, 128, 32)         320       ['random_translation[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 128, 128, 32)         128       ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_30 (MaxPooli  (None, 64, 64, 32)           0         ['batch_normalization_15[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)        (None, 64, 64, 32)           0         ['max_pooling2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 64, 64, 64)           18496     ['dropout_70[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 64, 64, 64)           256       ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_31 (MaxPooli  (None, 32, 32, 64)           0         ['batch_normalization_16[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)        (None, 32, 32, 64)           0         ['max_pooling2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 32, 32, 128)          73856     ['dropout_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 32, 32, 128)          512       ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_32 (MaxPooli  (None, 16, 16, 128)          0         ['batch_normalization_17[0][0]\n",
      " ng2D)                                                              ']                            \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)     [(None, 120)]                0         []                            \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)        (None, 16, 2048)             0         ['max_pooling2d_32[0][0]']    \n",
      "                                                                                                  \n",
      " token_embedding (Embedding  (None, 120, 256)             349184    ['dec_inputs[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " enc_proj (Dense)            (None, 16, 256)              524544    ['reshape_10[0][0]']          \n",
      "                                                                                                  \n",
      " transformer_decoder_layer_  (None, None, 256)            790784    ['token_embedding[0][0]',     \n",
      " 0 (Functional)                                                      'enc_proj[0][0]']            \n",
      "                                                                                                  \n",
      " transformer_decoder_layer_  (None, None, 256)            790784    ['transformer_decoder_layer_0[\n",
      " 1 (Functional)                                                     0][0]',                       \n",
      "                                                                     'enc_proj[0][0]']            \n",
      "                                                                                                  \n",
      " outputs (Dense)             (None, 120, 1364)            350548    ['transformer_decoder_layer_1[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2899412 (11.06 MB)\n",
      "Trainable params: 2898964 (11.06 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/100\n",
      "313/313 [==============================] - 211s 635ms/step - loss: 0.8515 - accuracy: 0.8809 - val_loss: 0.4444 - val_accuracy: 0.9128 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 248s 792ms/step - loss: 0.4336 - accuracy: 0.9110 - val_loss: 0.3759 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 122s 389ms/step - loss: 0.3845 - accuracy: 0.9186 - val_loss: 0.3470 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 120s 383ms/step - loss: 0.3581 - accuracy: 0.9228 - val_loss: 0.3290 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 128s 410ms/step - loss: 0.3406 - accuracy: 0.9251 - val_loss: 0.3175 - val_accuracy: 0.9313 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 127s 407ms/step - loss: 0.3254 - accuracy: 0.9273 - val_loss: 0.3052 - val_accuracy: 0.9335 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 129s 410ms/step - loss: 0.3120 - accuracy: 0.9289 - val_loss: 0.2954 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 125s 399ms/step - loss: 0.3000 - accuracy: 0.9304 - val_loss: 0.2876 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 127s 406ms/step - loss: 0.2893 - accuracy: 0.9316 - val_loss: 0.2776 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 131s 418ms/step - loss: 0.2787 - accuracy: 0.9329 - val_loss: 0.2701 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 134s 428ms/step - loss: 0.2676 - accuracy: 0.9344 - val_loss: 0.2585 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 130s 417ms/step - loss: 0.2561 - accuracy: 0.9353 - val_loss: 0.2481 - val_accuracy: 0.9401 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 132s 420ms/step - loss: 0.2459 - accuracy: 0.9366 - val_loss: 0.2428 - val_accuracy: 0.9410 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 126s 403ms/step - loss: 0.2369 - accuracy: 0.9376 - val_loss: 0.2368 - val_accuracy: 0.9415 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 127s 405ms/step - loss: 0.2288 - accuracy: 0.9385 - val_loss: 0.2289 - val_accuracy: 0.9421 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 127s 405ms/step - loss: 0.2218 - accuracy: 0.9393 - val_loss: 0.2250 - val_accuracy: 0.9428 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 126s 403ms/step - loss: 0.2157 - accuracy: 0.9398 - val_loss: 0.2228 - val_accuracy: 0.9428 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 125s 398ms/step - loss: 0.2094 - accuracy: 0.9407 - val_loss: 0.2188 - val_accuracy: 0.9434 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 125s 399ms/step - loss: 0.2046 - accuracy: 0.9412 - val_loss: 0.2161 - val_accuracy: 0.9436 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 125s 401ms/step - loss: 0.1995 - accuracy: 0.9418 - val_loss: 0.2140 - val_accuracy: 0.9441 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 125s 401ms/step - loss: 0.1948 - accuracy: 0.9424 - val_loss: 0.2130 - val_accuracy: 0.9439 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 127s 404ms/step - loss: 0.1903 - accuracy: 0.9433 - val_loss: 0.2096 - val_accuracy: 0.9444 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 129s 412ms/step - loss: 0.1862 - accuracy: 0.9436 - val_loss: 0.2080 - val_accuracy: 0.9452 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 129s 411ms/step - loss: 0.1828 - accuracy: 0.9438 - val_loss: 0.2081 - val_accuracy: 0.9448 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 129s 411ms/step - loss: 0.1791 - accuracy: 0.9445 - val_loss: 0.2067 - val_accuracy: 0.9447 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 129s 412ms/step - loss: 0.1751 - accuracy: 0.9453 - val_loss: 0.2054 - val_accuracy: 0.9455 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 136s 435ms/step - loss: 0.1715 - accuracy: 0.9455 - val_loss: 0.2045 - val_accuracy: 0.9452 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 126s 403ms/step - loss: 0.1686 - accuracy: 0.9455 - val_loss: 0.2049 - val_accuracy: 0.9460 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 163s 522ms/step - loss: 0.1650 - accuracy: 0.9463 - val_loss: 0.2036 - val_accuracy: 0.9461 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 122s 390ms/step - loss: 0.1622 - accuracy: 0.9470 - val_loss: 0.2017 - val_accuracy: 0.9458 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 116s 371ms/step - loss: 0.1592 - accuracy: 0.9472 - val_loss: 0.2023 - val_accuracy: 0.9463 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 120s 383ms/step - loss: 0.1566 - accuracy: 0.9476 - val_loss: 0.2025 - val_accuracy: 0.9464 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 127s 404ms/step - loss: 0.1541 - accuracy: 0.9484 - val_loss: 0.2015 - val_accuracy: 0.9462 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 322s 1s/step - loss: 0.1519 - accuracy: 0.9484 - val_loss: 0.2004 - val_accuracy: 0.9464 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 377s 1s/step - loss: 0.1497 - accuracy: 0.9489 - val_loss: 0.2017 - val_accuracy: 0.9466 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 378s 1s/step - loss: 0.1476 - accuracy: 0.9492 - val_loss: 0.2014 - val_accuracy: 0.9464 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 278s 885ms/step - loss: 0.1454 - accuracy: 0.9496 - val_loss: 0.2013 - val_accuracy: 0.9468 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 113s 360ms/step - loss: 0.1432 - accuracy: 0.9500 - val_loss: 0.2008 - val_accuracy: 0.9472 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9505\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "313/313 [==============================] - 117s 373ms/step - loss: 0.1410 - accuracy: 0.9505 - val_loss: 0.2015 - val_accuracy: 0.9472 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 124s 396ms/step - loss: 0.1348 - accuracy: 0.9521 - val_loss: 0.1999 - val_accuracy: 0.9473 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 123s 394ms/step - loss: 0.1333 - accuracy: 0.9523 - val_loss: 0.2001 - val_accuracy: 0.9471 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 133s 424ms/step - loss: 0.1330 - accuracy: 0.9525 - val_loss: 0.2004 - val_accuracy: 0.9472 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 135s 430ms/step - loss: 0.1323 - accuracy: 0.9525 - val_loss: 0.2001 - val_accuracy: 0.9472 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 133s 424ms/step - loss: 0.1317 - accuracy: 0.9528 - val_loss: 0.2004 - val_accuracy: 0.9477 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9529\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "313/313 [==============================] - 134s 427ms/step - loss: 0.1314 - accuracy: 0.9529 - val_loss: 0.2004 - val_accuracy: 0.9476 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 126s 402ms/step - loss: 0.1310 - accuracy: 0.9531 - val_loss: 0.2003 - val_accuracy: 0.9476 - lr: 1.0000e-06\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 131s 419ms/step - loss: 0.1310 - accuracy: 0.9528 - val_loss: 0.2003 - val_accuracy: 0.9476 - lr: 1.0000e-06\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 129s 412ms/step - loss: 0.1309 - accuracy: 0.9529 - val_loss: 0.2003 - val_accuracy: 0.9475 - lr: 1.0000e-06\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 127s 406ms/step - loss: 0.1306 - accuracy: 0.9530 - val_loss: 0.2003 - val_accuracy: 0.9476 - lr: 1.0000e-06\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9528Restoring model weights from the end of the best epoch: 40.\n",
      "313/313 [==============================] - 120s 383ms/step - loss: 0.1307 - accuracy: 0.9528 - val_loss: 0.2003 - val_accuracy: 0.9475 - lr: 1.0000e-06\n",
      "Epoch 50: early stopping\n",
      "\n",
      "Model saved \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Reshape, Dense,\n",
    "                                     BatchNormalization, Dropout, Embedding, MultiHeadAttention,\n",
    "                                     LayerNormalization, RandomRotation, RandomTranslation)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# --- Config ---\n",
    "# IMPORTANT: Update these paths to your actual data directories\n",
    "TRAIN_DIR = r\"D:\\OCR_MATH\\mathwriting-2024\\train\"\n",
    "VALID_DIR = r\"D:\\OCR_MATH\\mathwriting-2024\\valid\"\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "\n",
    "# CPU-friendly training defaults (tune when on GPU)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100             # CHANGED: Set high, EarlyStopping will find the best epoch count\n",
    "MAX_OUTPUT_TOKENS = 120\n",
    "EMBED_DIM = 256\n",
    "TRANSFORMER_DIM = 256\n",
    "NUM_HEADS = 4\n",
    "NUM_DEC_LAYERS = 2\n",
    "DFF = 512\n",
    "DROPOUT_RATE = 0.3       # CHANGED: Increased dropout for more regularization\n",
    "VOCAB_OOV_TOKEN = \"<UNK>\"\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "# CHANGED: Increased subset sizes for a more meaningful test.\n",
    "# For best results, comment these out to use the full dataset.\n",
    "TRAIN_SUBSET = 5000\n",
    "VALID_SUBSET = 500\n",
    "\n",
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "# --- InkML Processing Functions ---\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx-minx, maxy-miny\n",
    "    width, height = (width if width > 0 else 1), (height if height > 0 else 1)\n",
    "    scale = (IMG_SIZE - 2*PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx)*scale + PADDING, (y - miny)*scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x-STROKE_WIDTH, y-STROKE_WIDTH, x+STROKE_WIDTH, y+STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i-1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# --- Tokenization ---\n",
    "token_pattern = re.compile(r\"(\\\\[A-Za-z]+)|([{}^_])|(\\d+\\.\\d+|\\d+)|([A-Za-z]+)|([+\\-*/=(),\\[\\].:;%])|(\\\\.)|(\\S)\", re.VERBOSE)\n",
    "\n",
    "def tokenize_latex_lite(s):\n",
    "    if not s: return []\n",
    "    return [m.group(0) for m in token_pattern.finditer(s)]\n",
    "\n",
    "def normalize_label_for_tokenizer(label):\n",
    "    if not label or label.strip() == \"\":\n",
    "        return PAD_TOKEN\n",
    "    tokens = tokenize_latex_lite(label)\n",
    "    return f\"{START_TOKEN} \" + \" \".join(tokens) + f\" {END_TOKEN}\"\n",
    "\n",
    "def build_file_label_list(folder):\n",
    "    files, labels = [], []\n",
    "    for fp in glob(os.path.join(folder, \"*.inkml\")):\n",
    "        try:\n",
    "            tree = ET.parse(fp)\n",
    "            root = tree.getroot()\n",
    "            ann = root.find(XML_NS + \"annotation[@type='normalizedLabel']\")\n",
    "            label = ann.text.strip() if ann is not None and ann.text else \"\"\n",
    "        except Exception:\n",
    "            label = \"\"\n",
    "        files.append(fp)\n",
    "        labels.append(normalize_label_for_tokenizer(label))\n",
    "    return files, labels\n",
    "\n",
    "# --- Load filelists and subset ---\n",
    "train_files, train_labels = build_file_label_list(TRAIN_DIR)\n",
    "valid_files, valid_labels = build_file_label_list(VALID_DIR)\n",
    "\n",
    "if TRAIN_SUBSET > 0:\n",
    "    train_files, train_labels = train_files[:TRAIN_SUBSET], train_labels[:TRAIN_SUBSET]\n",
    "if VALID_SUBSET > 0:\n",
    "    valid_files, valid_labels = valid_files[:VALID_SUBSET], valid_labels[:VALID_SUBSET]\n",
    "\n",
    "print(f\"Train size: {len(train_files)}, Valid size: {len(valid_files)}\")\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token=VOCAB_OOV_TOKEN, split=' ')\n",
    "tokenizer.fit_on_texts([PAD_TOKEN, START_TOKEN, END_TOKEN] + train_labels + valid_labels)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v:k for k,v in word_index.items()}\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer saved to tokenizer.pkl\")\n",
    "\n",
    "# --- tf.data Pipeline ---\n",
    "def encode_label_to_sequence(label):\n",
    "    seq = tokenizer.texts_to_sequences([label])[0]\n",
    "    if len(seq) < MAX_OUTPUT_TOKENS:\n",
    "        seq = seq + [word_index.get(PAD_TOKEN, 0)] * (MAX_OUTPUT_TOKENS - len(seq))\n",
    "    else:\n",
    "        seq = seq[:MAX_OUTPUT_TOKENS]\n",
    "    return np.array(seq, dtype=np.int32)\n",
    "\n",
    "def gen_example(file_path, label):\n",
    "    img = inkml_to_image_array(file_path)\n",
    "    img = np.expand_dims(img, -1).astype(np.float32)\n",
    "    seq = encode_label_to_sequence(label)\n",
    "    return img, seq\n",
    "\n",
    "def tf_load_and_preprocess(path, label):\n",
    "    img, seq = tf.py_function(func=lambda p, l: gen_example(p.numpy().decode('utf-8'), l.numpy().decode('utf-8')),\n",
    "                              inp=[path, label], Tout=[tf.float32, tf.int32])\n",
    "    img.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "    seq.set_shape((MAX_OUTPUT_TOKENS,))\n",
    "    return img, seq\n",
    "\n",
    "def make_training_dataset(files, labels, batch=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(files), 1000))\n",
    "    ds = ds.map(tf_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    def prepare(img, seq):\n",
    "        target = seq[1:]\n",
    "        decoder_input = seq[:-1]\n",
    "        pad_token_id = word_index.get(PAD_TOKEN, 0)\n",
    "        decoder_input = tf.pad(decoder_input, [[0, 1]], constant_values=pad_token_id)\n",
    "        target = tf.pad(target, [[0, 1]], constant_values=pad_token_id)\n",
    "        return (img, decoder_input), target\n",
    "\n",
    "    ds = ds.map(prepare, num_parallel_calls=AUTOTUNE).batch(batch).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_tf = make_training_dataset(train_files, train_labels, batch=BATCH_SIZE)\n",
    "valid_ds_tf = make_training_dataset(valid_files, valid_labels, batch=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Transformer Model Definition ---\n",
    "def transformer_decoder_layer(d_model, num_heads, dff, dropout_rate, name=\"transformer_decoder_layer\"):\n",
    "    inputs_seq = Input(shape=(None, d_model))\n",
    "    enc_outputs = Input(shape=(None, d_model))\n",
    "    attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(inputs_seq, inputs_seq, attention_mask=None)\n",
    "    attn1 = Dropout(dropout_rate)(attn1)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs_seq + attn1)\n",
    "    attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(out1, enc_outputs)\n",
    "    attn2 = Dropout(dropout_rate)(attn2)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "    ffn = Dense(dff, activation='relu')(out2)\n",
    "    ffn = Dense(d_model)(ffn)\n",
    "    ffn = Dropout(dropout_rate)(ffn)\n",
    "    output = LayerNormalization(epsilon=1e-6)(out2 + ffn)\n",
    "    return Model([inputs_seq, enc_outputs], output, name=name)\n",
    "\n",
    "def build_transformer_model():\n",
    "    img_input = Input(shape=(IMG_SIZE, IMG_SIZE, 1), name=\"img_input\")\n",
    "\n",
    "    # NEW: Data Augmentation Layers to fight overfitting\n",
    "    x = RandomRotation(factor=0.03, fill_mode='constant', fill_value=0.0)(img_input)\n",
    "    x = RandomTranslation(height_factor=0.03, width_factor=0.03, fill_mode='constant', fill_value=0.0)(x)\n",
    "\n",
    "    x = Conv2D(32, (3,3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    x = Conv2D(64,(3,3),padding='same',activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    x = Conv2D(128,(3,3),padding='same',activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    pool_h, pool_w = IMG_SIZE//8, IMG_SIZE//8\n",
    "    x = Reshape((pool_h, pool_w*128))(x)\n",
    "    enc_outputs = Dense(TRANSFORMER_DIM, activation='tanh', name=\"enc_proj\")(x)\n",
    "\n",
    "    dec_inputs = Input(shape=(MAX_OUTPUT_TOKENS,), name=\"dec_inputs\")\n",
    "    embedding_layer = Embedding(vocab_size, EMBED_DIM, mask_zero=True, name=\"token_embedding\")\n",
    "    dec_emb = embedding_layer(dec_inputs)\n",
    "\n",
    "    if EMBED_DIM != TRANSFORMER_DIM:\n",
    "        dec_emb = Dense(TRANSFORMER_DIM,name=\"dec_emb_proj\")(dec_emb)\n",
    "\n",
    "    xdec = dec_emb\n",
    "    for i in range(NUM_DEC_LAYERS):\n",
    "        layer_name = f\"transformer_decoder_layer_{i}\"\n",
    "        layer = transformer_decoder_layer(TRANSFORMER_DIM, NUM_HEADS, DFF, DROPOUT_RATE, name=layer_name)\n",
    "        xdec = layer([xdec, enc_outputs])\n",
    "\n",
    "    outputs = Dense(vocab_size, activation='softmax', name=\"outputs\")(xdec)\n",
    "    model = Model([img_input, dec_inputs], outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "transformer_model = build_transformer_model()\n",
    "transformer_model.summary()\n",
    "\n",
    "# --- Training ---\n",
    "# NEW: Callbacks for smarter, more efficient training\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-6)\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "transformer_model.fit(train_ds_tf,\n",
    "                      validation_data=valid_ds_tf,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=[early_stopper, lr_reducer])\n",
    "\n",
    "transformer_model.save(\"math_ocrr.keras\") # Using modern .keras format\n",
    "print(\"\\nModel saved \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f0f1fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model and tokenizer...\n",
      "Loading complete.\n",
      "Generating and displaying input image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Input File:    0a3ea013f7fc601b.inkml\n",
      "Predicted LaTeX: \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt # CHANGED: Added matplotlib import\n",
    "\n",
    "# --- Configuration ---\n",
    "# These must match the settings used during training\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "MAX_OUTPUT_TOKENS = 120\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "# Paths to the saved model and tokenizer\n",
    "MODEL_PATH = \"D:\\OCR_MATH\\sample\\math_ocrr.keras\"\n",
    "TOKENIZER_PATH = \"tokenizer.pkl\"\n",
    "\n",
    "# --- InkML Processing Functions (Copied from training script) ---\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx - minx, maxy - miny\n",
    "    width, height = (width if width > 0 else 1), (height if height > 0 else 1)\n",
    "    scale = (IMG_SIZE - 2 * PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx) * scale + PADDING, (y - miny) * scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x - STROKE_WIDTH, y - STROKE_WIDTH, x + STROKE_WIDTH, y + STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i - 1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # Invert so strokes are 1s and background is 0\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# --- Prediction Function ---\n",
    "def predict_from_inkml(model, tokenizer, inkml_path):\n",
    "    # (This function remains unchanged)\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    start_idx = tokenizer.word_index.get(\"<START>\", 1)\n",
    "    end_idx = tokenizer.word_index.get(\"<END>\", 2)\n",
    "    pad_idx = tokenizer.word_index.get(\"<PAD>\", 0)\n",
    "    img_array = inkml_to_image_array(inkml_path)\n",
    "    img_tensor = np.expand_dims(img_array, axis=0)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=-1)\n",
    "    decoder_input_seq = [start_idx] + [pad_idx] * (MAX_OUTPUT_TOKENS - 1)\n",
    "    for pos in range(MAX_OUTPUT_TOKENS - 1):\n",
    "        decoder_input_tensor = np.array([decoder_input_seq])\n",
    "        preds = model.predict([img_tensor, decoder_input_tensor], verbose=0)\n",
    "        next_id = int(np.argmax(preds[0, pos, :]))\n",
    "        decoder_input_seq[pos + 1] = next_id\n",
    "        if next_id == end_idx:\n",
    "            break\n",
    "    tokens = [index_word.get(i, \"<UNK>\") for i in decoder_input_seq]\n",
    "    if tokens and tokens[0] == \"<START>\":\n",
    "        tokens = tokens[1:]\n",
    "    if \"<END>\" in tokens:\n",
    "        tokens = tokens[:tokens.index(\"<END>\")]\n",
    "    if \"<PAD>\" in tokens:\n",
    "        tokens = tokens[:tokens.index(\"<PAD>\")]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT ---\n",
    "    # Change this path to point to an InkML file you want to test\n",
    "    SAMPLE_INKML_PATH = r\"D:\\OCR_MATH\\sample\\0a3ea013f7fc601b.inkml\" # <--- CHANGE THIS\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH) or not os.path.exists(TOKENIZER_PATH):\n",
    "        print(\"Error: Model or tokenizer file not found.\")\n",
    "    elif not os.path.exists(SAMPLE_INKML_PATH):\n",
    "        print(f\"Error: Sample file not found at '{SAMPLE_INKML_PATH}'\")\n",
    "    else:\n",
    "        # Load the trained model and tokenizer\n",
    "        print(\"Loading trained model and tokenizer...\")\n",
    "        model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        with open(TOKENIZER_PATH, 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        print(\"Loading complete.\")\n",
    "\n",
    "        # --- CHANGED: CODE TO DISPLAY THE IMAGE ---\n",
    "        print(\"Generating and displaying input image...\")\n",
    "        # Convert the InkML to an image array\n",
    "        image_to_display = inkml_to_image_array(SAMPLE_INKML_PATH)\n",
    "        \n",
    "        # Use matplotlib to show the image\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image_to_display, cmap='gray') # Use a grayscale colormap\n",
    "        plt.title(f\"Input: {os.path.basename(SAMPLE_INKML_PATH)}\")\n",
    "        plt.axis('off') # Hide the axes\n",
    "        plt.show()\n",
    "        # --- END OF NEW CODE ---\n",
    "\n",
    "        # Make a prediction\n",
    "        predicted_latex = predict_from_inkml(model, tokenizer, SAMPLE_INKML_PATH)\n",
    "\n",
    "        # Display the result\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Input File:    {os.path.basename(SAMPLE_INKML_PATH)}\")\n",
    "        print(f\"Predicted LaTeX: {predicted_latex}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
