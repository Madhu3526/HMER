{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38c5c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense,\n",
    "                                     Embedding, LSTM, TimeDistributed, Reshape,\n",
    "                                     Multiply, Activation, Concatenate)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8068e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./mathwriting-2024\"          # root where train/ valid/ test/ folders are located\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VALID_DIR = os.path.join(DATA_ROOT, \"valid\")\n",
    "TEST_DIR = os.path.join(DATA_ROOT, \"test\")  # optional\n",
    "\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "SHUFFLE_BUFFER = 2000\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 120   # max tokens for output sequences (incl <START>, <END>)\n",
    "EMBED_DIM = 128\n",
    "DEC_UNITS = 256\n",
    "ATTN_UNITS = 256\n",
    "VOCAB_OOV_TOKEN = \"<UNK>\"\n",
    "\n",
    "BEAM_WIDTH = 3            # for inference beam search\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd76f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inkml_traces(inkml_path):\n",
    "    \"\"\"Return list of traces; each trace is list of (x,y) floats.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        text = trace.text or \"\"\n",
    "        pts = []\n",
    "        for part in text.strip().split(','):\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "            coords = part.split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    x = float(coords[0])\n",
    "                    y = float(coords[1])\n",
    "                except:\n",
    "                    continue\n",
    "                pts.append((x, y))\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces, out_size=IMG_SIZE, padding=PADDING):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for (x, y) in tr]\n",
    "    ys = [y for tr in traces for (x, y) in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width = maxx - minx if maxx - minx != 0 else 1.0\n",
    "    height = maxy - miny if maxy - miny != 0 else 1.0\n",
    "    avail = out_size - 2 * padding\n",
    "    scale = avail / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx) * scale + padding, (y - miny) * scale + padding) for (x, y) in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces, size=IMG_SIZE, stroke_width=STROKE_WIDTH):\n",
    "    img = Image.new(\"L\", (size, size), color=255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x - stroke_width, y - stroke_width, x + stroke_width, y + stroke_width], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i - 1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=stroke_width)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # invert so strokes=1.0\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces, out_size=IMG_SIZE, padding=PADDING)\n",
    "    img = render_traces_to_image(norm, size=IMG_SIZE, stroke_width=STROKE_WIDTH)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4b1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"\"\"\n",
    "    (\\\\[A-Za-z]+)        |   # LaTeX command like \\frac\n",
    "    ([{}^_])             |   # braces and supers/subscript symbols\n",
    "    (\\d+\\.\\d+|\\d+)       |   # numbers\n",
    "    ([A-Za-z]+)          |   # letters (variables)\n",
    "    (<=|>=|==|!=|->|<-|\\\\in|\\\\notin) | # multi-char operators / some commands\n",
    "    ([+\\-*/=(),\\[\\].:;%]) |  # punctuation/operators\n",
    "    (\\\\.)                |   # other backslash escapes like \\{\n",
    "    (\\S)                     # any other non-whitespace char\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "def tokenize_latex_lite(s):\n",
    "    if s is None:\n",
    "        return []\n",
    "    s = s.strip()\n",
    "    if s == \"\":\n",
    "        return []\n",
    "    # Make sure backslashes are kept intact (avoid splitting like \"\\frac\" into \"\\\" and \"frac\")\n",
    "    tokens = []\n",
    "    for m in token_pattern.finditer(s):\n",
    "        tok = m.group(0)\n",
    "        tokens.append(tok)\n",
    "    return tokens\n",
    "\n",
    "def normalize_label_for_tokenizer(label):\n",
    "    if not label or label.strip() == \"\":\n",
    "        return \"<EMPTY>\"\n",
    "    # Some basic normalization: collapse spaces, keep backslashes\n",
    "    return \" \".join(tokenize_latex_lite(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c97f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files (this may take a few seconds)...\n",
      "Train samples: 229864, Valid samples: 15674\n"
     ]
    }
   ],
   "source": [
    "def build_file_label_list(folder):\n",
    "    files = []\n",
    "    labels = []\n",
    "    for fp in glob(os.path.join(folder, \"*.inkml\")):\n",
    "        try:\n",
    "            tree = ET.parse(fp)\n",
    "            root = tree.getroot()\n",
    "            ann = root.find(XML_NS + \"annotation[@type='normalizedLabel']\")\n",
    "            label = ann.text.strip() if ann is not None and ann.text else \"\"\n",
    "        except Exception:\n",
    "            label = \"\"\n",
    "        files.append(fp)\n",
    "        labels.append(normalize_label_for_tokenizer(label))\n",
    "    return files, labels\n",
    "\n",
    "print(\"Listing files (this may take a few seconds)...\")\n",
    "train_files, train_labels = build_file_label_list(TRAIN_DIR)\n",
    "valid_files, valid_labels = build_file_label_list(VALID_DIR)\n",
    "print(f\"Train samples: {len(train_files)}, Valid samples: {len(valid_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6366872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5700\n"
     ]
    }
   ],
   "source": [
    "all_texts = train_labels + valid_labels\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token=VOCAB_OOV_TOKEN, split=' ')\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be7c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_to_sequence(label):\n",
    "    seq = tokenizer.texts_to_sequences([label])[0]\n",
    "    seq = seq[:MAX_OUTPUT_TOKENS-1]  # reserve space for <END> if any\n",
    "    # add <START> and <END> if not present token-wise\n",
    "    # Our labels already include tokens; we ensure start/end\n",
    "    start_idx = word_index.get(\"<START>\")\n",
    "    end_idx = word_index.get(\"<END>\")\n",
    "    if start_idx is None:\n",
    "        # add markers if tokenizer didn't see them\n",
    "        seq = [word_index.get(VOCAB_OOV_TOKEN, 1)] + seq\n",
    "    else:\n",
    "        seq = [start_idx] + seq\n",
    "    if end_idx is None:\n",
    "        seq = seq + [word_index.get(VOCAB_OOV_TOKEN, 1)]\n",
    "    else:\n",
    "        seq = seq + [end_idx]\n",
    "    # pad to MAX_OUTPUT_TOKENS\n",
    "    seq = pad_sequences([seq], maxlen=MAX_OUTPUT_TOKENS, padding='post')[0]\n",
    "    return seq\n",
    "\n",
    "def gen_example(file_path, label):\n",
    "    # returns: (image array float32 [H,W,1], label sequence int32 length MAX_OUTPUT_TOKENS)\n",
    "    img = inkml_to_image_array(file_path)  # float32 [H,W]\n",
    "    img = np.expand_dims(img, -1).astype(np.float32)\n",
    "    seq = encode_label_to_sequence(label).astype(np.int32)\n",
    "    return img, seq\n",
    "\n",
    "# wrappers for tf.data\n",
    "def tf_load_and_preprocess(path, label):\n",
    "    # path: tf string; label: tf string\n",
    "    # Use tf.py_function to call Python preprocessing (InkML rasterization)\n",
    "    img, seq = tf.py_function(func=lambda p, l: gen_example(p.numpy().decode('utf-8'), l.numpy().decode('utf-8')),\n",
    "                              inp=[path, label],\n",
    "                              Tout=[tf.float32, tf.int32])\n",
    "    img.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "    seq.set_shape((MAX_OUTPUT_TOKENS,))\n",
    "    return img, seq\n",
    "\n",
    "def make_dataset(file_list, label_list, batch=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_list, label_list))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(file_list), SHUFFLE_BUFFER))\n",
    "    ds = ds.map(tf_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(train_files, train_labels, batch=BATCH_SIZE, shuffle=True)\n",
    "valid_ds = make_dataset(valid_files, valid_labels, batch=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef19a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OCR_MATH\\venv\\lib\\site-packages\\keras\\src\\layers\\layer.py:970: UserWarning: Layer 'attention' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"train_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"train_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ img_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ img_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_10    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_10… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_11    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">729,600</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_11… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ decoder_embeddin… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, │            │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5700</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,924,100</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ img_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m320\u001b[0m │ img_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_9[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_10    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_10… │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_11    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m729,600\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_11… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m,      │    \u001b[38;5;34m394,240\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention (\u001b[38;5;33mLambda\u001b[0m)  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, │            │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m5700\u001b[0m) │  \u001b[38;5;34m2,924,100\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,173,636</span> (15.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,173,636\u001b[0m (15.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,173,636</span> (15.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,173,636\u001b[0m (15.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_models(img_h=IMG_SIZE, img_w=IMG_SIZE, channels=1,\n",
    "                 vocab_size=vocab_size, embed_dim=EMBED_DIM,\n",
    "                 dec_units=DEC_UNITS, attn_units=ATTN_UNITS,\n",
    "                 max_out_tokens=MAX_OUTPUT_TOKENS):\n",
    "    from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Dense, Embedding, LSTM, Concatenate, Lambda\n",
    "    from tensorflow.keras.models import Model\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    # ---------------- Encoder ----------------\n",
    "    img_input = Input(shape=(img_h, img_w, channels), name='img_input')\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding='same')(img_input)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    shape = tf.keras.backend.int_shape(x)\n",
    "    Hf, Wf, Cf = shape[1], shape[2], shape[3]\n",
    "    feat_seq = Reshape((Hf*Wf, Cf))(x)\n",
    "    feat_proj = Dense(dec_units, activation='tanh')(feat_seq)\n",
    "    encoder_model = Model(img_input, feat_proj, name='encoder_model')\n",
    "\n",
    "    # ---------------- Decoder ----------------\n",
    "    decoder_inputs = Input(shape=(max_out_tokens,), name='decoder_inputs')\n",
    "    emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True, name='decoder_embedding')(decoder_inputs)\n",
    "\n",
    "    decoder_lstm = LSTM(dec_units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    lstm_outputs, _, _ = decoder_lstm(emb)  # LSTM handles zeros internally\n",
    "\n",
    "    # ---------------- Attention ----------------\n",
    "    W1 = Dense(attn_units, use_bias=False, name='attn_W1')\n",
    "    W2 = Dense(attn_units, use_bias=False, name='attn_W2')\n",
    "    V  = Dense(1, use_bias=False, name='attn_V')\n",
    "\n",
    "    def apply_attention(feat, dec_h):\n",
    "        w1 = W1(feat)\n",
    "        w2 = W2(dec_h)\n",
    "        w1_exp = tf.expand_dims(w1, axis=1)\n",
    "        w2_exp = tf.expand_dims(w2, axis=2)\n",
    "        score = V(tf.nn.tanh(w1_exp + w2_exp))\n",
    "        score = tf.squeeze(score, axis=-1)\n",
    "        attn_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attn_weights, feat)\n",
    "        return context, attn_weights\n",
    "\n",
    "    context_vectors, attn_weights = Lambda(lambda args: apply_attention(args[0], args[1]),\n",
    "                                           name='attention')([feat_proj, lstm_outputs])\n",
    "\n",
    "    concat = Concatenate(axis=-1)([lstm_outputs, context_vectors])\n",
    "    decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(concat)\n",
    "\n",
    "    full_model = Model([img_input, decoder_inputs], decoder_outputs, name='train_model')\n",
    "    full_model.compile(optimizer=Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    inference_layers = {\n",
    "        'W1': W1, 'W2': W2, 'V': V,\n",
    "        'decoder_embedding_layer': full_model.get_layer('decoder_embedding'),\n",
    "        'decoder_lstm_layer': decoder_lstm,\n",
    "        'decoder_dense_layer': decoder_dense\n",
    "    }\n",
    "\n",
    "    return full_model, encoder_model, inference_layers\n",
    "\n",
    "# Build models\n",
    "print(\"Building models...\")\n",
    "train_model, encoder_model, inference_layers = build_models()\n",
    "train_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c71555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_dataset(file_list, label_list, batch=BATCH_SIZE, shuffle=True):\n",
    "    ds_files = tf.data.Dataset.from_tensor_slices((file_list, label_list))\n",
    "    if shuffle:\n",
    "        ds_files = ds_files.shuffle(buffer_size=min(len(file_list), SHUFFLE_BUFFER))\n",
    "    \n",
    "    ds = ds_files.map(tf_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    # ds yields (img, seq)\n",
    "\n",
    "    def prepare_for_training(img, seq):\n",
    "        seq = tf.cast(seq, tf.int32)\n",
    "        decoder_input = seq[:-1]\n",
    "        decoder_target = seq[1:]\n",
    "\n",
    "        # pad to MAX_OUTPUT_TOKENS\n",
    "        decoder_input = tf.pad(decoder_input, [[0, MAX_OUTPUT_TOKENS - tf.shape(decoder_input)[0]]])\n",
    "        decoder_target = tf.pad(decoder_target, [[0, MAX_OUTPUT_TOKENS - tf.shape(decoder_target)[0]]])\n",
    "\n",
    "        decoder_target = tf.expand_dims(decoder_target, -1)  # shape: (MAX_OUTPUT_TOKENS, 1)\n",
    "        return (img, decoder_input), decoder_target\n",
    "\n",
    "    ds = ds.map(prepare_for_training, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Create datasets\n",
    "train_ds_tf = make_training_dataset(train_files, train_labels, batch=BATCH_SIZE, shuffle=True)\n",
    "valid_ds_tf = make_training_dataset(valid_files, valid_labels, batch=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10a20cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6642s\u001b[0m 2s/step - accuracy: 0.9030 - loss: 0.5091 - val_accuracy: 0.9301 - val_loss: 0.3434\n",
      "Epoch 2/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6581s\u001b[0m 2s/step - accuracy: 0.9316 - loss: 0.3354 - val_accuracy: 0.9385 - val_loss: 0.2940\n",
      "Epoch 3/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6576s\u001b[0m 2s/step - accuracy: 0.9389 - loss: 0.2928 - val_accuracy: 0.9424 - val_loss: 0.2718\n",
      "Epoch 4/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6589s\u001b[0m 2s/step - accuracy: 0.9423 - loss: 0.2710 - val_accuracy: 0.9443 - val_loss: 0.2600\n",
      "Epoch 5/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7314s\u001b[0m 2s/step - accuracy: 0.9443 - loss: 0.2574 - val_accuracy: 0.9456 - val_loss: 0.2515\n",
      "Epoch 6/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7659s\u001b[0m 2s/step - accuracy: 0.9458 - loss: 0.2473 - val_accuracy: 0.9466 - val_loss: 0.2450\n",
      "Epoch 7/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6730s\u001b[0m 2s/step - accuracy: 0.9471 - loss: 0.2390 - val_accuracy: 0.9477 - val_loss: 0.2394\n",
      "Epoch 8/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8055s\u001b[0m 2s/step - accuracy: 0.9484 - loss: 0.2318 - val_accuracy: 0.9484 - val_loss: 0.2350\n",
      "Epoch 9/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6654s\u001b[0m 2s/step - accuracy: 0.9495 - loss: 0.2252 - val_accuracy: 0.9491 - val_loss: 0.2302\n",
      "Epoch 10/10\n",
      "\u001b[1m3592/3592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7735s\u001b[0m 2s/step - accuracy: 0.9506 - loss: 0.2191 - val_accuracy: 0.9499 - val_loss: 0.2257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ff2c7c1840>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "train_model.fit(train_ds_tf, validation_data=valid_ds_tf, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41725fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to math ocr.h5\n"
     ]
    }
   ],
   "source": [
    "train_model.save(\"math_ocr.h5\")\n",
    "print(\"Model saved to math ocr.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d21c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to math_ocr.keras\n"
     ]
    }
   ],
   "source": [
    "# Save model using the .keras format (recommended)\n",
    "train_model.save(\"math_ocr.keras\")\n",
    "print(\"Model saved to math_ocr.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe94328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# -------------------- GPU setup --------------------\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"✅ GPU enabled for TensorFlow\")\n",
    "    except:\n",
    "        print(\"⚠️ Could not set GPU memory growth\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected, using CPU\")\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "MAX_OUTPUT_TOKENS = 120\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "# -------------------- Utility functions --------------------\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords)>=2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces: return []\n",
    "    xs = [x for tr in traces for x,y in tr]\n",
    "    ys = [y for tr in traces for x,y in tr]\n",
    "    minx,maxx = min(xs), max(xs)\n",
    "    miny,maxy = min(ys), max(ys)\n",
    "    width,height = maxx-minx,maxy-miny\n",
    "    width,height = width if width>0 else 1, height if height>0 else 1\n",
    "    scale = (IMG_SIZE-2*PADDING)/max(width,height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x-minx)*scale+PADDING, (y-miny)*scale+PADDING) for x,y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\",(IMG_SIZE,IMG_SIZE),255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr)==1:\n",
    "            x,y=tr[0]\n",
    "            draw.ellipse([x-STROKE_WIDTH,y-STROKE_WIDTH,x+STROKE_WIDTH,y+STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1,len(tr)):\n",
    "                x1,y1=tr[i-1]; x2,y2=tr[i]\n",
    "                draw.line([x1,y1,x2,y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32)/255.0\n",
    "    arr = 1.0 - arr\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE,IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# -------------------- Tokenizer setup --------------------\n",
    "# Use the same tokenizer used during training\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v:k for k,v in word_index.items()}\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "# -------------------- Load model --------------------\n",
    "model = load_model(\"math_ocr_improved.h5\", compile=False)\n",
    "print(\"✅ Model loaded successfully\")\n",
    "\n",
    "# -------------------- Inference function --------------------\n",
    "def predict_latex(inkml_path, max_tokens=MAX_OUTPUT_TOKENS):\n",
    "    img = inkml_to_image_array(inkml_path)\n",
    "    img = np.expand_dims(img,-1)  # (H,W,1)\n",
    "    img = np.expand_dims(img,0)   # (1,H,W,1) batch dimension\n",
    "\n",
    "    # Greedy decoding\n",
    "    start_token = word_index.get(\"<START>\",1)\n",
    "    end_token = word_index.get(\"<END>\",2)\n",
    "    seq = [start_token]\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        dec_input = pad_sequences([seq], maxlen=MAX_OUTPUT_TOKENS, padding='post')\n",
    "        preds = model.predict([img, dec_input], verbose=0)\n",
    "        next_id = np.argmax(preds[0,len(seq)-1,:])\n",
    "        seq.append(next_id)\n",
    "        if next_id==end_token:\n",
    "            break\n",
    "\n",
    "    tokens = [index_word.get(i,\"<UNK>\") for i in seq if i not in [start_token,end_token]]\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfec6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Test --------------------\n",
    "sample_file = \"./mathwriting-2024/test/sample.inkml\"  # change to your test file\n",
    "predicted_latex = predict_latex(sample_file)\n",
    "print(\"Predicted LaTeX:\", predicted_latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f0c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading pillow-11.3.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/7.0 MB 1.3 MB/s eta 0:00:06\n",
      "      --------------------------------------- 0.1/7.0 MB 1.8 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.4/7.0 MB 4.6 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.1/7.0 MB 7.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.8/7.0 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.4/7.0 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 2.9/7.0 MB 9.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 3.7/7.0 MB 10.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 4.4/7.0 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.0/7.0 MB 11.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 5.9/7.0 MB 12.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 6.4/7.0 MB 12.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.0/7.0 MB 12.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.0/7.0 MB 11.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-11.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fdf00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Reshape, Dense,\n",
    "                                     Embedding, LSTM, Concatenate, Attention)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0c8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- Config --------------------\n",
    "DATA_ROOT = \"./mathwriting-2024\"  # change to your dataset root\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VALID_DIR = os.path.join(DATA_ROOT, \"valid\")\n",
    "\n",
    "IMG_SIZE = 128\n",
    "PADDING = 8\n",
    "STROKE_WIDTH = 2\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 120   # includes START and END\n",
    "DECODER_SEQ_LEN = MAX_OUTPUT_TOKENS - 1  # decoder input length (without last target shift)\n",
    "EMBED_DIM = 128\n",
    "DEC_UNITS = 256\n",
    "ATTN_UNITS = 256\n",
    "VOCAB_OOV_TOKEN = \"<UNK>\"\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "XML_NS = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "# Special tokens\n",
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "EMPTY_TOKEN = \"<EMPTY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cbef6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 229864, Valid samples: 15674\n",
      "Vocab size: 5696\n",
      "Index START/END: 5694 5695\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Utility Functions --------------------\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        text = trace.text or \"\"\n",
    "        # some traces have newlines/spaces; split by commas\n",
    "        for part in text.strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx - minx, maxy - miny\n",
    "    width, height = (width if width > 0 else 1), (height if height > 0 else 1)\n",
    "    scale = (IMG_SIZE - 2 * PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx) * scale + PADDING, (y - miny) * scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x - STROKE_WIDTH, y - STROKE_WIDTH, x + STROKE_WIDTH, y + STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i - 1]; x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr  # invert: background 0, strokes 1\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# -------------------- Tokenizer / Tokenization --------------------\n",
    "token_pattern = re.compile(\n",
    "    r\"(\\\\[A-Za-z]+)|([{}^_])|(\\d+\\.\\d+|\\d+)|([A-Za-z]+)|([+\\-*/=(),\\[\\].:;%])|(\\\\.)|(\\S)\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "def tokenize_latex_lite(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return [m.group(0) for m in token_pattern.finditer(s)]\n",
    "\n",
    "def normalize_label_for_tokenizer(label):\n",
    "    if not label or label.strip() == \"\":\n",
    "        return EMPTY_TOKEN\n",
    "    return \" \".join(tokenize_latex_lite(label.strip()))\n",
    "\n",
    "def build_file_label_list(folder):\n",
    "    files, labels = [], []\n",
    "    for fp in glob(os.path.join(folder, \"*.inkml\")):\n",
    "        try:\n",
    "            tree = ET.parse(fp)\n",
    "            root = tree.getroot()\n",
    "            ann = root.find(XML_NS + \"annotation[@type='normalizedLabel']\")\n",
    "            label = ann.text.strip() if ann is not None and ann.text else \"\"\n",
    "        except Exception:\n",
    "            label = \"\"\n",
    "        files.append(fp)\n",
    "        labels.append(normalize_label_for_tokenizer(label))\n",
    "    return files, labels\n",
    "\n",
    "# -------------------- Load file lists --------------------\n",
    "train_files, train_labels = build_file_label_list(TRAIN_DIR)\n",
    "valid_files, valid_labels = build_file_label_list(VALID_DIR)\n",
    "print(f\"Train samples: {len(train_files)}, Valid samples: {len(valid_files)}\")\n",
    "\n",
    "# Build tokenizer ensuring special tokens are included\n",
    "all_texts = [t if t and t.strip() else EMPTY_TOKEN for t in (train_labels + valid_labels)]\n",
    "# ensure START/END tokens included in tokenizer's vocabulary\n",
    "all_texts.append(START_TOKEN)\n",
    "all_texts.append(END_TOKEN)\n",
    "\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token=VOCAB_OOV_TOKEN, split=' ')\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Index START/END:\", word_index.get(START_TOKEN), word_index.get(END_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb636bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Encoding --------------------\n",
    "def encode_label_to_sequence(label):\n",
    "    if not label or label.strip() == \"\":\n",
    "        label = EMPTY_TOKEN\n",
    "    # convert to sequence of ids (body tokens)\n",
    "    body = tokenizer.texts_to_sequences([label])[0]\n",
    "    # reserve room for START and END\n",
    "    max_body = MAX_OUTPUT_TOKENS - 2\n",
    "    body = body[:max_body]\n",
    "    start_idx = word_index[START_TOKEN]\n",
    "    end_idx = word_index[END_TOKEN]\n",
    "    seq = [start_idx] + body + [end_idx]\n",
    "    # pad to MAX_OUTPUT_TOKENS\n",
    "    seq = pad_sequences([seq], maxlen=MAX_OUTPUT_TOKENS, padding='post', truncating='post')[0]\n",
    "    return seq.astype(np.int32)\n",
    "\n",
    "# -------------------- Dataset pipeline --------------------\n",
    "def gen_example(file_path, label):\n",
    "    img = inkml_to_image_array(file_path)\n",
    "    img = np.expand_dims(img, -1).astype(np.float32)\n",
    "    seq = encode_label_to_sequence(label)\n",
    "    return img, seq\n",
    "\n",
    "def tf_load_and_preprocess(path, label):\n",
    "    img, seq = tf.py_function(\n",
    "        func=lambda p, l: gen_example(p.decode('utf-8'), l.decode('utf-8')),\n",
    "        inp=[path, label],\n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    img.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "    seq.set_shape((MAX_OUTPUT_TOKENS,))\n",
    "    return img, seq\n",
    "\n",
    "def make_training_dataset(files, labels, batch=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(files), 2000))\n",
    "    ds = ds.map(tf_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    def prepare(img, seq):\n",
    "        # decoder input is seq[:-1], target is seq[1:]\n",
    "        decoder_input = seq[:-1]   # length MAX_OUTPUT_TOKENS-1\n",
    "        decoder_target = seq[1:]   # length MAX_OUTPUT_TOKENS-1\n",
    "        decoder_target = tf.expand_dims(decoder_target, -1)  # (timesteps, 1)\n",
    "        return (img, decoder_input), decoder_target\n",
    "\n",
    "    ds = ds.map(prepare, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_tf = make_training_dataset(train_files, train_labels)\n",
    "valid_ds_tf = make_training_dataset(valid_files, valid_labels, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ffd24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"full_seq_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " img_input (InputLayer)      [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 128, 128, 32)         320       ['img_input[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 32)           0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)           18496     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 64)           0         ['conv2d_1[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)          73856     ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, 119)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 128)          0         ['conv2d_2[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " embed (Embedding)           (None, 119, 128)             729088    ['decoder_inputs[0][0]']      \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 256, 128)             0         ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         [(None, 119, 256),           394240    ['embed[0][0]']               \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " feat_proj (Dense)           (None, 256, 256)             33024     ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " attention_layer (Attention  (None, 119, 256)             1         ['decoder_lstm[0][0]',        \n",
      " )                                                                   'feat_proj[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 119, 512)             0         ['decoder_lstm[0][0]',        \n",
      "                                                                     'attention_layer[0][0]']     \n",
      "                                                                                                  \n",
      " pred_dense (Dense)          (None, 119, 5696)            2922048   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4171073 (15.91 MB)\n",
      "Trainable params: 4171073 (15.91 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Model definition --------------------\n",
    "def build_models():\n",
    "    # Encoder\n",
    "    img_input = Input(shape=(IMG_SIZE, IMG_SIZE, 1), name=\"img_input\")\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(img_input)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Convert conv features to a sequence (encoder timesteps)\n",
    "    Hf, Wf, Cf = tf.keras.backend.int_shape(x)[1:]\n",
    "    feat_seq = Reshape((Hf * Wf, Cf))(x)  # (batch, enc_steps, Cf)\n",
    "    feat_proj = Dense(DEC_UNITS, activation='tanh', name=\"feat_proj\")(feat_seq)  # (batch, enc_steps, DEC_UNITS)\n",
    "\n",
    "    encoder_model = Model(img_input, feat_proj, name=\"encoder\")\n",
    "\n",
    "    # Decoder (training-time, teacher forcing)\n",
    "    decoder_inputs = Input(shape=(DECODER_SEQ_LEN,), name=\"decoder_inputs\")\n",
    "    emb = Embedding(vocab_size, EMBED_DIM, mask_zero=True, name=\"embed\")(decoder_inputs)\n",
    "    lstm_out, _, _ = LSTM(DEC_UNITS, return_sequences=True, return_state=True, name=\"decoder_lstm\")(emb)\n",
    "\n",
    "    # Attention: query=lstm_out (dec timesteps), value=feat_proj (enc timesteps)\n",
    "    attention = Attention(use_scale=True, name=\"attention_layer\")\n",
    "    context = attention([lstm_out, feat_proj])  # (batch, dec_steps, DEC_UNITS)\n",
    "    concat = Concatenate(axis=-1)([lstm_out, context])  # (batch, dec_steps, 2*DEC_UNITS)\n",
    "    outputs = Dense(vocab_size, activation='softmax', name=\"pred_dense\")(concat)  # (batch, dec_steps, vocab)\n",
    "\n",
    "    full_model = Model([img_input, decoder_inputs], outputs, name=\"full_seq_model\")\n",
    "    full_model.compile(optimizer=Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return full_model, encoder_model\n",
    "\n",
    "train_model, encoder_model = build_models()\n",
    "train_model.summary()\n",
    "\n",
    "# -------------------- Greedy inference utility --------------------\n",
    "# We'll use the training model as predictor: feed encoder image + current decoder_input sequence\n",
    "def greedy_decode(train_model, encoder_model, image_array, max_steps=DECODER_SEQ_LEN):\n",
    "    # image_array: (IMG_SIZE, IMG_SIZE) or (IMG_SIZE, IMG_SIZE, 1)\n",
    "    if image_array.ndim == 2:\n",
    "        image_array = np.expand_dims(image_array, -1)\n",
    "    enc_feats = encoder_model.predict(np.expand_dims(image_array, 0))  # (1, enc_steps, DEC_UNITS)\n",
    "\n",
    "    start_idx = word_index[START_TOKEN]\n",
    "    end_idx = word_index[END_TOKEN]\n",
    "\n",
    "    # decoder input initialized to zeros with start token first\n",
    "    dec_input = np.zeros((1, DECODER_SEQ_LEN), dtype=np.int32)\n",
    "    dec_input[0, 0] = start_idx\n",
    "\n",
    "    out_ids = []\n",
    "    for t in range(max_steps):\n",
    "        # get predictions for all decoder timesteps; we will look at timestep t\n",
    "        preds = train_model.predict([np.expand_dims(image_array, 0), dec_input], verbose=0)  # shape (1, dec_steps, vocab)\n",
    "        step_probs = preds[0, t]   # (vocab,)\n",
    "        next_id = int(np.argmax(step_probs))\n",
    "        if next_id == 0:\n",
    "            # 0 is padding; if model returns padding, break\n",
    "            break\n",
    "        if next_id == end_idx:\n",
    "            break\n",
    "        out_ids.append(next_id)\n",
    "        # put predicted token into decoder input for next step\n",
    "        if t + 1 < DECODER_SEQ_LEN:\n",
    "            dec_input[0, t + 1] = next_id\n",
    "    # map ids to tokens\n",
    "    out_tokens = [index_word.get(i, VOCAB_OOV_TOKEN) for i in out_ids]\n",
    "    return \" \".join(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9412849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: (5, 64, 64, 1)\n",
      "sequences: (5, 3)\n",
      "y_first: [2 2 2 2 2]\n",
      "1/1 [==============================] - 1s 763ms/step - loss: 2.7197\n",
      "✅ Tiny sanity check passed! Model runs with .inkml data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# --------------------------\n",
    "# 1. Parse InkML file → (image, label sequence)\n",
    "# --------------------------\n",
    "def parse_inkml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # extract strokes (mock: just random array for now)\n",
    "    img = np.random.rand(64, 64)  # you can replace this with real rendering\n",
    "\n",
    "    # extract labels (mock: suppose \"2+2\")\n",
    "    labels = [2, 10, 2]  # Example encoding (vocab indices)\n",
    "    return img, labels\n",
    "\n",
    "# --------------------------\n",
    "# 2. Load a few samples\n",
    "# --------------------------\n",
    "dataset_dir = \"sample\"\n",
    "files = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith(\".inkml\")]\n",
    "\n",
    "images, sequences = [], []\n",
    "for f in files[:5]:\n",
    "    img, seq = parse_inkml(f)\n",
    "    images.append(img)\n",
    "    sequences.append(seq)\n",
    "\n",
    "images = np.array(images)[..., np.newaxis]   # (batch, 64,64,1)\n",
    "\n",
    "# Pad sequences for batch\n",
    "max_len = max(len(s) for s in sequences)\n",
    "sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "print(\"images:\", images.shape)\n",
    "print(\"sequences:\", sequences.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Define vocabulary size (very important!)\n",
    "# --------------------------\n",
    "vocab_size = int(np.max(sequences)) + 1   # ensures labels fit in [0, vocab_size)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Build tiny CNN+Dense model\n",
    "# --------------------------\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(64,64,1)),\n",
    "    layers.Conv2D(16, 3, activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(vocab_size, activation='softmax')   # MATCH vocab size\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# --------------------------\n",
    "# 5. Train sanity check (only on first symbol of each sequence)\n",
    "# --------------------------\n",
    "y_first = sequences[:, 0]   # first token of each equation\n",
    "print(\"y_first:\", y_first)\n",
    "\n",
    "model.fit(images, y_first, epochs=1, verbose=1)\n",
    "\n",
    "print(\"✅ Tiny sanity check passed! Model runs with .inkml data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab93ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.6-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\ocr_math\\math\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\ocr_math\\math\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.1-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.3 MB 1.7 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.2/2.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.7/2.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.0/2.3 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.3/2.3 MB 4.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.6/2.3 MB 5.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.8/2.3 MB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.9/2.3 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.2/2.3 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.3/2.3 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\ocr_math\\math\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in d:\\ocr_math\\math\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ocr_math\\math\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.6 pyparsing-3.2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51e82ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in d:\\ocr_math\\math\\lib\\site-packages (from opencv-python) (2.2.6)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.12.0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fd248dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.25.2\n",
      "  Downloading numpy-1.25.2-cp310-cp310-win_amd64.whl (15.6 MB)\n",
      "     ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/15.6 MB 2.3 MB/s eta 0:00:07\n",
      "     - -------------------------------------- 0.4/15.6 MB 3.9 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.7/15.6 MB 4.6 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.8/15.6 MB 4.1 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 1.1/15.6 MB 4.3 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.2/15.6 MB 4.4 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.3/15.6 MB 3.9 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.5/15.6 MB 4.1 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/15.6 MB 3.7 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.7/15.6 MB 3.8 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.9/15.6 MB 3.5 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 2.0/15.6 MB 3.7 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 2.1/15.6 MB 3.4 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.4/15.6 MB 3.6 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.7/15.6 MB 3.8 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.9/15.6 MB 3.8 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 3.0/15.6 MB 4.0 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 3.3/15.6 MB 3.8 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.5/15.6 MB 3.9 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.9/15.6 MB 4.1 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.9/15.6 MB 4.1 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.9/15.6 MB 4.1 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 4.2/15.6 MB 3.9 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 4.3/15.6 MB 3.9 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 4.5/15.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.7/15.6 MB 3.9 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.7/15.6 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.9/15.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 5.3/15.6 MB 3.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 5.7/15.6 MB 4.0 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.9/15.6 MB 4.1 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 6.3/15.6 MB 4.2 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 6.5/15.6 MB 4.3 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.8/15.6 MB 4.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 7.3/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.5/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.9/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.9/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.2/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.2/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.6/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.6/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.9/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 9.1/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 9.3/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 9.5/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 9.7/15.6 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 10.0/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 10.4/15.6 MB 4.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 10.6/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 10.9/15.6 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 11.3/15.6 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 11.5/15.6 MB 4.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 12.0/15.6 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 12.2/15.6 MB 5.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 12.6/15.6 MB 5.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 13.0/15.6 MB 5.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 13.2/15.6 MB 5.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 13.6/15.6 MB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 14.0/15.6 MB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 14.3/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 14.7/15.6 MB 6.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.1/15.6 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.4/15.6 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.6/15.6 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.6/15.6 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.6/15.6 MB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.25.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.25.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.25.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a315dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "     ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/11.3 MB 991.0 kB/s eta 0:00:12\n",
      "     ---------------------------------------- 0.1/11.3 MB 1.4 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.4/11.3 MB 2.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.6/11.3 MB 3.5 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.6/11.3 MB 3.6 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.0/11.3 MB 3.6 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.2/11.3 MB 4.0 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.6/11.3 MB 4.4 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 1.8/11.3 MB 4.5 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.2/11.3 MB 4.9 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.5/11.3 MB 4.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.0/11.3 MB 5.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.2/11.3 MB 5.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.6/11.3 MB 5.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.0/11.3 MB 5.6 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.3/11.3 MB 5.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 4.6/11.3 MB 5.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 4.8/11.3 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.1/11.3 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 5.4/11.3 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 5.6/11.3 MB 5.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.0/11.3 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 6.3/11.3 MB 6.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.6/11.3 MB 6.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 6.9/11.3 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.3/11.3 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.5/11.3 MB 6.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.8/11.3 MB 6.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 8.3/11.3 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.6/11.3 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.1/11.3 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.4/11.3 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.5/11.3 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 9.9/11.3 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 10.1/11.3 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.6/11.3 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.8/11.3 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.2/11.3 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.3/11.3 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 11.3/11.3 MB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\ocr_math\\math\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ocr_math\\math\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ocr_math\\math\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "Collecting scipy>=1.8.0\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in d:\\ocr_math\\math\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\ocr_math\\math\\lib\\site-packages (11.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af3e8024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: colorama in d:\\ocr_math\\math\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47f3ebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "     ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/4.0 MB 871.5 kB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.2/4.0 MB 1.5 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.3/4.0 MB 2.2 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.6/4.0 MB 2.9 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.8/4.0 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.1/4.0 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 1.3/4.0 MB 4.0 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 1.6/4.0 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.9/4.0 MB 4.6 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 2.3/4.0 MB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.6/4.0 MB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 3.0/4.0 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.2/4.0 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 3.5/4.0 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.8/4.0 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.0/4.0 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.0/4.0 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-6.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9739b5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 64, 64, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 32, 32, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 64)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVec  (None, 50, 64)            0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50, 128)           98816     \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 50, 764)           98556     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216188 (844.48 KB)\n",
      "Trainable params: 216188 (844.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 9s 243ms/step - loss: 3.5528 - accuracy: 0.8810 - val_loss: 0.4483 - val_accuracy: 0.9568\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 4s 168ms/step - loss: 0.3782 - accuracy: 0.9577 - val_loss: 0.3752 - val_accuracy: 0.9568\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 5s 194ms/step - loss: 0.3570 - accuracy: 0.9577 - val_loss: 0.3564 - val_accuracy: 0.9568\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 7s 260ms/step - loss: 0.3372 - accuracy: 0.9577 - val_loss: 0.3488 - val_accuracy: 0.9568\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 6s 244ms/step - loss: 0.3288 - accuracy: 0.9577 - val_loss: 0.3452 - val_accuracy: 0.9568\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 0. Install dependencies (run once)\n",
    "# -------------------------------\n",
    "# !pip install lxml pillow numpy matplotlib tensorflow scikit-learn\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Imports\n",
    "# -------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lxml import etree\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "                                     RepeatVector, LSTM, Dense, TimeDistributed)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 128\n",
    "MAX_LEN = 50   # max token length\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Parse InkML & Extract Label\n",
    "# -------------------------------\n",
    "def parse_inkml(file_path):\n",
    "    tree = etree.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Extract strokes\n",
    "    traces = {}\n",
    "    for trace in root.findall('{http://www.w3.org/2003/InkML}trace'):\n",
    "        trace_id = trace.get(\"id\")\n",
    "        coords = []\n",
    "        for point in trace.text.strip().split(\",\"):\n",
    "            xy = point.strip().split(\" \")\n",
    "            if len(xy) >= 2:\n",
    "                coords.append((float(xy[0]), float(xy[1])))\n",
    "        traces[trace_id] = coords\n",
    "    \n",
    "    # Extract normalizedLabel\n",
    "    annotation = root.find('{http://www.w3.org/2003/InkML}annotation')\n",
    "    norm_label = annotation.text if annotation is not None else \"unknown\"\n",
    "    \n",
    "    return traces, norm_label\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Convert InkML → Image\n",
    "# -------------------------------\n",
    "def inkml_to_image(traces, img_size=IMG_SIZE):\n",
    "    img = Image.new(\"L\", (img_size, img_size), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    all_points = [pt for trace in traces.values() for pt in trace]\n",
    "    if not all_points:\n",
    "        return np.array(img)/255.0\n",
    "    \n",
    "    xs, ys = zip(*all_points)\n",
    "    min_x, max_x = min(xs), max(xs)\n",
    "    min_y, max_y = min(ys), max(ys)\n",
    "    \n",
    "    for trace in traces.values():\n",
    "        scaled = [((x-min_x)/(max_x-min_x+1e-5)*img_size,\n",
    "                   (y-min_y)/(max_y-min_y+1e-5)*img_size) for x, y in trace]\n",
    "        if len(scaled) > 1:\n",
    "            draw.line(scaled, fill=0, width=2)\n",
    "    return np.array(img)/255.0\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Load Dataset\n",
    "# -------------------------------\n",
    "def load_dataset(inkml_dir, max_samples=None):\n",
    "    X, Y = [], []\n",
    "    files = [f for f in os.listdir(inkml_dir) if f.endswith(\".inkml\")]\n",
    "    if max_samples:\n",
    "        files = files[:max_samples]\n",
    "    for file in files:\n",
    "        traces, label = parse_inkml(os.path.join(inkml_dir, file))\n",
    "        img = inkml_to_image(traces)\n",
    "        X.append(img)\n",
    "        Y.append(label)\n",
    "    return np.array(X), Y\n",
    "\n",
    "data_dir = r\"D:\\OCR_MATH\\mathwriting-2024\\train\"  # update to your dataset path\n",
    "X, Y = load_dataset(data_dir, max_samples=500)  # limit for testing\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Tokenize Labels\n",
    "# -------------------------------\n",
    "tokenizer = Tokenizer(char_level=False, filters='', oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(Y)\n",
    "Y_seq = tokenizer.texts_to_sequences(Y)\n",
    "Y_seq = pad_sequences(Y_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "# Convert X to 4D tensor\n",
    "X = np.expand_dims(X, -1)\n",
    "\n",
    "# Split train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y_seq, test_size=0.2, random_state=42)\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "\n",
    "num_tokens = len(tokenizer.word_index) + 1\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Build CNN + LSTM Model\n",
    "# -------------------------------\n",
    "def build_cnn_lstm(img_size=IMG_SIZE, num_tokens=num_tokens, max_len=MAX_LEN):\n",
    "    inputs = Input(shape=(img_size, img_size,1))\n",
    "    \n",
    "    x = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    # Fix timesteps: GlobalAveragePooling + RepeatVector\n",
    "    x = GlobalAveragePooling2D()(x)   # (batch, 64)\n",
    "    x = RepeatVector(max_len)(x)       # (batch, MAX_LEN, 64)\n",
    "    \n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    outputs = TimeDistributed(Dense(num_tokens, activation=\"softmax\"))(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_cnn_lstm()\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Train Model\n",
    "# -------------------------------\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Plot Accuracy\n",
    "# -------------------------------\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df2613f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 532ms/step\n",
      "Predicted LaTeX: \n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Function to predict single InkML file\n",
    "# -------------------------------\n",
    "def predict_inkml(file_path, model, tokenizer, img_size=IMG_SIZE, max_len=MAX_LEN):\n",
    "    traces, _ = parse_inkml(file_path)\n",
    "    img = inkml_to_image(traces)\n",
    "    img = np.expand_dims(img, axis=(0,-1))  # shape: (1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    \n",
    "    pred = model.predict(img)  # (1, MAX_LEN, num_tokens)\n",
    "    pred_tokens = np.argmax(pred, axis=-1)[0]  # get token IDs\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    index_word = {v:k for k,v in tokenizer.word_index.items()}\n",
    "    index_word[0] = ''  # padding token\n",
    "    predicted_text = ''.join([index_word.get(tok, '') for tok in pred_tokens])\n",
    "    \n",
    "    return predicted_text\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Example Usage\n",
    "# -------------------------------\n",
    "test_file = r\"D:\\OCR_MATH\\sample\\0a6d07d9a838a060.inkml\"  # replace with a test InkML file\n",
    "predicted_label = predict_inkml(test_file, model, tokenizer)\n",
    "print(\"Predicted LaTeX:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ceb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Reshape, Dense, \n",
    "                                     BatchNormalization, Dropout, Embedding, MultiHeadAttention,\n",
    "                                     LayerNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- InkML Processing Functions ---\n",
    "def parse_inkml_traces(inkml_path):\n",
    "    try:\n",
    "        tree = ET.parse(inkml_path)\n",
    "        root = tree.getroot()\n",
    "    except:\n",
    "        return []\n",
    "    traces = []\n",
    "    for trace in root.findall(XML_NS + \"trace\"):\n",
    "        pts = []\n",
    "        for part in (trace.text or \"\").strip().split(','):\n",
    "            coords = part.strip().split()\n",
    "            if len(coords) >= 2:\n",
    "                try:\n",
    "                    pts.append((float(coords[0]), float(coords[1])))\n",
    "                except:\n",
    "                    continue\n",
    "        if pts:\n",
    "            traces.append(pts)\n",
    "    return traces\n",
    "\n",
    "def normalize_and_scale_traces(traces):\n",
    "    if not traces:\n",
    "        return []\n",
    "    xs = [x for tr in traces for x, y in tr]\n",
    "    ys = [y for tr in traces for x, y in tr]\n",
    "    minx, maxx = min(xs), max(xs)\n",
    "    miny, maxy = min(ys), max(ys)\n",
    "    width, height = maxx-minx, maxy-miny\n",
    "    width, height = width if width > 0 else 1, height if height > 0 else 1\n",
    "    scale = (IMG_SIZE - 2*PADDING) / max(width, height)\n",
    "    norm = []\n",
    "    for tr in traces:\n",
    "        pts = [((x - minx)*scale + PADDING, (y - miny)*scale + PADDING) for x, y in tr]\n",
    "        norm.append(pts)\n",
    "    return norm\n",
    "\n",
    "def render_traces_to_image(traces):\n",
    "    img = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for tr in traces:\n",
    "        if len(tr) == 1:\n",
    "            x, y = tr[0]\n",
    "            draw.ellipse([x-STROKE_WIDTH, y-STROKE_WIDTH, x+STROKE_WIDTH, y+STROKE_WIDTH], fill=0)\n",
    "        else:\n",
    "            for i in range(1, len(tr)):\n",
    "                x1, y1 = tr[i-1]\n",
    "                x2, y2 = tr[i]\n",
    "                draw.line([x1, y1, x2, y2], fill=0, width=STROKE_WIDTH)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = 1.0 - arr\n",
    "    return arr\n",
    "\n",
    "def inkml_to_image_array(path):\n",
    "    traces = parse_inkml_traces(path)\n",
    "    if not traces:\n",
    "        return np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    norm = normalize_and_scale_traces(traces)\n",
    "    return render_traces_to_image(norm)\n",
    "\n",
    "# --- Tokenization ---\n",
    "token_pattern = re.compile(r\"(\\[A-Za-z]+)|([{}^_])|(\\d+\\.\\d+|\\d+)|([A-Za-z]+)|([+\\-*/=(),\\[\\].:;%])|(\\.)|(\\S)\", re.VERBOSE)\n",
    "def tokenize_latex_lite(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return [m.group(0) for m in token_pattern.finditer(s)]\n",
    "\n",
    "def normalize_label_for_tokenizer(label):\n",
    "    if not label or label.strip() == \"\":\n",
    "        return PAD_TOKEN\n",
    "    return f\"{START_TOKEN} \" + \" \".join(tokenize_latex_lite(label)) + f\" {END_TOKEN}\"\n",
    "\n",
    "def build_file_label_list(folder):\n",
    "    files, labels = [], []\n",
    "    for fp in glob(os.path.join(folder, \"*.inkml\")):\n",
    "        try:\n",
    "            tree = ET.parse(fp)\n",
    "            root = tree.getroot()\n",
    "            ann = root.find(XML_NS + \"annotation[@type='normalizedLabel']\")\n",
    "            label = ann.text.strip() if ann is not None else \"\"\n",
    "        except:\n",
    "            label = \"\"\n",
    "        files.append(fp)\n",
    "        labels.append(normalize_label_for_tokenizer(label))\n",
    "    return files, labels\n",
    "\n",
    "train_files, train_labels = build_file_label_list(TRAIN_DIR)\n",
    "valid_files, valid_labels = build_file_label_list(VALID_DIR)\n",
    "\n",
    "train_files, train_labels = train_files[:TRAIN_SUBSET], train_labels[:TRAIN_SUBSET]\n",
    "valid_files, valid_labels = valid_files[:VALID_SUBSET], valid_labels[:VALID_SUBSET]\n",
    "\n",
    "print(f\"Train size: {len(train_files)}, Valid size: {len(valid_files)}\")\n",
    "\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token=VOCAB_OOV_TOKEN, split=' ')\n",
    "tokenizer.fit_on_texts([START_TOKEN, END_TOKEN, PAD_TOKEN] + train_labels + valid_labels)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "def encode_label_to_sequence(label):\n",
    "    seq = tokenizer.texts_to_sequences([label])[0]\n",
    "    if len(seq) < MAX_OUTPUT_TOKENS:\n",
    "        seq += [word_index[PAD_TOKEN]] * (MAX_OUTPUT_TOKENS - len(seq))\n",
    "    else:\n",
    "        seq = seq[:MAX_OUTPUT_TOKENS]\n",
    "    return np.array(seq).astype(np.int32)\n",
    "\n",
    "def gen_example(file_path, label):\n",
    "    img = inkml_to_image_array(file_path)\n",
    "    img = np.expand_dims(img, -1).astype(np.float32)\n",
    "    seq = encode_label_to_sequence(label)\n",
    "    return img, seq\n",
    "\n",
    "def tf_load_and_preprocess(path, label):\n",
    "    img, seq = tf.py_function(func=lambda p, l: gen_example(p.numpy().decode('utf-8'), l.numpy().decode('utf-8')),\n",
    "                              inp=[path, label],\n",
    "                              Tout=[tf.float32, tf.int32])\n",
    "    img.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "    seq.set_shape((MAX_OUTPUT_TOKENS,))\n",
    "    return img, seq\n",
    "\n",
    "def make_training_dataset(files, labels, batch=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(files), 1000))\n",
    "    ds = ds.map(tf_load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    def prepare(img, seq):\n",
    "        return (img, seq[:-1]), seq[1:]\n",
    "    ds = ds.map(prepare, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds_tf = make_training_dataset(train_files, train_labels)\n",
    "valid_ds_tf = make_training_dataset(valid_files, valid_labels, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformer Decoder Layer ---\n",
    "def transformer_decoder_layer(d_model, num_heads, dff, dropout_rate):\n",
    "    inputs_seq = Input(shape=(None, d_model))\n",
    "    enc_outputs = Input(shape=(None, d_model))\n",
    "\n",
    "    # Masked self-attention\n",
    "    attn1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(inputs_seq, inputs_seq)\n",
    "    attn1 = Dropout(dropout_rate)(attn1)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs_seq + attn1)\n",
    "\n",
    "    # Cross attention\n",
    "    attn2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(out1, enc_outputs)\n",
    "    attn2 = Dropout(dropout_rate)(attn2)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "\n",
    "    # Feed-forward\n",
    "    ffn = Dense(dff, activation='relu')(out2)\n",
    "    ffn = Dense(d_model)(ffn)\n",
    "    ffn = Dropout(dropout_rate)(ffn)\n",
    "    output = LayerNormalization(epsilon=1e-6)(out2 + ffn)\n",
    "\n",
    "    return Model([inputs_seq, enc_outputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Full Model ---\n",
    "def build_transformer_model():\n",
    "    img_input = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Reshape((IMG_SIZE // 8, (IMG_SIZE // 8) * 128))(x)\n",
    "    enc_outputs = Dense(TRANSFORMER_DIM, activation='tanh')(x)\n",
    "\n",
    "    dec_inputs = Input(shape=(MAX_OUTPUT_TOKENS,))\n",
    "    embedding_layer = Embedding(vocab_size, EMBED_DIM, mask_zero=True)\n",
    "    dec_emb = embedding_layer(dec_inputs)\n",
    "\n",
    "    xdec = dec_emb\n",
    "    for _ in range(NUM_DEC_LAYERS):\n",
    "        layer = transformer_decoder_layer(TRANSFORMER_DIM, NUM_HEADS, DFF, DROPOUT_RATE)\n",
    "        xdec = layer([xdec, enc_outputs])\n",
    "\n",
    "    outputs = Dense(vocab_size, activation='softmax')(xdec)\n",
    "\n",
    "    model = Model([img_input, dec_inputs], outputs)\n",
    "    model.compile(optimizer=Adam(1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36266ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate and train ---\n",
    "transformer_model = build_transformer_model()\n",
    "transformer_model.summary()\n",
    "transformer_model.fit(train_ds_tf, validation_data=valid_ds_tf, epochs=EPOCHS)\n",
    "transformer_model.save(\"math_ocr_transformer_final.h5\")\n",
    "print(\"Training complete. Model saved as 'math_ocr_final.h5'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
