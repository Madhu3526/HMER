{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "19CkVnVOMHEh"
      },
      "outputs": [],
      "source": [
        "# This command unzips your dataset from Drive into your Colab workspace\n",
        "# The '-q' flag makes it quiet (less verbose output)\n",
        "!unzip -q \"/content/drive/MyDrive/formula_images.zip\" -d \"/content/datasets/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess_input\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "EMBEDDING_DIM = 256\n",
        "UNITS = 512\n",
        "IMG_SIZE = (224, 224)\n",
        "EPOCHS = 5\n",
        "FINE_TUNE_EPOCHS = 3  # Additional epochs for fine-tuning\n",
        "FINE_TUNE_AT = 140  # Unfreeze layers from this point\n",
        "\n",
        "# --- 2. DATA LOADING ---\n",
        "TRAIN_IMG_DIR = '/content/datasets/formula_images/train/images'\n",
        "TRAIN_LABELS_FILE = '/content/datasets/formula_images/train/labels.txt'\n",
        "VALID_IMG_DIR = '/content/datasets/formula_images/valid/images'\n",
        "VALID_LABELS_FILE = '/content/datasets/formula_images/valid/labels.txt'\n",
        "\n",
        "def load_dataset_from_files(img_dir, labels_file):\n",
        "    image_paths, captions = [], []\n",
        "    with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                filename, caption = line.strip().split(',', 1)\n",
        "                image_paths.append(os.path.join(img_dir, filename))\n",
        "                # Add start and end tokens\n",
        "                captions.append(f'<start> {caption} <end>')\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return image_paths, captions\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "train_image_paths, train_captions = load_dataset_from_files(TRAIN_IMG_DIR, TRAIN_LABELS_FILE)\n",
        "valid_image_paths, valid_captions = load_dataset_from_files(VALID_IMG_DIR, VALID_LABELS_FILE)\n",
        "\n",
        "# --- 3. TEXT PREPARATION ---\n",
        "print(\"Tokenizing text...\")\n",
        "all_captions = train_captions + valid_captions\n",
        "max_length = max(len(t.split(\" \")) for t in all_captions)\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=None,\n",
        "    ragged=False,\n",
        "    output_sequence_length=max_length\n",
        ")\n",
        "tokenizer.adapt(all_captions)\n",
        "vocab_size = tokenizer.vocabulary_size()\n",
        "\n",
        "full_vocabulary = tokenizer.get_vocabulary()\n",
        "clean_vocabulary = full_vocabulary[2:]\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    vocabulary=clean_vocabulary,\n",
        "    invert=True,\n",
        "    oov_token='[UNK]',\n",
        "    mask_token=''\n",
        ")\n",
        "start_token_index = tokenizer.get_vocabulary().index('<start>')\n",
        "end_token_index = tokenizer.get_vocabulary().index('<end>')\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(f\"Max Sequence Length: {max_length}\")\n",
        "\n",
        "# --- 4. CREATE TF.DATA PIPELINE ---\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = resnet_preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def map_func(img_path, caption):\n",
        "    img = load_and_preprocess_image(img_path)\n",
        "    vectorized_caption = tokenizer(caption)\n",
        "    return img, vectorized_caption\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_captions))\\\n",
        "    .map(map_func, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "    .shuffle(BUFFER_SIZE)\\\n",
        "    .batch(BATCH_SIZE)\\\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_image_paths, valid_captions))\\\n",
        "    .map(map_func, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "    .batch(BATCH_SIZE)\\\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"\\nDataset pipelines created successfully!\")\n",
        "\n",
        "# --- 5. MODEL COMPONENTS ---\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, trainable=False):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.image_model = ResNet50V2(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(*IMG_SIZE, 3)\n",
        "        )\n",
        "        self.image_model.trainable = trainable\n",
        "\n",
        "        # Extract features from conv5_block3_out\n",
        "        feature_output_layer = self.image_model.get_layer('conv5_block3_out').output\n",
        "        self.reshape = tf.keras.layers.Reshape((-1, feature_output_layer.shape[-1]))\n",
        "\n",
        "        self.feature_extractor = tf.keras.Model(\n",
        "            inputs=self.image_model.input,\n",
        "            outputs=self.reshape(feature_output_layer)\n",
        "        )\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # Pass training parameter to maintain batch norm behavior\n",
        "        return self.feature_extractor(x, training=training)\n",
        "\n",
        "    def unfreeze_top_layers(self, from_layer=140):\n",
        "        \"\"\"Unfreeze top layers for fine-tuning\"\"\"\n",
        "        self.image_model.trainable = True\n",
        "        for layer in self.image_model.layers[:from_layer]:\n",
        "            layer.trainable = False\n",
        "        print(f\"Unfrozen layers from {from_layer} onwards\")\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            self.units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden, training=False):\n",
        "        context_vector, attention_weights = self.attention(features, hidden[0])\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.fc2(x)\n",
        "        return x, [state_h, state_c], attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return [tf.zeros((batch_size, self.units)) for _ in range(2)]\n",
        "\n",
        "# --- 6. INITIALIZE MODELS, OPTIMIZER, AND LOSS ---\n",
        "encoder = CNN_Encoder(trainable=False)\n",
        "decoder = RNN_Decoder(EMBEDDING_DIM, UNITS, vocab_size)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# --- 7. METRICS ---\n",
        "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
        "valid_loss_metric = tf.keras.metrics.Mean(name='valid_loss')\n",
        "\n",
        "# --- 8. CUSTOM TRAINING & VALIDATION LOOPS ---\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([start_token_index] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor, training=True)\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden, training=True)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = loss / int(target.shape[1])\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "@tf.function\n",
        "def validation_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([start_token_index] * target.shape[0], 1)\n",
        "    features = encoder(img_tensor, training=False)\n",
        "\n",
        "    for i in range(1, target.shape[1]):\n",
        "        predictions, hidden, _ = decoder(dec_input, features, hidden, training=False)\n",
        "        loss += loss_function(target[:, i], predictions)\n",
        "        dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    return loss / int(target.shape[1])\n",
        "\n",
        "# --- 9. PREDICTION FUNCTION WITH BEAM SEARCH ---\n",
        "def evaluate(image_path, encoder_model, decoder_model, max_len=None):\n",
        "    \"\"\"Generate caption using greedy decoding\"\"\"\n",
        "    if max_len is None:\n",
        "        max_len = max_length\n",
        "\n",
        "    img_tensor = load_and_preprocess_image(image_path)\n",
        "    features = encoder_model(tf.expand_dims(img_tensor, 0), training=False)\n",
        "    hidden = decoder_model.reset_state(batch_size=1)\n",
        "    dec_input = tf.expand_dims([start_token_index], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "        predictions, hidden, _ = decoder_model(dec_input, features, hidden, training=False)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        predicted_word = index_to_word(tf.constant([predicted_id])).numpy()[0].decode('utf-8')\n",
        "\n",
        "        if predicted_word == '<end>':\n",
        "            break\n",
        "        if predicted_word not in ['<start>', '[UNK]', '']:\n",
        "            result.append(predicted_word)\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# --- 10. MAIN EXECUTION BLOCK ---\n",
        "if __name__ == \"__main__\":\n",
        "    checkpoint_dir = './training_checkpoints'\n",
        "    checkpoint = tf.train.Checkpoint(\n",
        "        optimizer=optimizer,\n",
        "        encoder=encoder,\n",
        "        decoder=decoder\n",
        "    )\n",
        "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
        "\n",
        "    if manager.latest_checkpoint:\n",
        "        checkpoint.restore(manager.latest_checkpoint)\n",
        "        print(f'Restored from checkpoint: {manager.latest_checkpoint}')\n",
        "    else:\n",
        "        print('Initializing from scratch.')\n",
        "\n",
        "    # --- Phase 1: Train with frozen encoder ---\n",
        "    print(\"\\n=== PHASE 1: Training with frozen encoder ===\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "        train_loss_metric.reset_state()\n",
        "        valid_loss_metric.reset_state()\n",
        "\n",
        "        for batch, (img_tensor, target) in enumerate(train_dataset):\n",
        "            batch_loss = train_step(img_tensor, target)\n",
        "            train_loss_metric.update_state(batch_loss)\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss:.4f}')\n",
        "\n",
        "        for img_tensor, target in valid_dataset:\n",
        "            batch_loss = validation_step(img_tensor, target)\n",
        "            valid_loss_metric.update_state(batch_loss)\n",
        "\n",
        "        epoch_train_loss = train_loss_metric.result()\n",
        "        epoch_valid_loss = valid_loss_metric.result()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_train_loss:.6f} | Valid Loss: {epoch_valid_loss:.6f}')\n",
        "        manager.save()\n",
        "        print(f'Checkpoint saved | Time: {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "    # --- Phase 2: Fine-tune with unfrozen top layers ---\n",
        "    print(\"\\n=== PHASE 2: Fine-tuning with unfrozen encoder layers ===\")\n",
        "    encoder.unfreeze_top_layers(from_layer=FINE_TUNE_AT)\n",
        "    optimizer.learning_rate = 1e-5  # Lower learning rate for fine-tuning\n",
        "\n",
        "    for epoch in range(FINE_TUNE_EPOCHS):\n",
        "        start = time.time()\n",
        "        train_loss_metric.reset_state()\n",
        "        valid_loss_metric.reset_state()\n",
        "\n",
        "        for batch, (img_tensor, target) in enumerate(train_dataset):\n",
        "            batch_loss = train_step(img_tensor, target)\n",
        "            train_loss_metric.update_state(batch_loss)\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                print(f'Fine-tune Epoch {epoch+1} Batch {batch} Loss {batch_loss:.4f}')\n",
        "\n",
        "        for img_tensor, target in valid_dataset:\n",
        "            batch_loss = validation_step(img_tensor, target)\n",
        "            valid_loss_metric.update_state(batch_loss)\n",
        "\n",
        "        epoch_train_loss = train_loss_metric.result()\n",
        "        epoch_valid_loss = valid_loss_metric.result()\n",
        "\n",
        "        print(f'Fine-tune Epoch {epoch+1}/{FINE_TUNE_EPOCHS} | Train Loss: {epoch_train_loss:.6f} | Valid Loss: {epoch_valid_loss:.6f}')\n",
        "        manager.save()\n",
        "        print(f'Checkpoint saved | Time: {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # --- Test predictions ---\n",
        "    print(\"\\n=== Testing Predictions ===\")\n",
        "    test_images = valid_image_paths[:5]\n",
        "    for img_path in test_images:\n",
        "        prediction = evaluate(img_path, encoder, decoder)\n",
        "        print(f\"Image: {os.path.basename(img_path)}\")\n",
        "        print(f\"Prediction: {prediction}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0tUl4lQOxjk",
        "outputId": "38141b53-6695-42ef-91fb-281f7f4afdbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Tokenizing text...\n",
            "Vocabulary Size: 64456\n",
            "Max Sequence Length: 14\n",
            "\n",
            "Dataset pipelines created successfully!\n",
            "Initializing from scratch.\n",
            "\n",
            "=== PHASE 1: Training with frozen encoder ===\n",
            "Epoch 1 Batch 0 Loss 3.4542\n",
            "Epoch 1 Batch 50 Loss 1.4392\n",
            "Epoch 1 Batch 100 Loss 1.3640\n",
            "Epoch 1 Batch 150 Loss 1.0079\n",
            "Epoch 1 Batch 200 Loss 1.2770\n",
            "Epoch 1 Batch 250 Loss 1.0719\n",
            "Epoch 1 Batch 300 Loss 1.0466\n",
            "Epoch 1 Batch 350 Loss 0.9517\n",
            "Epoch 1 Batch 400 Loss 1.0150\n",
            "Epoch 1 Batch 450 Loss 0.9343\n",
            "Epoch 1 Batch 500 Loss 1.1574\n",
            "Epoch 1 Batch 550 Loss 1.0190\n",
            "Epoch 1 Batch 600 Loss 0.9572\n",
            "Epoch 1 Batch 650 Loss 0.9655\n",
            "Epoch 1 Batch 700 Loss 1.0713\n",
            "Epoch 1 Batch 750 Loss 0.9407\n",
            "Epoch 1 Batch 800 Loss 0.9063\n",
            "Epoch 1 Batch 850 Loss 0.9068\n",
            "Epoch 1 Batch 900 Loss 1.0507\n",
            "Epoch 1 Batch 950 Loss 1.0422\n",
            "Epoch 1 Batch 1000 Loss 0.8699\n",
            "Epoch 1 Batch 1050 Loss 1.0054\n",
            "Epoch 1 Batch 1100 Loss 1.0414\n",
            "Epoch 1 Batch 1150 Loss 0.9659\n",
            "Epoch 1 Batch 1200 Loss 1.0291\n",
            "Epoch 1 Batch 1250 Loss 0.8735\n",
            "Epoch 1 Batch 1300 Loss 1.0702\n",
            "Epoch 1 Batch 1350 Loss 1.0140\n",
            "Epoch 1 Batch 1400 Loss 0.9336\n",
            "Epoch 1 Batch 1450 Loss 0.9872\n",
            "Epoch 1 Batch 1500 Loss 0.8699\n",
            "Epoch 1 Batch 1550 Loss 1.0033\n",
            "Epoch 1 Batch 1600 Loss 0.9017\n",
            "Epoch 1 Batch 1650 Loss 0.8540\n",
            "Epoch 1 Batch 1700 Loss 0.8947\n",
            "Epoch 1 Batch 1750 Loss 0.8434\n",
            "Epoch 1 Batch 1800 Loss 0.7510\n",
            "Epoch 1 Batch 1850 Loss 0.9207\n",
            "Epoch 1 Batch 1900 Loss 0.9550\n",
            "Epoch 1 Batch 1950 Loss 0.8350\n",
            "Epoch 1 Batch 2000 Loss 0.9382\n",
            "Epoch 1 Batch 2050 Loss 0.9628\n",
            "Epoch 1 Batch 2100 Loss 0.8858\n",
            "Epoch 1 Batch 2150 Loss 0.7964\n",
            "Epoch 1 Batch 2200 Loss 0.8639\n",
            "Epoch 1 Batch 2250 Loss 0.8458\n",
            "Epoch 1 Batch 2300 Loss 0.8906\n",
            "Epoch 1 Batch 2350 Loss 0.9105\n",
            "Epoch 1 Batch 2400 Loss 0.8999\n",
            "Epoch 1 Batch 2450 Loss 0.8528\n",
            "Epoch 1 Batch 2500 Loss 0.8673\n",
            "Epoch 1 Batch 2550 Loss 0.8966\n",
            "Epoch 1 Batch 2600 Loss 0.8454\n",
            "Epoch 1 Batch 2650 Loss 0.9596\n",
            "Epoch 1 Batch 2700 Loss 1.0509\n",
            "Epoch 1 Batch 2750 Loss 0.9266\n",
            "Epoch 1 Batch 2800 Loss 0.9173\n",
            "Epoch 1 Batch 2850 Loss 0.7523\n",
            "Epoch 1 Batch 2900 Loss 0.8399\n",
            "Epoch 1 Batch 2950 Loss 0.8321\n",
            "Epoch 1 Batch 3000 Loss 0.9325\n",
            "Epoch 1 Batch 3050 Loss 0.9836\n",
            "Epoch 1 Batch 3100 Loss 0.8260\n",
            "Epoch 1 Batch 3150 Loss 0.8738\n",
            "Epoch 1 Batch 3200 Loss 0.8394\n",
            "Epoch 1 Batch 3250 Loss 0.9654\n",
            "Epoch 1 Batch 3300 Loss 0.8634\n",
            "Epoch 1 Batch 3350 Loss 0.8224\n",
            "Epoch 1 Batch 3400 Loss 0.9612\n",
            "Epoch 1 Batch 3450 Loss 0.8807\n",
            "Epoch 1 Batch 3500 Loss 0.8376\n",
            "Epoch 1 Batch 3550 Loss 0.8292\n",
            "Epoch 1 Batch 3600 Loss 0.8313\n",
            "Epoch 1 Batch 3650 Loss 0.8197\n",
            "Epoch 1 Batch 3700 Loss 0.9093\n",
            "Epoch 1 Batch 3750 Loss 0.9117\n",
            "Epoch 1 Batch 3800 Loss 0.9675\n",
            "Epoch 1 Batch 3850 Loss 0.8134\n",
            "Epoch 1 Batch 3900 Loss 0.8913\n",
            "Epoch 1 Batch 3950 Loss 0.7510\n",
            "Epoch 1 Batch 4000 Loss 0.9648\n",
            "Epoch 1 Batch 4050 Loss 0.7821\n",
            "Epoch 1 Batch 4100 Loss 0.8722\n",
            "Epoch 1 Batch 4150 Loss 0.8902\n",
            "Epoch 1 Batch 4200 Loss 0.9453\n",
            "Epoch 1 Batch 4250 Loss 0.7780\n",
            "Epoch 1 Batch 4300 Loss 1.0150\n",
            "Epoch 1 Batch 4350 Loss 0.8777\n",
            "Epoch 1 Batch 4400 Loss 0.7955\n",
            "Epoch 1 Batch 4450 Loss 0.7701\n",
            "Epoch 1 Batch 4500 Loss 0.8163\n",
            "Epoch 1 Batch 4550 Loss 1.1763\n",
            "Epoch 1 Batch 4600 Loss 0.8397\n",
            "Epoch 1 Batch 4650 Loss 0.8485\n",
            "Epoch 1 Batch 4700 Loss 0.9249\n",
            "Epoch 1 Batch 4750 Loss 0.8511\n",
            "Epoch 1 Batch 4800 Loss 0.8497\n",
            "Epoch 1 Batch 4850 Loss 0.8037\n",
            "Epoch 1 Batch 4900 Loss 0.7509\n",
            "Epoch 1 Batch 4950 Loss 0.8263\n",
            "Epoch 1 Batch 5000 Loss 0.9598\n",
            "Epoch 1 Batch 5050 Loss 0.9533\n",
            "Epoch 1 Batch 5100 Loss 0.7791\n",
            "Epoch 1 Batch 5150 Loss 0.8922\n",
            "Epoch 1 Batch 5200 Loss 0.7758\n",
            "Epoch 1 Batch 5250 Loss 0.8860\n",
            "Epoch 1 Batch 5300 Loss 0.8230\n",
            "Epoch 1 Batch 5350 Loss 0.7962\n",
            "Epoch 1 Batch 5400 Loss 0.7841\n",
            "Epoch 1 Batch 5450 Loss 0.6705\n",
            "Epoch 1 Batch 5500 Loss 0.8879\n",
            "Epoch 1 Batch 5550 Loss 0.7405\n",
            "Epoch 1 Batch 5600 Loss 0.9303\n",
            "Epoch 1 Batch 5650 Loss 0.7394\n",
            "Epoch 1 Batch 5700 Loss 0.8416\n",
            "Epoch 1 Batch 5750 Loss 0.8399\n",
            "Epoch 1 Batch 5800 Loss 0.7126\n",
            "Epoch 1 Batch 5850 Loss 0.8105\n",
            "Epoch 1 Batch 5900 Loss 0.7982\n",
            "Epoch 1 Batch 5950 Loss 0.9471\n",
            "Epoch 1 Batch 6000 Loss 0.8945\n",
            "Epoch 1 Batch 6050 Loss 0.7716\n",
            "Epoch 1 Batch 6100 Loss 0.7259\n",
            "Epoch 1 Batch 6150 Loss 0.8078\n",
            "Epoch 1 Batch 6200 Loss 0.8682\n",
            "Epoch 1 Batch 6250 Loss 0.7641\n",
            "Epoch 1 Batch 6300 Loss 0.8469\n",
            "Epoch 1 Batch 6350 Loss 0.8567\n",
            "Epoch 1 Batch 6400 Loss 0.8105\n",
            "Epoch 1 Batch 6450 Loss 0.7736\n",
            "Epoch 1 Batch 6500 Loss 0.8324\n",
            "Epoch 1 Batch 6550 Loss 0.8298\n",
            "Epoch 1 Batch 6600 Loss 0.8566\n",
            "Epoch 1 Batch 6650 Loss 0.7751\n",
            "Epoch 1 Batch 6700 Loss 0.8667\n",
            "Epoch 1 Batch 6750 Loss 0.7552\n",
            "Epoch 1 Batch 6800 Loss 0.9254\n",
            "Epoch 1 Batch 6850 Loss 0.7957\n",
            "Epoch 1 Batch 6900 Loss 0.8451\n",
            "Epoch 1 Batch 6950 Loss 0.6550\n",
            "Epoch 1 Batch 7000 Loss 0.7544\n",
            "Epoch 1 Batch 7050 Loss 0.7474\n",
            "Epoch 1 Batch 7100 Loss 0.8288\n",
            "Epoch 1 Batch 7150 Loss 0.7391\n",
            "Epoch 1/5 | Train Loss: 0.897422 | Valid Loss: 0.873220\n",
            "Checkpoint saved | Time: 1955.68 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.7876\n",
            "Epoch 2 Batch 50 Loss 0.7309\n",
            "Epoch 2 Batch 100 Loss 0.7601\n",
            "Epoch 2 Batch 150 Loss 0.7381\n",
            "Epoch 2 Batch 200 Loss 0.8041\n",
            "Epoch 2 Batch 250 Loss 0.7185\n",
            "Epoch 2 Batch 300 Loss 0.7351\n",
            "Epoch 2 Batch 350 Loss 0.7749\n",
            "Epoch 2 Batch 400 Loss 0.7547\n",
            "Epoch 2 Batch 450 Loss 0.8460\n",
            "Epoch 2 Batch 500 Loss 0.7664\n",
            "Epoch 2 Batch 550 Loss 0.7937\n",
            "Epoch 2 Batch 600 Loss 0.8352\n",
            "Epoch 2 Batch 650 Loss 0.6952\n",
            "Epoch 2 Batch 700 Loss 0.8506\n",
            "Epoch 2 Batch 750 Loss 0.6905\n",
            "Epoch 2 Batch 800 Loss 0.8131\n",
            "Epoch 2 Batch 850 Loss 0.8990\n",
            "Epoch 2 Batch 900 Loss 0.8142\n",
            "Epoch 2 Batch 950 Loss 0.8038\n",
            "Epoch 2 Batch 1000 Loss 0.6898\n",
            "Epoch 2 Batch 1050 Loss 0.8076\n",
            "Epoch 2 Batch 1100 Loss 0.8006\n",
            "Epoch 2 Batch 1150 Loss 0.6878\n",
            "Epoch 2 Batch 1200 Loss 0.8047\n",
            "Epoch 2 Batch 1250 Loss 0.6484\n",
            "Epoch 2 Batch 1300 Loss 0.7344\n",
            "Epoch 2 Batch 1350 Loss 0.6373\n",
            "Epoch 2 Batch 1400 Loss 0.6567\n",
            "Epoch 2 Batch 1450 Loss 0.6015\n",
            "Epoch 2 Batch 1500 Loss 0.7561\n",
            "Epoch 2 Batch 1550 Loss 0.6760\n",
            "Epoch 2 Batch 1600 Loss 0.6785\n",
            "Epoch 2 Batch 1650 Loss 0.8318\n",
            "Epoch 2 Batch 1700 Loss 0.6532\n",
            "Epoch 2 Batch 1750 Loss 0.6187\n",
            "Epoch 2 Batch 1800 Loss 0.7969\n",
            "Epoch 2 Batch 1850 Loss 0.7532\n",
            "Epoch 2 Batch 1900 Loss 0.9617\n",
            "Epoch 2 Batch 1950 Loss 0.7525\n",
            "Epoch 2 Batch 2000 Loss 0.8166\n",
            "Epoch 2 Batch 2050 Loss 0.6394\n",
            "Epoch 2 Batch 2100 Loss 0.8142\n",
            "Epoch 2 Batch 2150 Loss 0.7015\n",
            "Epoch 2 Batch 2200 Loss 0.7387\n",
            "Epoch 2 Batch 2250 Loss 0.7464\n",
            "Epoch 2 Batch 2300 Loss 0.6467\n",
            "Epoch 2 Batch 2350 Loss 0.9252\n",
            "Epoch 2 Batch 2400 Loss 0.6740\n",
            "Epoch 2 Batch 2450 Loss 0.7976\n",
            "Epoch 2 Batch 2500 Loss 0.6926\n",
            "Epoch 2 Batch 2550 Loss 0.7598\n",
            "Epoch 2 Batch 2600 Loss 0.8168\n",
            "Epoch 2 Batch 2650 Loss 0.8980\n",
            "Epoch 2 Batch 2700 Loss 0.7150\n",
            "Epoch 2 Batch 2750 Loss 0.7540\n",
            "Epoch 2 Batch 2800 Loss 0.8194\n",
            "Epoch 2 Batch 2850 Loss 0.6214\n",
            "Epoch 2 Batch 2900 Loss 0.7359\n",
            "Epoch 2 Batch 2950 Loss 0.7469\n",
            "Epoch 2 Batch 3000 Loss 0.7273\n",
            "Epoch 2 Batch 3050 Loss 0.7425\n",
            "Epoch 2 Batch 3100 Loss 0.7050\n",
            "Epoch 2 Batch 3150 Loss 0.7977\n",
            "Epoch 2 Batch 3200 Loss 0.9097\n",
            "Epoch 2 Batch 3250 Loss 0.7213\n",
            "Epoch 2 Batch 3300 Loss 0.6583\n",
            "Epoch 2 Batch 3350 Loss 0.7577\n",
            "Epoch 2 Batch 3400 Loss 0.6807\n",
            "Epoch 2 Batch 3450 Loss 0.6546\n",
            "Epoch 2 Batch 3500 Loss 0.7394\n",
            "Epoch 2 Batch 3550 Loss 0.7599\n",
            "Epoch 2 Batch 3600 Loss 0.7873\n",
            "Epoch 2 Batch 3650 Loss 0.7427\n",
            "Epoch 2 Batch 3700 Loss 0.7493\n",
            "Epoch 2 Batch 3750 Loss 0.8295\n",
            "Epoch 2 Batch 3800 Loss 0.8615\n",
            "Epoch 2 Batch 3850 Loss 0.6952\n",
            "Epoch 2 Batch 3900 Loss 0.6779\n",
            "Epoch 2 Batch 3950 Loss 0.7271\n",
            "Epoch 2 Batch 4000 Loss 0.7483\n",
            "Epoch 2 Batch 4050 Loss 0.7033\n",
            "Epoch 2 Batch 4100 Loss 0.7283\n",
            "Epoch 2 Batch 4150 Loss 0.6710\n",
            "Epoch 2 Batch 4200 Loss 0.7391\n",
            "Epoch 2 Batch 4250 Loss 0.7255\n",
            "Epoch 2 Batch 4300 Loss 0.7199\n",
            "Epoch 2 Batch 4350 Loss 0.6986\n",
            "Epoch 2 Batch 4400 Loss 0.8017\n",
            "Epoch 2 Batch 4450 Loss 0.7142\n",
            "Epoch 2 Batch 4500 Loss 0.9014\n",
            "Epoch 2 Batch 4550 Loss 0.8706\n",
            "Epoch 2 Batch 4600 Loss 0.7882\n",
            "Epoch 2 Batch 4650 Loss 0.6802\n",
            "Epoch 2 Batch 4700 Loss 0.7333\n",
            "Epoch 2 Batch 4750 Loss 0.8615\n",
            "Epoch 2 Batch 4800 Loss 0.7079\n",
            "Epoch 2 Batch 4850 Loss 0.6906\n",
            "Epoch 2 Batch 4900 Loss 0.6016\n",
            "Epoch 2 Batch 4950 Loss 0.8914\n",
            "Epoch 2 Batch 5000 Loss 0.7777\n",
            "Epoch 2 Batch 5050 Loss 0.7822\n",
            "Epoch 2 Batch 5100 Loss 0.7556\n",
            "Epoch 2 Batch 5150 Loss 0.7634\n",
            "Epoch 2 Batch 5200 Loss 0.7332\n",
            "Epoch 2 Batch 5250 Loss 0.7802\n",
            "Epoch 2 Batch 5300 Loss 0.7588\n",
            "Epoch 2 Batch 5350 Loss 0.7101\n",
            "Epoch 2 Batch 5400 Loss 0.7349\n",
            "Epoch 2 Batch 5450 Loss 0.7523\n",
            "Epoch 2 Batch 5500 Loss 0.7584\n",
            "Epoch 2 Batch 5550 Loss 0.7387\n",
            "Epoch 2 Batch 5600 Loss 0.7837\n",
            "Epoch 2 Batch 5650 Loss 0.7597\n",
            "Epoch 2 Batch 5700 Loss 0.6853\n",
            "Epoch 2 Batch 5750 Loss 0.7547\n",
            "Epoch 2 Batch 5800 Loss 0.8782\n",
            "Epoch 2 Batch 5850 Loss 0.6979\n",
            "Epoch 2 Batch 5900 Loss 0.7723\n",
            "Epoch 2 Batch 5950 Loss 0.6359\n",
            "Epoch 2 Batch 6000 Loss 0.7530\n",
            "Epoch 2 Batch 6050 Loss 0.6723\n",
            "Epoch 2 Batch 6100 Loss 0.8404\n",
            "Epoch 2 Batch 6150 Loss 0.6599\n",
            "Epoch 2 Batch 6200 Loss 0.7728\n",
            "Epoch 2 Batch 6250 Loss 0.8152\n",
            "Epoch 2 Batch 6300 Loss 0.6817\n",
            "Epoch 2 Batch 6350 Loss 0.7087\n",
            "Epoch 2 Batch 6400 Loss 0.8400\n",
            "Epoch 2 Batch 6450 Loss 0.6224\n",
            "Epoch 2 Batch 6500 Loss 0.8012\n",
            "Epoch 2 Batch 6550 Loss 0.7500\n",
            "Epoch 2 Batch 6600 Loss 0.7109\n",
            "Epoch 2 Batch 6650 Loss 0.7202\n",
            "Epoch 2 Batch 6700 Loss 0.7519\n",
            "Epoch 2 Batch 6750 Loss 0.7098\n",
            "Epoch 2 Batch 6800 Loss 0.8603\n",
            "Epoch 2 Batch 6850 Loss 0.6746\n",
            "Epoch 2 Batch 6900 Loss 0.7036\n",
            "Epoch 2 Batch 6950 Loss 0.8292\n",
            "Epoch 2 Batch 7000 Loss 0.7107\n",
            "Epoch 2 Batch 7050 Loss 0.6915\n",
            "Epoch 2 Batch 7100 Loss 0.6963\n",
            "Epoch 2 Batch 7150 Loss 0.7758\n",
            "Epoch 2/5 | Train Loss: 0.743340 | Valid Loss: 0.894833\n",
            "Checkpoint saved | Time: 1920.41 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.7588\n",
            "Epoch 3 Batch 50 Loss 0.6101\n",
            "Epoch 3 Batch 100 Loss 0.7737\n",
            "Epoch 3 Batch 150 Loss 0.6096\n",
            "Epoch 3 Batch 200 Loss 0.6603\n",
            "Epoch 3 Batch 250 Loss 0.7836\n",
            "Epoch 3 Batch 300 Loss 0.6999\n",
            "Epoch 3 Batch 350 Loss 0.6565\n",
            "Epoch 3 Batch 400 Loss 0.6885\n",
            "Epoch 3 Batch 450 Loss 0.6262\n",
            "Epoch 3 Batch 500 Loss 0.6428\n",
            "Epoch 3 Batch 550 Loss 0.6091\n",
            "Epoch 3 Batch 600 Loss 0.6393\n",
            "Epoch 3 Batch 650 Loss 0.7441\n",
            "Epoch 3 Batch 700 Loss 0.6687\n",
            "Epoch 3 Batch 750 Loss 0.6043\n",
            "Epoch 3 Batch 800 Loss 0.7906\n",
            "Epoch 3 Batch 850 Loss 0.6726\n",
            "Epoch 3 Batch 900 Loss 0.7496\n",
            "Epoch 3 Batch 950 Loss 0.7001\n",
            "Epoch 3 Batch 1000 Loss 0.6739\n",
            "Epoch 3 Batch 1050 Loss 0.6785\n",
            "Epoch 3 Batch 1100 Loss 0.7066\n",
            "Epoch 3 Batch 1150 Loss 0.6583\n",
            "Epoch 3 Batch 1200 Loss 0.7534\n",
            "Epoch 3 Batch 1250 Loss 0.7130\n",
            "Epoch 3 Batch 1300 Loss 0.6582\n",
            "Epoch 3 Batch 1350 Loss 0.6542\n",
            "Epoch 3 Batch 1400 Loss 0.5736\n",
            "Epoch 3 Batch 1450 Loss 0.7044\n",
            "Epoch 3 Batch 1500 Loss 0.6707\n",
            "Epoch 3 Batch 1550 Loss 0.6660\n",
            "Epoch 3 Batch 1600 Loss 0.6150\n",
            "Epoch 3 Batch 1650 Loss 0.6267\n",
            "Epoch 3 Batch 1700 Loss 0.7836\n",
            "Epoch 3 Batch 1750 Loss 0.7887\n",
            "Epoch 3 Batch 1800 Loss 0.6478\n",
            "Epoch 3 Batch 1850 Loss 0.7148\n",
            "Epoch 3 Batch 1900 Loss 0.7677\n",
            "Epoch 3 Batch 1950 Loss 0.6234\n",
            "Epoch 3 Batch 2000 Loss 0.7895\n",
            "Epoch 3 Batch 2050 Loss 0.7149\n",
            "Epoch 3 Batch 2100 Loss 0.5438\n",
            "Epoch 3 Batch 2150 Loss 0.6280\n",
            "Epoch 3 Batch 2200 Loss 0.6539\n",
            "Epoch 3 Batch 2250 Loss 0.6169\n",
            "Epoch 3 Batch 2300 Loss 0.7302\n",
            "Epoch 3 Batch 2350 Loss 0.7018\n",
            "Epoch 3 Batch 2400 Loss 0.6678\n",
            "Epoch 3 Batch 2450 Loss 0.6672\n",
            "Epoch 3 Batch 2500 Loss 0.6979\n",
            "Epoch 3 Batch 2550 Loss 0.5528\n",
            "Epoch 3 Batch 2600 Loss 0.6405\n",
            "Epoch 3 Batch 2650 Loss 0.8172\n",
            "Epoch 3 Batch 2700 Loss 0.6752\n",
            "Epoch 3 Batch 2750 Loss 0.7537\n",
            "Epoch 3 Batch 2800 Loss 0.7407\n",
            "Epoch 3 Batch 2850 Loss 0.7951\n",
            "Epoch 3 Batch 2900 Loss 0.8040\n",
            "Epoch 3 Batch 2950 Loss 0.7805\n",
            "Epoch 3 Batch 3000 Loss 0.8475\n",
            "Epoch 3 Batch 3050 Loss 0.5760\n",
            "Epoch 3 Batch 3100 Loss 0.7410\n",
            "Epoch 3 Batch 3150 Loss 0.5998\n",
            "Epoch 3 Batch 3200 Loss 0.7013\n",
            "Epoch 3 Batch 3250 Loss 0.6785\n",
            "Epoch 3 Batch 3300 Loss 0.6961\n",
            "Epoch 3 Batch 3350 Loss 0.6589\n",
            "Epoch 3 Batch 3400 Loss 0.6198\n",
            "Epoch 3 Batch 3450 Loss 0.5975\n",
            "Epoch 3 Batch 3500 Loss 0.6599\n",
            "Epoch 3 Batch 3550 Loss 0.5572\n",
            "Epoch 3 Batch 3600 Loss 0.8063\n",
            "Epoch 3 Batch 3650 Loss 0.5750\n",
            "Epoch 3 Batch 3700 Loss 0.8139\n",
            "Epoch 3 Batch 3750 Loss 0.7189\n",
            "Epoch 3 Batch 3800 Loss 0.7625\n",
            "Epoch 3 Batch 3850 Loss 0.6616\n",
            "Epoch 3 Batch 3900 Loss 0.7404\n",
            "Epoch 3 Batch 3950 Loss 0.7107\n",
            "Epoch 3 Batch 4000 Loss 0.6840\n",
            "Epoch 3 Batch 4050 Loss 0.6098\n",
            "Epoch 3 Batch 4100 Loss 0.8143\n",
            "Epoch 3 Batch 4150 Loss 0.6251\n",
            "Epoch 3 Batch 4200 Loss 0.7649\n",
            "Epoch 3 Batch 4250 Loss 0.6637\n",
            "Epoch 3 Batch 4300 Loss 0.7280\n",
            "Epoch 3 Batch 4350 Loss 0.6870\n",
            "Epoch 3 Batch 4400 Loss 0.7075\n",
            "Epoch 3 Batch 4450 Loss 0.5449\n",
            "Epoch 3 Batch 4500 Loss 0.6086\n",
            "Epoch 3 Batch 4550 Loss 0.5369\n",
            "Epoch 3 Batch 4600 Loss 0.6451\n",
            "Epoch 3 Batch 4650 Loss 0.6873\n",
            "Epoch 3 Batch 4700 Loss 0.6527\n",
            "Epoch 3 Batch 4750 Loss 0.7096\n",
            "Epoch 3 Batch 4800 Loss 0.6382\n",
            "Epoch 3 Batch 4850 Loss 0.6510\n",
            "Epoch 3 Batch 4900 Loss 0.6833\n",
            "Epoch 3 Batch 4950 Loss 0.5987\n",
            "Epoch 3 Batch 5000 Loss 0.6342\n",
            "Epoch 3 Batch 5050 Loss 0.7681\n",
            "Epoch 3 Batch 5100 Loss 0.6287\n",
            "Epoch 3 Batch 5150 Loss 0.6599\n",
            "Epoch 3 Batch 5200 Loss 0.6565\n",
            "Epoch 3 Batch 5250 Loss 0.8460\n",
            "Epoch 3 Batch 5300 Loss 0.7381\n",
            "Epoch 3 Batch 5350 Loss 0.6517\n",
            "Epoch 3 Batch 5400 Loss 0.5457\n",
            "Epoch 3 Batch 5450 Loss 0.5682\n",
            "Epoch 3 Batch 5500 Loss 0.6404\n",
            "Epoch 3 Batch 5550 Loss 0.6032\n",
            "Epoch 3 Batch 5600 Loss 0.6762\n",
            "Epoch 3 Batch 5650 Loss 0.6778\n",
            "Epoch 3 Batch 5700 Loss 0.6026\n",
            "Epoch 3 Batch 5750 Loss 0.6265\n",
            "Epoch 3 Batch 5800 Loss 0.6360\n",
            "Epoch 3 Batch 5850 Loss 0.6862\n",
            "Epoch 3 Batch 5900 Loss 0.6308\n",
            "Epoch 3 Batch 5950 Loss 0.6142\n",
            "Epoch 3 Batch 6000 Loss 0.7467\n",
            "Epoch 3 Batch 6050 Loss 0.5060\n",
            "Epoch 3 Batch 6100 Loss 0.7076\n",
            "Epoch 3 Batch 6150 Loss 0.6214\n",
            "Epoch 3 Batch 6200 Loss 0.5512\n",
            "Epoch 3 Batch 6250 Loss 0.6961\n",
            "Epoch 3 Batch 6300 Loss 0.5693\n",
            "Epoch 3 Batch 6350 Loss 0.6750\n",
            "Epoch 3 Batch 6400 Loss 0.5927\n",
            "Epoch 3 Batch 6450 Loss 0.6633\n",
            "Epoch 3 Batch 6500 Loss 0.5955\n",
            "Epoch 3 Batch 6550 Loss 0.5840\n",
            "Epoch 3 Batch 6600 Loss 0.8178\n",
            "Epoch 3 Batch 6650 Loss 0.5791\n",
            "Epoch 3 Batch 6700 Loss 0.7245\n",
            "Epoch 3 Batch 6750 Loss 0.7720\n",
            "Epoch 3 Batch 6800 Loss 0.7909\n",
            "Epoch 3 Batch 6850 Loss 0.6105\n",
            "Epoch 3 Batch 6900 Loss 0.6352\n",
            "Epoch 3 Batch 6950 Loss 0.6266\n",
            "Epoch 3 Batch 7000 Loss 0.7109\n",
            "Epoch 3 Batch 7050 Loss 0.5975\n",
            "Epoch 3 Batch 7100 Loss 0.6786\n",
            "Epoch 3 Batch 7150 Loss 0.5893\n",
            "Epoch 3/5 | Train Loss: 0.678199 | Valid Loss: 0.851559\n",
            "Checkpoint saved | Time: 1930.12 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6498\n",
            "Epoch 4 Batch 50 Loss 0.5633\n",
            "Epoch 4 Batch 100 Loss 0.6446\n",
            "Epoch 4 Batch 150 Loss 0.6083\n",
            "Epoch 4 Batch 200 Loss 0.5830\n",
            "Epoch 4 Batch 250 Loss 0.7275\n",
            "Epoch 4 Batch 300 Loss 0.6842\n",
            "Epoch 4 Batch 350 Loss 0.4590\n",
            "Epoch 4 Batch 400 Loss 0.5764\n",
            "Epoch 4 Batch 450 Loss 0.5447\n",
            "Epoch 4 Batch 500 Loss 0.6729\n",
            "Epoch 4 Batch 550 Loss 0.6647\n",
            "Epoch 4 Batch 600 Loss 0.5893\n",
            "Epoch 4 Batch 650 Loss 0.5143\n",
            "Epoch 4 Batch 700 Loss 0.6051\n",
            "Epoch 4 Batch 750 Loss 0.6094\n",
            "Epoch 4 Batch 800 Loss 0.5528\n",
            "Epoch 4 Batch 850 Loss 0.6792\n",
            "Epoch 4 Batch 900 Loss 0.6268\n",
            "Epoch 4 Batch 950 Loss 0.7769\n",
            "Epoch 4 Batch 1000 Loss 0.6067\n",
            "Epoch 4 Batch 1050 Loss 0.5935\n",
            "Epoch 4 Batch 1100 Loss 0.7156\n",
            "Epoch 4 Batch 1150 Loss 0.7095\n",
            "Epoch 4 Batch 1200 Loss 0.5884\n",
            "Epoch 4 Batch 1250 Loss 0.7628\n",
            "Epoch 4 Batch 1300 Loss 0.7805\n",
            "Epoch 4 Batch 1350 Loss 0.6230\n",
            "Epoch 4 Batch 1400 Loss 0.6564\n",
            "Epoch 4 Batch 1450 Loss 0.5151\n",
            "Epoch 4 Batch 1500 Loss 0.6449\n",
            "Epoch 4 Batch 1550 Loss 0.6525\n",
            "Epoch 4 Batch 1600 Loss 0.6012\n",
            "Epoch 4 Batch 1650 Loss 0.5606\n",
            "Epoch 4 Batch 1700 Loss 0.6764\n",
            "Epoch 4 Batch 1750 Loss 0.5143\n",
            "Epoch 4 Batch 1800 Loss 0.5659\n",
            "Epoch 4 Batch 1850 Loss 0.5557\n",
            "Epoch 4 Batch 1900 Loss 0.6680\n",
            "Epoch 4 Batch 1950 Loss 0.7020\n",
            "Epoch 4 Batch 2000 Loss 0.6827\n",
            "Epoch 4 Batch 2050 Loss 0.7797\n",
            "Epoch 4 Batch 2100 Loss 0.6148\n",
            "Epoch 4 Batch 2150 Loss 0.6763\n",
            "Epoch 4 Batch 2200 Loss 0.8073\n",
            "Epoch 4 Batch 2250 Loss 0.6544\n",
            "Epoch 4 Batch 2300 Loss 0.5329\n",
            "Epoch 4 Batch 2350 Loss 0.6772\n",
            "Epoch 4 Batch 2400 Loss 0.6069\n",
            "Epoch 4 Batch 2450 Loss 0.7408\n",
            "Epoch 4 Batch 2500 Loss 0.7073\n",
            "Epoch 4 Batch 2550 Loss 0.5894\n",
            "Epoch 4 Batch 2600 Loss 0.6506\n",
            "Epoch 4 Batch 2650 Loss 0.6202\n",
            "Epoch 4 Batch 2700 Loss 0.5621\n",
            "Epoch 4 Batch 2750 Loss 0.6397\n",
            "Epoch 4 Batch 2800 Loss 0.6458\n",
            "Epoch 4 Batch 2850 Loss 0.6223\n",
            "Epoch 4 Batch 2900 Loss 0.6831\n",
            "Epoch 4 Batch 2950 Loss 0.5884\n",
            "Epoch 4 Batch 3000 Loss 0.5917\n",
            "Epoch 4 Batch 3050 Loss 0.8011\n",
            "Epoch 4 Batch 3100 Loss 0.6547\n",
            "Epoch 4 Batch 3150 Loss 0.5544\n",
            "Epoch 4 Batch 3200 Loss 0.7364\n",
            "Epoch 4 Batch 3250 Loss 0.6104\n",
            "Epoch 4 Batch 3300 Loss 0.5772\n",
            "Epoch 4 Batch 3350 Loss 0.7416\n",
            "Epoch 4 Batch 3400 Loss 0.7060\n",
            "Epoch 4 Batch 3450 Loss 0.7166\n",
            "Epoch 4 Batch 3500 Loss 0.6756\n",
            "Epoch 4 Batch 3550 Loss 0.5637\n",
            "Epoch 4 Batch 3600 Loss 0.6217\n",
            "Epoch 4 Batch 3650 Loss 0.6676\n",
            "Epoch 4 Batch 3700 Loss 0.6872\n",
            "Epoch 4 Batch 3750 Loss 0.5511\n",
            "Epoch 4 Batch 3800 Loss 0.5573\n",
            "Epoch 4 Batch 3850 Loss 0.6738\n",
            "Epoch 4 Batch 3900 Loss 0.5980\n",
            "Epoch 4 Batch 3950 Loss 0.6047\n",
            "Epoch 4 Batch 4000 Loss 0.6641\n",
            "Epoch 4 Batch 4050 Loss 0.4978\n",
            "Epoch 4 Batch 4100 Loss 0.5428\n",
            "Epoch 4 Batch 4150 Loss 0.6027\n",
            "Epoch 4 Batch 4200 Loss 0.7855\n",
            "Epoch 4 Batch 4250 Loss 0.6419\n",
            "Epoch 4 Batch 4300 Loss 0.7661\n",
            "Epoch 4 Batch 4350 Loss 0.5720\n",
            "Epoch 4 Batch 4400 Loss 0.5667\n",
            "Epoch 4 Batch 4450 Loss 0.6967\n",
            "Epoch 4 Batch 4500 Loss 0.6291\n",
            "Epoch 4 Batch 4550 Loss 0.6632\n",
            "Epoch 4 Batch 4600 Loss 0.7058\n",
            "Epoch 4 Batch 4650 Loss 0.6054\n",
            "Epoch 4 Batch 4700 Loss 0.6722\n",
            "Epoch 4 Batch 4750 Loss 0.6569\n",
            "Epoch 4 Batch 4800 Loss 0.5304\n",
            "Epoch 4 Batch 4850 Loss 0.5995\n",
            "Epoch 4 Batch 4900 Loss 0.6582\n",
            "Epoch 4 Batch 4950 Loss 0.5435\n",
            "Epoch 4 Batch 5000 Loss 0.6237\n",
            "Epoch 4 Batch 5050 Loss 0.6757\n",
            "Epoch 4 Batch 5100 Loss 0.6755\n",
            "Epoch 4 Batch 5150 Loss 0.6277\n",
            "Epoch 4 Batch 5200 Loss 0.5540\n",
            "Epoch 4 Batch 5250 Loss 0.5993\n",
            "Epoch 4 Batch 5300 Loss 0.6404\n",
            "Epoch 4 Batch 5350 Loss 0.6505\n",
            "Epoch 4 Batch 5400 Loss 0.6403\n",
            "Epoch 4 Batch 5450 Loss 0.5563\n",
            "Epoch 4 Batch 5500 Loss 0.6314\n",
            "Epoch 4 Batch 5550 Loss 0.5601\n",
            "Epoch 4 Batch 5600 Loss 0.6705\n",
            "Epoch 4 Batch 5650 Loss 0.6021\n",
            "Epoch 4 Batch 5700 Loss 0.7599\n",
            "Epoch 4 Batch 5750 Loss 0.5669\n",
            "Epoch 4 Batch 5800 Loss 0.5625\n",
            "Epoch 4 Batch 5850 Loss 0.5569\n",
            "Epoch 4 Batch 5900 Loss 0.6270\n",
            "Epoch 4 Batch 5950 Loss 0.6980\n",
            "Epoch 4 Batch 6000 Loss 0.5696\n",
            "Epoch 4 Batch 6050 Loss 0.6277\n",
            "Epoch 4 Batch 6100 Loss 0.5850\n",
            "Epoch 4 Batch 6150 Loss 0.6210\n",
            "Epoch 4 Batch 6200 Loss 0.5465\n",
            "Epoch 4 Batch 6250 Loss 0.6416\n",
            "Epoch 4 Batch 6300 Loss 0.6177\n",
            "Epoch 4 Batch 6350 Loss 0.6016\n",
            "Epoch 4 Batch 6400 Loss 0.6626\n",
            "Epoch 4 Batch 6450 Loss 0.6197\n",
            "Epoch 4 Batch 6500 Loss 0.6117\n",
            "Epoch 4 Batch 6550 Loss 0.7214\n",
            "Epoch 4 Batch 6600 Loss 0.7091\n",
            "Epoch 4 Batch 6650 Loss 0.7930\n",
            "Epoch 4 Batch 6700 Loss 0.6232\n",
            "Epoch 4 Batch 6750 Loss 0.5706\n",
            "Epoch 4 Batch 6800 Loss 0.7202\n",
            "Epoch 4 Batch 6850 Loss 0.5696\n",
            "Epoch 4 Batch 6900 Loss 0.5870\n",
            "Epoch 4 Batch 6950 Loss 0.6125\n",
            "Epoch 4 Batch 7000 Loss 0.5395\n",
            "Epoch 4 Batch 7050 Loss 0.6769\n",
            "Epoch 4 Batch 7100 Loss 0.6549\n",
            "Epoch 4 Batch 7150 Loss 0.5413\n",
            "Epoch 4/5 | Train Loss: 0.630248 | Valid Loss: 0.877227\n",
            "Checkpoint saved | Time: 1932.36 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.7445\n",
            "Epoch 5 Batch 50 Loss 0.5635\n",
            "Epoch 5 Batch 100 Loss 0.5369\n",
            "Epoch 5 Batch 150 Loss 0.6580\n",
            "Epoch 5 Batch 200 Loss 0.6136\n",
            "Epoch 5 Batch 250 Loss 0.6961\n",
            "Epoch 5 Batch 300 Loss 0.6441\n",
            "Epoch 5 Batch 350 Loss 0.4837\n",
            "Epoch 5 Batch 400 Loss 0.5552\n",
            "Epoch 5 Batch 450 Loss 0.5847\n",
            "Epoch 5 Batch 500 Loss 0.4188\n",
            "Epoch 5 Batch 550 Loss 0.6291\n",
            "Epoch 5 Batch 600 Loss 0.7638\n",
            "Epoch 5 Batch 650 Loss 0.7090\n",
            "Epoch 5 Batch 700 Loss 0.6021\n",
            "Epoch 5 Batch 750 Loss 0.6142\n",
            "Epoch 5 Batch 800 Loss 0.5400\n",
            "Epoch 5 Batch 850 Loss 0.5778\n",
            "Epoch 5 Batch 900 Loss 0.4740\n",
            "Epoch 5 Batch 950 Loss 0.5203\n",
            "Epoch 5 Batch 1000 Loss 0.6777\n",
            "Epoch 5 Batch 1050 Loss 0.6663\n",
            "Epoch 5 Batch 1100 Loss 0.7321\n",
            "Epoch 5 Batch 1150 Loss 0.6043\n",
            "Epoch 5 Batch 1200 Loss 0.4397\n",
            "Epoch 5 Batch 1250 Loss 0.5620\n",
            "Epoch 5 Batch 1300 Loss 0.5778\n",
            "Epoch 5 Batch 1350 Loss 0.7277\n",
            "Epoch 5 Batch 1400 Loss 0.6156\n",
            "Epoch 5 Batch 1450 Loss 0.6632\n",
            "Epoch 5 Batch 1500 Loss 0.6259\n",
            "Epoch 5 Batch 1550 Loss 0.6436\n",
            "Epoch 5 Batch 1600 Loss 0.5406\n",
            "Epoch 5 Batch 1650 Loss 0.6103\n",
            "Epoch 5 Batch 1700 Loss 0.5248\n",
            "Epoch 5 Batch 1750 Loss 0.6932\n",
            "Epoch 5 Batch 1800 Loss 0.5619\n",
            "Epoch 5 Batch 1850 Loss 0.5455\n",
            "Epoch 5 Batch 1900 Loss 0.7001\n",
            "Epoch 5 Batch 1950 Loss 0.6026\n",
            "Epoch 5 Batch 2000 Loss 0.4694\n",
            "Epoch 5 Batch 2050 Loss 0.5071\n",
            "Epoch 5 Batch 2100 Loss 0.6618\n",
            "Epoch 5 Batch 2150 Loss 0.5941\n",
            "Epoch 5 Batch 2200 Loss 0.6057\n",
            "Epoch 5 Batch 2250 Loss 0.5532\n",
            "Epoch 5 Batch 2300 Loss 0.7023\n",
            "Epoch 5 Batch 2350 Loss 0.5702\n",
            "Epoch 5 Batch 2400 Loss 0.5864\n",
            "Epoch 5 Batch 2450 Loss 0.5691\n",
            "Epoch 5 Batch 2500 Loss 0.6951\n",
            "Epoch 5 Batch 2550 Loss 0.4750\n",
            "Epoch 5 Batch 2600 Loss 0.5099\n",
            "Epoch 5 Batch 2650 Loss 0.5760\n",
            "Epoch 5 Batch 2700 Loss 0.6744\n",
            "Epoch 5 Batch 2750 Loss 0.4996\n",
            "Epoch 5 Batch 2800 Loss 0.6611\n",
            "Epoch 5 Batch 2850 Loss 0.6031\n",
            "Epoch 5 Batch 2900 Loss 0.5743\n",
            "Epoch 5 Batch 2950 Loss 0.6742\n",
            "Epoch 5 Batch 3000 Loss 0.6617\n",
            "Epoch 5 Batch 3050 Loss 0.5668\n",
            "Epoch 5 Batch 3100 Loss 0.5644\n",
            "Epoch 5 Batch 3150 Loss 0.5113\n",
            "Epoch 5 Batch 3200 Loss 0.5950\n",
            "Epoch 5 Batch 3250 Loss 0.5140\n",
            "Epoch 5 Batch 3300 Loss 0.7031\n",
            "Epoch 5 Batch 3350 Loss 0.6303\n",
            "Epoch 5 Batch 3400 Loss 0.5538\n",
            "Epoch 5 Batch 3450 Loss 0.4811\n",
            "Epoch 5 Batch 3500 Loss 0.5529\n",
            "Epoch 5 Batch 3550 Loss 0.7310\n",
            "Epoch 5 Batch 3600 Loss 0.7049\n",
            "Epoch 5 Batch 3650 Loss 0.7109\n",
            "Epoch 5 Batch 3700 Loss 0.5698\n",
            "Epoch 5 Batch 3750 Loss 0.6034\n",
            "Epoch 5 Batch 3800 Loss 0.6007\n",
            "Epoch 5 Batch 3850 Loss 0.5901\n",
            "Epoch 5 Batch 3900 Loss 0.5727\n",
            "Epoch 5 Batch 3950 Loss 0.4706\n",
            "Epoch 5 Batch 4000 Loss 0.6456\n",
            "Epoch 5 Batch 4050 Loss 0.5426\n",
            "Epoch 5 Batch 4100 Loss 0.5026\n",
            "Epoch 5 Batch 4150 Loss 0.5717\n",
            "Epoch 5 Batch 4200 Loss 0.5684\n",
            "Epoch 5 Batch 4250 Loss 0.4968\n",
            "Epoch 5 Batch 4300 Loss 0.5102\n",
            "Epoch 5 Batch 4350 Loss 0.6616\n",
            "Epoch 5 Batch 4400 Loss 0.6106\n",
            "Epoch 5 Batch 4450 Loss 0.4834\n",
            "Epoch 5 Batch 4500 Loss 0.6241\n",
            "Epoch 5 Batch 4550 Loss 0.7144\n",
            "Epoch 5 Batch 4600 Loss 0.4298\n",
            "Epoch 5 Batch 4650 Loss 0.6101\n",
            "Epoch 5 Batch 4700 Loss 0.5877\n",
            "Epoch 5 Batch 4750 Loss 0.5797\n",
            "Epoch 5 Batch 4800 Loss 0.5801\n",
            "Epoch 5 Batch 4850 Loss 0.5473\n",
            "Epoch 5 Batch 4900 Loss 0.6491\n",
            "Epoch 5 Batch 4950 Loss 0.5968\n",
            "Epoch 5 Batch 5000 Loss 0.5835\n",
            "Epoch 5 Batch 5050 Loss 0.5466\n",
            "Epoch 5 Batch 5100 Loss 0.4963\n",
            "Epoch 5 Batch 5150 Loss 0.6232\n",
            "Epoch 5 Batch 5200 Loss 0.5920\n",
            "Epoch 5 Batch 5250 Loss 0.4735\n",
            "Epoch 5 Batch 5300 Loss 0.7186\n",
            "Epoch 5 Batch 5350 Loss 0.5998\n",
            "Epoch 5 Batch 5400 Loss 0.4776\n",
            "Epoch 5 Batch 5450 Loss 0.5362\n",
            "Epoch 5 Batch 5500 Loss 0.5156\n",
            "Epoch 5 Batch 5550 Loss 0.5599\n",
            "Epoch 5 Batch 5600 Loss 0.5517\n",
            "Epoch 5 Batch 5650 Loss 0.7952\n",
            "Epoch 5 Batch 5700 Loss 0.5752\n",
            "Epoch 5 Batch 5750 Loss 0.5613\n",
            "Epoch 5 Batch 5800 Loss 0.5633\n",
            "Epoch 5 Batch 5850 Loss 0.6130\n",
            "Epoch 5 Batch 5900 Loss 0.5974\n",
            "Epoch 5 Batch 5950 Loss 0.5373\n",
            "Epoch 5 Batch 6000 Loss 0.6554\n",
            "Epoch 5 Batch 6050 Loss 0.5430\n",
            "Epoch 5 Batch 6100 Loss 0.5873\n",
            "Epoch 5 Batch 6150 Loss 0.5863\n",
            "Epoch 5 Batch 6200 Loss 0.6583\n",
            "Epoch 5 Batch 6250 Loss 0.5685\n",
            "Epoch 5 Batch 6300 Loss 0.6238\n",
            "Epoch 5 Batch 6350 Loss 0.5525\n",
            "Epoch 5 Batch 6400 Loss 0.4943\n",
            "Epoch 5 Batch 6450 Loss 0.6260\n",
            "Epoch 5 Batch 6500 Loss 0.5529\n",
            "Epoch 5 Batch 6550 Loss 0.6232\n",
            "Epoch 5 Batch 6600 Loss 0.5601\n",
            "Epoch 5 Batch 6650 Loss 0.6220\n",
            "Epoch 5 Batch 6700 Loss 0.5663\n",
            "Epoch 5 Batch 6750 Loss 0.6439\n",
            "Epoch 5 Batch 6800 Loss 0.4886\n",
            "Epoch 5 Batch 6850 Loss 0.5065\n",
            "Epoch 5 Batch 6900 Loss 0.4810\n",
            "Epoch 5 Batch 6950 Loss 0.6281\n",
            "Epoch 5 Batch 7000 Loss 0.7287\n",
            "Epoch 5 Batch 7050 Loss 0.6560\n",
            "Epoch 5 Batch 7100 Loss 0.5381\n",
            "Epoch 5 Batch 7150 Loss 0.5235\n",
            "Epoch 5/5 | Train Loss: 0.591462 | Valid Loss: 0.858048\n",
            "Checkpoint saved | Time: 1921.18 sec\n",
            "\n",
            "\n",
            "=== PHASE 2: Fine-tuning with unfrozen encoder layers ===\n",
            "Unfrozen layers from 140 onwards\n",
            "Fine-tune Epoch 1 Batch 0 Loss 0.5500\n",
            "Fine-tune Epoch 1 Batch 50 Loss 0.6333\n",
            "Fine-tune Epoch 1 Batch 100 Loss 0.4871\n",
            "Fine-tune Epoch 1 Batch 150 Loss 0.4489\n",
            "Fine-tune Epoch 1 Batch 200 Loss 0.6113\n",
            "Fine-tune Epoch 1 Batch 250 Loss 0.6121\n",
            "Fine-tune Epoch 1 Batch 300 Loss 0.6085\n",
            "Fine-tune Epoch 1 Batch 350 Loss 0.5465\n",
            "Fine-tune Epoch 1 Batch 400 Loss 0.5191\n",
            "Fine-tune Epoch 1 Batch 450 Loss 0.5759\n",
            "Fine-tune Epoch 1 Batch 500 Loss 0.7088\n",
            "Fine-tune Epoch 1 Batch 550 Loss 0.6004\n",
            "Fine-tune Epoch 1 Batch 600 Loss 0.4551\n",
            "Fine-tune Epoch 1 Batch 650 Loss 0.5850\n",
            "Fine-tune Epoch 1 Batch 700 Loss 0.5900\n",
            "Fine-tune Epoch 1 Batch 750 Loss 0.6435\n",
            "Fine-tune Epoch 1 Batch 800 Loss 0.4465\n",
            "Fine-tune Epoch 1 Batch 850 Loss 0.4831\n",
            "Fine-tune Epoch 1 Batch 900 Loss 0.6312\n",
            "Fine-tune Epoch 1 Batch 950 Loss 0.4458\n",
            "Fine-tune Epoch 1 Batch 1000 Loss 0.6049\n",
            "Fine-tune Epoch 1 Batch 1050 Loss 0.5193\n",
            "Fine-tune Epoch 1 Batch 1100 Loss 0.5579\n",
            "Fine-tune Epoch 1 Batch 1150 Loss 0.5444\n",
            "Fine-tune Epoch 1 Batch 1200 Loss 0.4752\n",
            "Fine-tune Epoch 1 Batch 1250 Loss 0.6237\n",
            "Fine-tune Epoch 1 Batch 1300 Loss 0.6378\n",
            "Fine-tune Epoch 1 Batch 1350 Loss 0.5562\n",
            "Fine-tune Epoch 1 Batch 1400 Loss 0.5872\n",
            "Fine-tune Epoch 1 Batch 1450 Loss 0.5726\n",
            "Fine-tune Epoch 1 Batch 1500 Loss 0.5523\n",
            "Fine-tune Epoch 1 Batch 1550 Loss 0.4873\n",
            "Fine-tune Epoch 1 Batch 1600 Loss 0.4542\n",
            "Fine-tune Epoch 1 Batch 1650 Loss 0.4812\n",
            "Fine-tune Epoch 1 Batch 1700 Loss 0.5466\n",
            "Fine-tune Epoch 1 Batch 1750 Loss 0.5206\n",
            "Fine-tune Epoch 1 Batch 1800 Loss 0.5716\n",
            "Fine-tune Epoch 1 Batch 1850 Loss 0.6390\n",
            "Fine-tune Epoch 1 Batch 1900 Loss 0.5656\n",
            "Fine-tune Epoch 1 Batch 1950 Loss 0.6282\n",
            "Fine-tune Epoch 1 Batch 2000 Loss 0.5658\n",
            "Fine-tune Epoch 1 Batch 2050 Loss 0.5370\n",
            "Fine-tune Epoch 1 Batch 2100 Loss 0.4926\n",
            "Fine-tune Epoch 1 Batch 2150 Loss 0.4699\n",
            "Fine-tune Epoch 1 Batch 2200 Loss 0.5679\n",
            "Fine-tune Epoch 1 Batch 2250 Loss 0.5370\n",
            "Fine-tune Epoch 1 Batch 2300 Loss 0.5106\n",
            "Fine-tune Epoch 1 Batch 2350 Loss 0.4636\n",
            "Fine-tune Epoch 1 Batch 2400 Loss 0.6125\n",
            "Fine-tune Epoch 1 Batch 2450 Loss 0.4395\n",
            "Fine-tune Epoch 1 Batch 2500 Loss 0.5956\n",
            "Fine-tune Epoch 1 Batch 2550 Loss 0.6391\n",
            "Fine-tune Epoch 1 Batch 2600 Loss 0.5722\n",
            "Fine-tune Epoch 1 Batch 2650 Loss 0.6277\n",
            "Fine-tune Epoch 1 Batch 2700 Loss 0.4895\n",
            "Fine-tune Epoch 1 Batch 2750 Loss 0.5472\n",
            "Fine-tune Epoch 1 Batch 2800 Loss 0.4952\n",
            "Fine-tune Epoch 1 Batch 2850 Loss 0.5061\n",
            "Fine-tune Epoch 1 Batch 2900 Loss 0.7041\n",
            "Fine-tune Epoch 1 Batch 2950 Loss 0.6888\n",
            "Fine-tune Epoch 1 Batch 3000 Loss 0.4914\n",
            "Fine-tune Epoch 1 Batch 3050 Loss 0.4026\n",
            "Fine-tune Epoch 1 Batch 3100 Loss 0.4355\n",
            "Fine-tune Epoch 1 Batch 3150 Loss 0.5458\n",
            "Fine-tune Epoch 1 Batch 3200 Loss 0.5042\n",
            "Fine-tune Epoch 1 Batch 3250 Loss 0.6140\n",
            "Fine-tune Epoch 1 Batch 3300 Loss 0.4973\n",
            "Fine-tune Epoch 1 Batch 3350 Loss 0.5497\n",
            "Fine-tune Epoch 1 Batch 3400 Loss 0.5845\n",
            "Fine-tune Epoch 1 Batch 3450 Loss 0.5944\n",
            "Fine-tune Epoch 1 Batch 3500 Loss 0.4665\n",
            "Fine-tune Epoch 1 Batch 3550 Loss 0.5259\n",
            "Fine-tune Epoch 1 Batch 3600 Loss 0.5845\n",
            "Fine-tune Epoch 1 Batch 3650 Loss 0.5995\n",
            "Fine-tune Epoch 1 Batch 3700 Loss 0.5413\n",
            "Fine-tune Epoch 1 Batch 3750 Loss 0.5651\n",
            "Fine-tune Epoch 1 Batch 3800 Loss 0.5704\n",
            "Fine-tune Epoch 1 Batch 3850 Loss 0.4394\n",
            "Fine-tune Epoch 1 Batch 3900 Loss 0.5401\n",
            "Fine-tune Epoch 1 Batch 3950 Loss 0.5385\n",
            "Fine-tune Epoch 1 Batch 4000 Loss 0.8180\n",
            "Fine-tune Epoch 1 Batch 4050 Loss 0.6858\n",
            "Fine-tune Epoch 1 Batch 4100 Loss 0.4605\n",
            "Fine-tune Epoch 1 Batch 4150 Loss 0.4923\n",
            "Fine-tune Epoch 1 Batch 4200 Loss 0.5249\n",
            "Fine-tune Epoch 1 Batch 4250 Loss 0.5792\n",
            "Fine-tune Epoch 1 Batch 4300 Loss 0.7555\n",
            "Fine-tune Epoch 1 Batch 4350 Loss 0.5075\n",
            "Fine-tune Epoch 1 Batch 4400 Loss 0.4914\n",
            "Fine-tune Epoch 1 Batch 4450 Loss 0.4728\n",
            "Fine-tune Epoch 1 Batch 4500 Loss 0.6735\n",
            "Fine-tune Epoch 1 Batch 4550 Loss 0.5839\n",
            "Fine-tune Epoch 1 Batch 4600 Loss 0.5865\n",
            "Fine-tune Epoch 1 Batch 4650 Loss 0.5672\n",
            "Fine-tune Epoch 1 Batch 4700 Loss 0.6273\n",
            "Fine-tune Epoch 1 Batch 4750 Loss 0.4891\n",
            "Fine-tune Epoch 1 Batch 4800 Loss 0.4133\n",
            "Fine-tune Epoch 1 Batch 4850 Loss 0.7265\n",
            "Fine-tune Epoch 1 Batch 4900 Loss 0.5048\n",
            "Fine-tune Epoch 1 Batch 4950 Loss 0.5745\n",
            "Fine-tune Epoch 1 Batch 5000 Loss 0.6939\n",
            "Fine-tune Epoch 1 Batch 5050 Loss 0.5038\n",
            "Fine-tune Epoch 1 Batch 5100 Loss 0.5225\n",
            "Fine-tune Epoch 1 Batch 5150 Loss 0.6587\n",
            "Fine-tune Epoch 1 Batch 5200 Loss 0.5113\n",
            "Fine-tune Epoch 1 Batch 5250 Loss 0.5221\n",
            "Fine-tune Epoch 1 Batch 5300 Loss 0.7031\n",
            "Fine-tune Epoch 1 Batch 5350 Loss 0.4699\n",
            "Fine-tune Epoch 1 Batch 5400 Loss 0.5253\n",
            "Fine-tune Epoch 1 Batch 5450 Loss 0.4963\n",
            "Fine-tune Epoch 1 Batch 5500 Loss 0.5148\n",
            "Fine-tune Epoch 1 Batch 5550 Loss 0.5555\n",
            "Fine-tune Epoch 1 Batch 5600 Loss 0.4915\n",
            "Fine-tune Epoch 1 Batch 5650 Loss 0.4174\n",
            "Fine-tune Epoch 1 Batch 5700 Loss 0.6062\n",
            "Fine-tune Epoch 1 Batch 5750 Loss 0.6001\n",
            "Fine-tune Epoch 1 Batch 5800 Loss 0.6786\n",
            "Fine-tune Epoch 1 Batch 5850 Loss 0.5448\n",
            "Fine-tune Epoch 1 Batch 5900 Loss 0.5660\n",
            "Fine-tune Epoch 1 Batch 5950 Loss 0.6601\n",
            "Fine-tune Epoch 1 Batch 6000 Loss 0.3941\n",
            "Fine-tune Epoch 1 Batch 6050 Loss 0.5077\n",
            "Fine-tune Epoch 1 Batch 6100 Loss 0.4716\n",
            "Fine-tune Epoch 1 Batch 6150 Loss 0.5033\n",
            "Fine-tune Epoch 1 Batch 6200 Loss 0.4147\n",
            "Fine-tune Epoch 1 Batch 6250 Loss 0.6284\n",
            "Fine-tune Epoch 1 Batch 6300 Loss 0.5689\n",
            "Fine-tune Epoch 1 Batch 6350 Loss 0.4718\n",
            "Fine-tune Epoch 1 Batch 6400 Loss 0.3918\n",
            "Fine-tune Epoch 1 Batch 6450 Loss 0.5294\n",
            "Fine-tune Epoch 1 Batch 6500 Loss 0.5164\n",
            "Fine-tune Epoch 1 Batch 6550 Loss 0.6265\n",
            "Fine-tune Epoch 1 Batch 6600 Loss 0.5378\n",
            "Fine-tune Epoch 1 Batch 6650 Loss 0.6360\n",
            "Fine-tune Epoch 1 Batch 6700 Loss 0.5636\n",
            "Fine-tune Epoch 1 Batch 6750 Loss 0.4916\n",
            "Fine-tune Epoch 1 Batch 6800 Loss 0.4823\n",
            "Fine-tune Epoch 1 Batch 6850 Loss 0.5176\n",
            "Fine-tune Epoch 1 Batch 6900 Loss 0.4871\n",
            "Fine-tune Epoch 1 Batch 6950 Loss 0.4897\n",
            "Fine-tune Epoch 1 Batch 7000 Loss 0.5538\n",
            "Fine-tune Epoch 1 Batch 7050 Loss 0.5957\n",
            "Fine-tune Epoch 1 Batch 7100 Loss 0.4450\n",
            "Fine-tune Epoch 1 Batch 7150 Loss 0.5498\n",
            "Fine-tune Epoch 1/3 | Train Loss: 0.551938 | Valid Loss: 0.877615\n",
            "Checkpoint saved | Time: 1942.88 sec\n",
            "\n",
            "Fine-tune Epoch 2 Batch 0 Loss 0.4550\n",
            "Fine-tune Epoch 2 Batch 50 Loss 0.6134\n",
            "Fine-tune Epoch 2 Batch 100 Loss 0.4571\n",
            "Fine-tune Epoch 2 Batch 150 Loss 0.6459\n",
            "Fine-tune Epoch 2 Batch 200 Loss 0.6642\n",
            "Fine-tune Epoch 2 Batch 250 Loss 0.6230\n",
            "Fine-tune Epoch 2 Batch 300 Loss 0.6454\n",
            "Fine-tune Epoch 2 Batch 350 Loss 0.4828\n",
            "Fine-tune Epoch 2 Batch 400 Loss 0.5737\n",
            "Fine-tune Epoch 2 Batch 450 Loss 0.6369\n",
            "Fine-tune Epoch 2 Batch 500 Loss 0.5696\n",
            "Fine-tune Epoch 2 Batch 550 Loss 0.4474\n",
            "Fine-tune Epoch 2 Batch 600 Loss 0.5890\n",
            "Fine-tune Epoch 2 Batch 650 Loss 0.5427\n",
            "Fine-tune Epoch 2 Batch 700 Loss 0.4861\n",
            "Fine-tune Epoch 2 Batch 750 Loss 0.7002\n",
            "Fine-tune Epoch 2 Batch 800 Loss 0.5235\n",
            "Fine-tune Epoch 2 Batch 850 Loss 0.6134\n",
            "Fine-tune Epoch 2 Batch 900 Loss 0.5425\n",
            "Fine-tune Epoch 2 Batch 950 Loss 0.5268\n",
            "Fine-tune Epoch 2 Batch 1000 Loss 0.5182\n",
            "Fine-tune Epoch 2 Batch 1050 Loss 0.5251\n",
            "Fine-tune Epoch 2 Batch 1100 Loss 0.6607\n",
            "Fine-tune Epoch 2 Batch 1150 Loss 0.5997\n",
            "Fine-tune Epoch 2 Batch 1200 Loss 0.4970\n",
            "Fine-tune Epoch 2 Batch 1250 Loss 0.4674\n",
            "Fine-tune Epoch 2 Batch 1300 Loss 0.5614\n",
            "Fine-tune Epoch 2 Batch 1350 Loss 0.6455\n",
            "Fine-tune Epoch 2 Batch 1400 Loss 0.6649\n",
            "Fine-tune Epoch 2 Batch 1450 Loss 0.5688\n",
            "Fine-tune Epoch 2 Batch 1500 Loss 0.5532\n",
            "Fine-tune Epoch 2 Batch 1550 Loss 0.5495\n",
            "Fine-tune Epoch 2 Batch 1600 Loss 0.3689\n",
            "Fine-tune Epoch 2 Batch 1650 Loss 0.5371\n",
            "Fine-tune Epoch 2 Batch 1700 Loss 0.5709\n",
            "Fine-tune Epoch 2 Batch 1750 Loss 0.4710\n",
            "Fine-tune Epoch 2 Batch 1800 Loss 0.3479\n",
            "Fine-tune Epoch 2 Batch 1850 Loss 0.5162\n",
            "Fine-tune Epoch 2 Batch 1900 Loss 0.5535\n",
            "Fine-tune Epoch 2 Batch 1950 Loss 0.6489\n",
            "Fine-tune Epoch 2 Batch 2000 Loss 0.7692\n",
            "Fine-tune Epoch 2 Batch 2050 Loss 0.5333\n",
            "Fine-tune Epoch 2 Batch 2100 Loss 0.5872\n",
            "Fine-tune Epoch 2 Batch 2150 Loss 0.5971\n",
            "Fine-tune Epoch 2 Batch 2200 Loss 0.5014\n",
            "Fine-tune Epoch 2 Batch 2250 Loss 0.6548\n",
            "Fine-tune Epoch 2 Batch 2300 Loss 0.5364\n",
            "Fine-tune Epoch 2 Batch 2350 Loss 0.7585\n",
            "Fine-tune Epoch 2 Batch 2400 Loss 0.4534\n",
            "Fine-tune Epoch 2 Batch 2450 Loss 0.5423\n",
            "Fine-tune Epoch 2 Batch 2500 Loss 0.3892\n",
            "Fine-tune Epoch 2 Batch 2550 Loss 0.5667\n",
            "Fine-tune Epoch 2 Batch 2600 Loss 0.4709\n",
            "Fine-tune Epoch 2 Batch 2650 Loss 0.6893\n",
            "Fine-tune Epoch 2 Batch 2700 Loss 0.5204\n",
            "Fine-tune Epoch 2 Batch 2750 Loss 0.5884\n",
            "Fine-tune Epoch 2 Batch 2800 Loss 0.5150\n",
            "Fine-tune Epoch 2 Batch 2850 Loss 0.4490\n",
            "Fine-tune Epoch 2 Batch 2900 Loss 0.5383\n",
            "Fine-tune Epoch 2 Batch 2950 Loss 0.5443\n",
            "Fine-tune Epoch 2 Batch 3000 Loss 0.6488\n",
            "Fine-tune Epoch 2 Batch 3050 Loss 0.5383\n",
            "Fine-tune Epoch 2 Batch 3100 Loss 0.5082\n",
            "Fine-tune Epoch 2 Batch 3150 Loss 0.5521\n",
            "Fine-tune Epoch 2 Batch 3200 Loss 0.5473\n",
            "Fine-tune Epoch 2 Batch 3250 Loss 0.5769\n",
            "Fine-tune Epoch 2 Batch 3300 Loss 0.5798\n",
            "Fine-tune Epoch 2 Batch 3350 Loss 0.6218\n",
            "Fine-tune Epoch 2 Batch 3400 Loss 0.5896\n",
            "Fine-tune Epoch 2 Batch 3450 Loss 0.5753\n",
            "Fine-tune Epoch 2 Batch 3500 Loss 0.5690\n",
            "Fine-tune Epoch 2 Batch 3550 Loss 0.7056\n",
            "Fine-tune Epoch 2 Batch 3600 Loss 0.4521\n",
            "Fine-tune Epoch 2 Batch 3650 Loss 0.7006\n",
            "Fine-tune Epoch 2 Batch 3700 Loss 0.4821\n",
            "Fine-tune Epoch 2 Batch 3750 Loss 0.4782\n",
            "Fine-tune Epoch 2 Batch 3800 Loss 0.7217\n",
            "Fine-tune Epoch 2 Batch 3850 Loss 0.5831\n",
            "Fine-tune Epoch 2 Batch 3900 Loss 0.6850\n",
            "Fine-tune Epoch 2 Batch 3950 Loss 0.4660\n",
            "Fine-tune Epoch 2 Batch 4000 Loss 0.7356\n",
            "Fine-tune Epoch 2 Batch 4050 Loss 0.7483\n",
            "Fine-tune Epoch 2 Batch 4100 Loss 0.4361\n",
            "Fine-tune Epoch 2 Batch 4150 Loss 0.6199\n",
            "Fine-tune Epoch 2 Batch 4200 Loss 0.4784\n",
            "Fine-tune Epoch 2 Batch 4250 Loss 0.5341\n",
            "Fine-tune Epoch 2 Batch 4300 Loss 0.5640\n",
            "Fine-tune Epoch 2 Batch 4350 Loss 0.6312\n",
            "Fine-tune Epoch 2 Batch 4400 Loss 0.4711\n",
            "Fine-tune Epoch 2 Batch 4450 Loss 0.4076\n",
            "Fine-tune Epoch 2 Batch 4500 Loss 0.4680\n",
            "Fine-tune Epoch 2 Batch 4550 Loss 0.4429\n",
            "Fine-tune Epoch 2 Batch 4600 Loss 0.5605\n",
            "Fine-tune Epoch 2 Batch 4650 Loss 0.6992\n",
            "Fine-tune Epoch 2 Batch 4700 Loss 0.6122\n",
            "Fine-tune Epoch 2 Batch 4750 Loss 0.5353\n",
            "Fine-tune Epoch 2 Batch 4800 Loss 0.4910\n",
            "Fine-tune Epoch 2 Batch 4850 Loss 0.5752\n",
            "Fine-tune Epoch 2 Batch 4900 Loss 0.4680\n",
            "Fine-tune Epoch 2 Batch 4950 Loss 0.4837\n",
            "Fine-tune Epoch 2 Batch 5000 Loss 0.4005\n",
            "Fine-tune Epoch 2 Batch 5050 Loss 0.5558\n",
            "Fine-tune Epoch 2 Batch 5100 Loss 0.6133\n",
            "Fine-tune Epoch 2 Batch 5150 Loss 0.5498\n",
            "Fine-tune Epoch 2 Batch 5200 Loss 0.5080\n",
            "Fine-tune Epoch 2 Batch 5250 Loss 0.6244\n",
            "Fine-tune Epoch 2 Batch 5300 Loss 0.5935\n",
            "Fine-tune Epoch 2 Batch 5350 Loss 0.5024\n",
            "Fine-tune Epoch 2 Batch 5400 Loss 0.5486\n",
            "Fine-tune Epoch 2 Batch 5450 Loss 0.5616\n",
            "Fine-tune Epoch 2 Batch 5500 Loss 0.4681\n",
            "Fine-tune Epoch 2 Batch 5550 Loss 0.4097\n",
            "Fine-tune Epoch 2 Batch 5600 Loss 0.5370\n",
            "Fine-tune Epoch 2 Batch 5650 Loss 0.7351\n",
            "Fine-tune Epoch 2 Batch 5700 Loss 0.4393\n",
            "Fine-tune Epoch 2 Batch 5750 Loss 0.5915\n",
            "Fine-tune Epoch 2 Batch 5800 Loss 0.5262\n",
            "Fine-tune Epoch 2 Batch 5850 Loss 0.5898\n",
            "Fine-tune Epoch 2 Batch 5900 Loss 0.5265\n",
            "Fine-tune Epoch 2 Batch 5950 Loss 0.5040\n",
            "Fine-tune Epoch 2 Batch 6000 Loss 0.5205\n",
            "Fine-tune Epoch 2 Batch 6050 Loss 0.5531\n",
            "Fine-tune Epoch 2 Batch 6100 Loss 0.4695\n",
            "Fine-tune Epoch 2 Batch 6150 Loss 0.5769\n",
            "Fine-tune Epoch 2 Batch 6200 Loss 0.4672\n",
            "Fine-tune Epoch 2 Batch 6250 Loss 0.4942\n",
            "Fine-tune Epoch 2 Batch 6300 Loss 0.5578\n",
            "Fine-tune Epoch 2 Batch 6350 Loss 0.5036\n",
            "Fine-tune Epoch 2 Batch 6400 Loss 0.5689\n",
            "Fine-tune Epoch 2 Batch 6450 Loss 0.5430\n",
            "Fine-tune Epoch 2 Batch 6500 Loss 0.4300\n",
            "Fine-tune Epoch 2 Batch 6550 Loss 0.4768\n",
            "Fine-tune Epoch 2 Batch 6600 Loss 0.5373\n",
            "Fine-tune Epoch 2 Batch 6650 Loss 0.4246\n",
            "Fine-tune Epoch 2 Batch 6700 Loss 0.6279\n",
            "Fine-tune Epoch 2 Batch 6750 Loss 0.5785\n",
            "Fine-tune Epoch 2 Batch 6800 Loss 0.5295\n",
            "Fine-tune Epoch 2 Batch 6850 Loss 0.5653\n",
            "Fine-tune Epoch 2 Batch 6900 Loss 0.5282\n",
            "Fine-tune Epoch 2 Batch 6950 Loss 0.4856\n",
            "Fine-tune Epoch 2 Batch 7000 Loss 0.6510\n",
            "Fine-tune Epoch 2 Batch 7050 Loss 0.5095\n",
            "Fine-tune Epoch 2 Batch 7100 Loss 0.4711\n",
            "Fine-tune Epoch 2 Batch 7150 Loss 0.6915\n",
            "Fine-tune Epoch 2/3 | Train Loss: 0.543777 | Valid Loss: 0.874280\n",
            "Checkpoint saved | Time: 1929.18 sec\n",
            "\n",
            "Fine-tune Epoch 3 Batch 0 Loss 0.6009\n",
            "Fine-tune Epoch 3 Batch 50 Loss 0.5329\n",
            "Fine-tune Epoch 3 Batch 100 Loss 0.4988\n",
            "Fine-tune Epoch 3 Batch 150 Loss 0.5409\n",
            "Fine-tune Epoch 3 Batch 200 Loss 0.5347\n",
            "Fine-tune Epoch 3 Batch 250 Loss 0.5002\n",
            "Fine-tune Epoch 3 Batch 300 Loss 0.6623\n",
            "Fine-tune Epoch 3 Batch 350 Loss 0.4815\n",
            "Fine-tune Epoch 3 Batch 400 Loss 0.6766\n",
            "Fine-tune Epoch 3 Batch 450 Loss 0.5609\n",
            "Fine-tune Epoch 3 Batch 500 Loss 0.5274\n",
            "Fine-tune Epoch 3 Batch 550 Loss 0.5207\n",
            "Fine-tune Epoch 3 Batch 600 Loss 0.5186\n",
            "Fine-tune Epoch 3 Batch 650 Loss 0.6595\n",
            "Fine-tune Epoch 3 Batch 700 Loss 0.5351\n",
            "Fine-tune Epoch 3 Batch 750 Loss 0.4435\n",
            "Fine-tune Epoch 3 Batch 800 Loss 0.5034\n",
            "Fine-tune Epoch 3 Batch 850 Loss 0.5576\n",
            "Fine-tune Epoch 3 Batch 900 Loss 0.5391\n",
            "Fine-tune Epoch 3 Batch 950 Loss 0.6504\n",
            "Fine-tune Epoch 3 Batch 1000 Loss 0.5723\n",
            "Fine-tune Epoch 3 Batch 1050 Loss 0.5303\n",
            "Fine-tune Epoch 3 Batch 1100 Loss 0.4524\n",
            "Fine-tune Epoch 3 Batch 1150 Loss 0.4073\n",
            "Fine-tune Epoch 3 Batch 1200 Loss 0.5734\n",
            "Fine-tune Epoch 3 Batch 1250 Loss 0.4880\n",
            "Fine-tune Epoch 3 Batch 1300 Loss 0.4805\n",
            "Fine-tune Epoch 3 Batch 1350 Loss 0.5666\n",
            "Fine-tune Epoch 3 Batch 1400 Loss 0.5239\n",
            "Fine-tune Epoch 3 Batch 1450 Loss 0.6359\n",
            "Fine-tune Epoch 3 Batch 1500 Loss 0.6665\n",
            "Fine-tune Epoch 3 Batch 1550 Loss 0.6286\n",
            "Fine-tune Epoch 3 Batch 1600 Loss 0.4832\n",
            "Fine-tune Epoch 3 Batch 1650 Loss 0.3935\n",
            "Fine-tune Epoch 3 Batch 1700 Loss 0.4530\n",
            "Fine-tune Epoch 3 Batch 1750 Loss 0.5196\n",
            "Fine-tune Epoch 3 Batch 1800 Loss 0.7217\n",
            "Fine-tune Epoch 3 Batch 1850 Loss 0.5512\n",
            "Fine-tune Epoch 3 Batch 1900 Loss 0.5485\n",
            "Fine-tune Epoch 3 Batch 1950 Loss 0.5644\n",
            "Fine-tune Epoch 3 Batch 2000 Loss 0.5764\n",
            "Fine-tune Epoch 3 Batch 2050 Loss 0.4118\n",
            "Fine-tune Epoch 3 Batch 2100 Loss 0.5807\n",
            "Fine-tune Epoch 3 Batch 2150 Loss 0.5244\n",
            "Fine-tune Epoch 3 Batch 2200 Loss 0.6452\n",
            "Fine-tune Epoch 3 Batch 2250 Loss 0.5056\n",
            "Fine-tune Epoch 3 Batch 2300 Loss 0.4559\n",
            "Fine-tune Epoch 3 Batch 2350 Loss 0.6158\n",
            "Fine-tune Epoch 3 Batch 2400 Loss 0.5299\n",
            "Fine-tune Epoch 3 Batch 2450 Loss 0.6690\n",
            "Fine-tune Epoch 3 Batch 2500 Loss 0.6933\n",
            "Fine-tune Epoch 3 Batch 2550 Loss 0.3592\n",
            "Fine-tune Epoch 3 Batch 2600 Loss 0.6936\n",
            "Fine-tune Epoch 3 Batch 2650 Loss 0.5520\n",
            "Fine-tune Epoch 3 Batch 2700 Loss 0.5620\n",
            "Fine-tune Epoch 3 Batch 2750 Loss 0.5306\n",
            "Fine-tune Epoch 3 Batch 2800 Loss 0.6274\n",
            "Fine-tune Epoch 3 Batch 2850 Loss 0.5888\n",
            "Fine-tune Epoch 3 Batch 2900 Loss 0.5161\n",
            "Fine-tune Epoch 3 Batch 2950 Loss 0.4821\n",
            "Fine-tune Epoch 3 Batch 3000 Loss 0.6598\n",
            "Fine-tune Epoch 3 Batch 3050 Loss 0.5120\n",
            "Fine-tune Epoch 3 Batch 3100 Loss 0.5585\n",
            "Fine-tune Epoch 3 Batch 3150 Loss 0.4925\n",
            "Fine-tune Epoch 3 Batch 3200 Loss 0.5821\n",
            "Fine-tune Epoch 3 Batch 3250 Loss 0.5130\n",
            "Fine-tune Epoch 3 Batch 3300 Loss 0.5643\n",
            "Fine-tune Epoch 3 Batch 3350 Loss 0.5035\n",
            "Fine-tune Epoch 3 Batch 3400 Loss 0.5755\n",
            "Fine-tune Epoch 3 Batch 3450 Loss 0.5114\n",
            "Fine-tune Epoch 3 Batch 3500 Loss 0.4705\n",
            "Fine-tune Epoch 3 Batch 3550 Loss 0.6388\n",
            "Fine-tune Epoch 3 Batch 3600 Loss 0.5311\n",
            "Fine-tune Epoch 3 Batch 3650 Loss 0.6261\n",
            "Fine-tune Epoch 3 Batch 3700 Loss 0.5609\n",
            "Fine-tune Epoch 3 Batch 3750 Loss 0.5238\n",
            "Fine-tune Epoch 3 Batch 3800 Loss 0.5276\n",
            "Fine-tune Epoch 3 Batch 3850 Loss 0.5372\n",
            "Fine-tune Epoch 3 Batch 3900 Loss 0.5181\n",
            "Fine-tune Epoch 3 Batch 3950 Loss 0.5777\n",
            "Fine-tune Epoch 3 Batch 4000 Loss 0.4611\n",
            "Fine-tune Epoch 3 Batch 4050 Loss 0.5960\n",
            "Fine-tune Epoch 3 Batch 4100 Loss 0.4623\n",
            "Fine-tune Epoch 3 Batch 4150 Loss 0.5711\n",
            "Fine-tune Epoch 3 Batch 4200 Loss 0.4771\n",
            "Fine-tune Epoch 3 Batch 4250 Loss 0.4520\n",
            "Fine-tune Epoch 3 Batch 4300 Loss 0.5368\n",
            "Fine-tune Epoch 3 Batch 4350 Loss 0.4909\n",
            "Fine-tune Epoch 3 Batch 4400 Loss 0.5621\n",
            "Fine-tune Epoch 3 Batch 4450 Loss 0.5230\n",
            "Fine-tune Epoch 3 Batch 4500 Loss 0.5725\n",
            "Fine-tune Epoch 3 Batch 4550 Loss 0.4193\n",
            "Fine-tune Epoch 3 Batch 4600 Loss 0.6302\n",
            "Fine-tune Epoch 3 Batch 4650 Loss 0.6510\n",
            "Fine-tune Epoch 3 Batch 4700 Loss 0.4392\n",
            "Fine-tune Epoch 3 Batch 4750 Loss 0.6761\n",
            "Fine-tune Epoch 3 Batch 4800 Loss 0.4639\n",
            "Fine-tune Epoch 3 Batch 4850 Loss 0.5618\n",
            "Fine-tune Epoch 3 Batch 4900 Loss 0.4353\n",
            "Fine-tune Epoch 3 Batch 4950 Loss 0.7216\n",
            "Fine-tune Epoch 3 Batch 5000 Loss 0.4419\n",
            "Fine-tune Epoch 3 Batch 5050 Loss 0.6631\n",
            "Fine-tune Epoch 3 Batch 5100 Loss 0.6172\n",
            "Fine-tune Epoch 3 Batch 5150 Loss 0.6097\n",
            "Fine-tune Epoch 3 Batch 5200 Loss 0.4751\n",
            "Fine-tune Epoch 3 Batch 5250 Loss 0.6121\n",
            "Fine-tune Epoch 3 Batch 5300 Loss 0.5163\n",
            "Fine-tune Epoch 3 Batch 5350 Loss 0.5226\n",
            "Fine-tune Epoch 3 Batch 5400 Loss 0.5101\n",
            "Fine-tune Epoch 3 Batch 5450 Loss 0.5038\n",
            "Fine-tune Epoch 3 Batch 5500 Loss 0.5474\n",
            "Fine-tune Epoch 3 Batch 5550 Loss 0.5598\n",
            "Fine-tune Epoch 3 Batch 5600 Loss 0.4694\n",
            "Fine-tune Epoch 3 Batch 5650 Loss 0.6617\n",
            "Fine-tune Epoch 3 Batch 5700 Loss 0.5076\n",
            "Fine-tune Epoch 3 Batch 5750 Loss 0.5277\n",
            "Fine-tune Epoch 3 Batch 5800 Loss 0.4399\n",
            "Fine-tune Epoch 3 Batch 5850 Loss 0.5046\n",
            "Fine-tune Epoch 3 Batch 5900 Loss 0.4144\n",
            "Fine-tune Epoch 3 Batch 5950 Loss 0.4710\n",
            "Fine-tune Epoch 3 Batch 6000 Loss 0.5619\n",
            "Fine-tune Epoch 3 Batch 6050 Loss 0.5421\n",
            "Fine-tune Epoch 3 Batch 6100 Loss 0.6187\n",
            "Fine-tune Epoch 3 Batch 6150 Loss 0.6084\n",
            "Fine-tune Epoch 3 Batch 6200 Loss 0.3882\n",
            "Fine-tune Epoch 3 Batch 6250 Loss 0.5547\n",
            "Fine-tune Epoch 3 Batch 6300 Loss 0.4946\n",
            "Fine-tune Epoch 3 Batch 6350 Loss 0.5466\n",
            "Fine-tune Epoch 3 Batch 6400 Loss 0.4433\n",
            "Fine-tune Epoch 3 Batch 6450 Loss 0.4903\n",
            "Fine-tune Epoch 3 Batch 6500 Loss 0.4878\n",
            "Fine-tune Epoch 3 Batch 6550 Loss 0.5338\n",
            "Fine-tune Epoch 3 Batch 6600 Loss 0.6347\n",
            "Fine-tune Epoch 3 Batch 6650 Loss 0.5631\n",
            "Fine-tune Epoch 3 Batch 6700 Loss 0.5917\n",
            "Fine-tune Epoch 3 Batch 6750 Loss 0.5570\n",
            "Fine-tune Epoch 3 Batch 6800 Loss 0.5278\n",
            "Fine-tune Epoch 3 Batch 6850 Loss 0.4517\n",
            "Fine-tune Epoch 3 Batch 6900 Loss 0.6288\n",
            "Fine-tune Epoch 3 Batch 6950 Loss 0.4292\n",
            "Fine-tune Epoch 3 Batch 7000 Loss 0.5228\n",
            "Fine-tune Epoch 3 Batch 7050 Loss 0.5425\n",
            "Fine-tune Epoch 3 Batch 7100 Loss 0.4702\n",
            "Fine-tune Epoch 3 Batch 7150 Loss 0.3936\n",
            "Fine-tune Epoch 3/3 | Train Loss: 0.538498 | Valid Loss: 0.869944\n",
            "Checkpoint saved | Time: 1929.57 sec\n",
            "\n",
            "Training complete.\n",
            "\n",
            "=== Testing Predictions ===\n",
            "Image: 00000b332dcd6fe5.png\n",
            "Prediction: (\\begin{matrix}j\\\\ m+1\\end{matrix})\n",
            "\n",
            "Image: 000453d57c3d334d.png\n",
            "Prediction: \\int K(x-y;T)dy=1\n",
            "\n",
            "Image: 0005d0be8e507b24.png\n",
            "Prediction: 2^{-r_{2}}\\sqrt{|\\Delta_{K}|}\n",
            "\n",
            "Image: 0005ea8e21185d36.png\n",
            "Prediction: S=\\int Ldx_{3}\n",
            "\n",
            "Image: 00061be0501a1fa8.png\n",
            "Prediction: \\sum_{i=0}^{4}(\\begin{matrix}8\\\\ 2i\\end{matrix})(\\begin{matrix}28\\\\ 16-i\\end{matrix})\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In your Colab notebook (after the tokenizer has been adapted)\n",
        "vocab = tokenizer.get_vocabulary()\n",
        "\n",
        "# Save the vocabulary to a text file\n",
        "with open('vocab.txt', 'w', encoding='utf-8') as f:\n",
        "    for token in vocab:\n",
        "        f.write(token + '\\n')\n",
        "\n",
        "# Download the vocab.txt file to your computer\n",
        "from google.colab import files\n",
        "files.download('vocab.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AmXQyhQK3SdH",
        "outputId": "a802090f-fc44-4fa8-c55c-64ce35a477ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_952c13b1-b01d-4484-b484-00ba696eb7fd\", \"vocab.txt\", 1544746)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}