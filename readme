Handwritten Math Recognition ‚úçÔ∏è‚û°Ô∏èüßÆ
This project explores different deep learning approaches to translate images of handwritten mathematical expressions into digital formats. It implements two distinct methodologies: a traditional character-level classification system (Baseline) and an advanced end-to-end image-to-sequence model with an attention mechanism (Advanced).

The goal is to provide a robust solution for digitizing handwritten formulas, enhancing their searchability, accessibility, and archival.

Features
Two Distinct Approaches Implemented:

Approach 1 (Baseline - Character Classification):

Utilizes classic computer vision (OpenCV) for contour detection and character segmentation.

Employs a simple Convolutional Neural Network (CNN) for individual character classification.

Outputs a linear string of predicted characters.

Approach 2 (Advanced - Image-to-LaTeX End-to-End):

Translates entire handwritten formula images into LaTeX code directly.

Leverages a sophisticated Encoder-Decoder architecture with Bahdanau Attention.

Transfer Learning: Employs a pre-trained ResNet50V2 as the encoder for powerful visual feature extraction.

Two-Phase Training: Implements a strategy with initial feature-extraction (frozen encoder) and subsequent fine-tuning.

Designed to handle 2D structural complexities inherent in mathematical expressions.

Evaluation Suite: Includes code to calculate standard sequence-to-sequence metrics (Character Error Rate, Exact Match Rate, BLEU score) for Approach 2.

Interactive Web App: A user-friendly interface built with Streamlit (for Approach 2) to:

Upload a handwritten image.

Get the predicted LaTeX formula.

Dynamically input values for detected variables.

Calculate the numerical result of the expression using SymPy.

Model Architecture (Approach 2: Advanced Model)
The advanced model, which powers the Streamlit application, is composed of three main parts:

Encoder (ResNet50V2):

A pre-trained ResNet50V2 model (with weights from ImageNet) acts as the visual feature extractor.

It processes the input image (3 channels, 224x224) and converts it into a rich, spatially aware feature map (7x7x2048).

The initial layers are often "frozen" during early training phases, with later layers "fine-tuned."

Attention Mechanism (BahdanauAttention):

This layer allows the decoder to dynamically "look back" and focus on the most relevant parts of the image's feature map at each step of the decoding process.

Crucial for correctly parsing 2D structures like fractions, exponents, and integrals by understanding spatial relationships.

Decoder (LSTM):

A LSTM (Long Short-Term Memory) based Recurrent Neural Network (RNN) generates the output LaTeX sequence one token at a time in an autoregressive fashion.

It takes the embedding of the previous token and the context vector (from the attention mechanism) to predict the next token.

Technology Stack
Backend: Python 3.9+

Deep Learning: TensorFlow, Keras

Computer Vision: OpenCV (cv2)

Web Interface: Streamlit

Mathematical Parsing: SymPy

Data Handling & Visualization: NumPy, Pandas, Matplotlib, Seaborn, Pillow

Metrics: Scikit-learn, NLTK (for BLEU score)

Project Structure
.
‚îú‚îÄ‚îÄ app.py                  # Streamlit web application (for Approach 2)
‚îú‚îÄ‚îÄ train_approach2.py      # Main training script for the Advanced Model
‚îú‚îÄ‚îÄ train_approach1.py      # Training script for the Baseline Model (if separate)
‚îú‚îÄ‚îÄ predict_approach1.py    # Prediction script for Approach 1 (your contour detection + CNN code)
‚îú‚îÄ‚îÄ eda.ipynb               # Jupyter notebook for Exploratory Data Analysis
‚îú‚îÄ‚îÄ vocab.txt               # Vocabulary file generated during training (for Approach 2)
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ training_checkpoints/     # Folder for saved model weights (for Approach 2)
‚îú‚îÄ‚îÄ math_model/             # Folder for saved model weights (for Approach 1, if applicable)
‚îú‚îÄ‚îÄ extracted_images/       # Directory containing segmented characters (for Approach 1)
‚îú‚îÄ‚îÄ formula_images/         # Directory containing full formula images (for Approach 2)
‚îú‚îÄ‚îÄ image/                  # Directory for sample input images
‚îî‚îÄ‚îÄ README.md
(Note: Adjust the .py script names above if your training/prediction scripts for Approach 1 and Approach 2 are combined or named differently, e.g., main_training.py or inference.py)

Setup and Installation
Follow these steps to set up the project on your local machine.

1. Clone the Repository
Bash

git clone <your-repository-url>
cd <repository-name>
2. Create a Virtual Environment
It's highly recommended to use a virtual environment to manage dependencies.

Bash

python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
3. Install Dependencies
Install all the required Python libraries using the requirements.txt file.

Bash

pip install -r requirements.txt
(Note: If you don't have a requirements.txt file, you can create one by running pip freeze > requirements.txt in your activated virtual environment.)

4. Download Data & Models
Approach 1 Data (extracted_images/): If you wish to re-train or run the baseline model, ensure your extracted_images directory (containing subfolders of classified characters) is populated.

Approach 2 Data (formula_images/): Download the CROHME dataset (or your processed image-to-LaTeX dataset) and place it in the formula_images/train/images, formula_images/train/labels.txt, etc., structure as expected by train_approach2.py.

Model Checkpoints:

For Approach 2: Download the training_checkpoints folder and vocab.txt file and place them in the root directory of the project. This contains the trained weights for the advanced model.

For Approach 1: If you have a saved math_model directory from training Approach 1, ensure it's in the project root.

Usage
1. Running Approach 1 (Character Classification)
This approach is demonstrated through a script that takes an image, segments it, and predicts character-by-character.

To run predictions with the baseline model:

Bash

python predict_approach1.py --image_path "image/image1.jpg"
# Or wherever your prediction script is located and expects input
(Note: Adjust the script name and arguments based on your actual predict_approach1.py file.)

2. Running Approach 2 (End-to-End Image-to-LaTeX)
a) Using the Streamlit Web Application (Recommended)
This is the easiest way to interact with the advanced model.

Bash

streamlit run app.py
Your web browser will open with the application interface. Upload a handwritten image, click "Predict Formula", and then you can input variable values to calculate the result.

b) Using the Command-Line Script for Prediction
You can also make predictions directly from your terminal using the advanced model.

Bash

python train_approach2.py --mode predict --image_path "image/your_complex_formula.png"
# Or a dedicated prediction script if you have one, e.g., `python predict_approach2.py --image "image/your_complex_formula.png"`
(Note: The example above assumes your train_approach2.py has a prediction mode, or you have a separate predict_approach2.py file configured to load the advanced model.)

3. Training the Models (Optional)
If you wish to re-train either model:

For Approach 1:

Bash

python train_approach1.py
For Approach 2:

Bash

python train_approach2.py
(Ensure your dataset paths in these scripts are correctly configured).

Results and Performance
Approach 1 (Baseline - Character Classification)
Output Example: For an input image like "y2+Ny1+1=N", the model produced y2+Ny1+1=N.

Analysis: This model successfully identifies individual characters but fundamentally struggles with the 2D structure of mathematical expressions (e.g., exponents, fractions, nested terms). Its predictions are a linear sequence, which is insufficient for representing complex math.

Approach 2 (Advanced - Image-to-LaTeX)
Key Metrics (on Validation Set - 1000 samples):

Exact Match Rate: 9.50%

Character Error Rate (CER): 80.17%

Corpus BLEU Score: 0.0142

Output Example: For an input image, the model produced \frac{d}{dt}N_{2}.

Analysis: While the current metrics indicate room for improvement (suggesting potential under-training or need for more data/hyperparameter tuning), this model demonstrates the capability to generate complex LaTeX structures like fractions (\frac{}{}) and subscripts (N_{2}). This is a significant advancement over the baseline, as it implicitly learns the 2D grammar of mathematics through the attention mechanism. Further training and optimization are expected to yield higher accuracy.

Future Work
Extensive Training: Train Approach 2 for more epochs on a larger dataset to improve convergence and reduce CER.

Data Augmentation: Implement more sophisticated data augmentation techniques (e.g., elastic deformations, varying stroke thickness) for Approach 2 to enhance robustness to handwriting variability.

Advanced Decoders: Explore Transformer-based decoders instead of LSTM in Approach 2, which are known to capture long-range dependencies more effectively.

Beam Search: Implement beam search during inference for Approach 2 to generate more accurate predictions by exploring multiple candidate sequences.

Improved Baseline: Enhance Approach 1 with a more robust segmentation algorithm and a more powerful CNN classifier.

Error Analysis Tool: Develop a more in-depth tool to visualize specific error types for both models.
